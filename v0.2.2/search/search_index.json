{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"dev-health-ops","text":"<p>Developer health and operations analytics, built to be transparent, affordable, and useful.</p>"},{"location":"#why-this-exists","title":"Why this exists","text":"<p>Developer health tooling drifted into expensive, opaque scoring systems that are easy to misuse. This project is intentionally different.</p> <ul> <li>Accessibility over extraction</li> <li>Learning, not judgment</li> <li>Trends over absolutes</li> <li>Inspectable by default</li> </ul>"},{"location":"#what-you-can-do-here","title":"What you can do here","text":"<ul> <li>Sync data from GitHub, GitLab, Jira, and local Git repositories.</li> <li>Normalize and store work items, commits, and blame data.</li> <li>Compute metrics like throughput, cycle time, rework, and predictability.</li> <li>Explore Grafana dashboards for hotspots, investment flow, and team health.</li> </ul>"},{"location":"#quick-links","title":"Quick links","text":"<ul> <li>Getting started: <code>getting-started.md</code></li> <li>Architecture: <code>architecture.md</code></li> <li>CLI reference: <code>cli.md</code></li> <li>Metrics: <code>metrics.md</code></li> <li>Grafana: <code>grafana.md</code></li> </ul>"},{"location":"CONNECTOR_USAGE/","title":"Using <code>cli.py</code> with GitHub and GitLab Connectors","text":"<p>The <code>cli.py</code> front end exposes target-specific sync jobs with a provider switch:</p> <ol> <li>Sync targets: <code>git</code>, <code>prs</code>, <code>blame</code>, <code>cicd</code>, <code>deployments</code>, <code>incidents</code></li> <li>Providers: <code>local</code>, <code>github</code>, <code>gitlab</code>, <code>synthetic</code></li> </ol>"},{"location":"CONNECTOR_USAGE/#simplified-library-dispatch","title":"Simplified Library Dispatch","text":"<p>The dedicated CLI (<code>python cli.py sync ...</code>) now exposes:</p> <ul> <li>Unified authentication with <code>--auth</code> (works for both GitHub and GitLab)</li> <li>Auto-detection of database type from connection string URL scheme</li> <li>Consolidated batch processing arguments that work with both connectors (<code>-s/--search</code>, <code>--group</code>, <code>--batch-size</code>, <code>--max-concurrent</code>)</li> </ul>"},{"location":"CONNECTOR_USAGE/#local-repository-mode","title":"Local Repository Mode","text":"<p>This is the original mode that analyzes a local git repository.</p> <pre><code># Using environment variables\nexport DATABASE_URI=\"postgresql+asyncpg://localhost:5432/mergestat\"\nexport REPO_PATH=\"/path/to/repo\"\npython cli.py sync git --provider local\n\n# Using command-line arguments (auto-detects database type from URL)\npython cli.py sync git --provider local --db \"postgresql+asyncpg://localhost:5432/mergestat\" --repo-path \"/path/to/repo\"\n\n# Limit to commits and blames since a date (ISO date or datetime)\npython cli.py sync git --provider local --db \"sqlite+aiosqlite:///mergestat.db\" --repo-path \"/path/to/repo\" --since 2024-01-01\n\n# Explicit connector type (not needed with the CLI mode, but shown for parity)\npython cli.py sync git --provider local --db \"postgresql://...\" --repo-path \"/path/to/repo\"\n</code></pre> <p>How filtering works: <code>--since</code> / <code>--date</code> restricts commits and commit stats to changes at or after the timestamp.</p>"},{"location":"CONNECTOR_USAGE/#github-connector-mode","title":"GitHub Connector Mode","text":"<p>Fetch repository data directly from GitHub without cloning. Fully supports both public and private repositories.</p>"},{"location":"CONNECTOR_USAGE/#requirements","title":"Requirements","text":"<ul> <li>GitHub personal access token with appropriate permissions</li> <li>For public repositories: Any valid token (for higher rate limits)</li> <li>For private repositories: Token must have <code>repo</code> scope</li> <li>Repository owner and name</li> </ul>"},{"location":"CONNECTOR_USAGE/#usage","title":"Usage","text":"<pre><code># Public repository with unified auth\nexport GITHUB_TOKEN=\"ghp_xxxxxxxxxxxx\"\npython cli.py sync git --provider github \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner torvalds \\\n  --repo linux\n\n# Private repository (token must have 'repo' scope)\npython cli.py sync git --provider github \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner your-org \\\n  --repo your-private-repo\n\n# Token from environment variable (GITHUB_TOKEN) - no --auth needed\nexport GITHUB_TOKEN=\"ghp_xxxxxxxxxxxx\"\npython cli.py sync git --provider github \\\n  --db \"postgresql://...\" \\\n  --owner torvalds \\\n  --repo linux\n</code></pre>"},{"location":"CONNECTOR_USAGE/#batch-processing-multiple-repositories","title":"Batch Processing Multiple Repositories","text":"<pre><code># Process repositories matching a pattern\npython cli.py sync git --provider github \\\n  --db \"sqlite+aiosqlite:///mergestat.db\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  -s \"myorg/api-*\" \\\n  --group myorg \\\n  --batch-size 10 \\\n  --max-concurrent 4 \\\n  --max-repos 50 \\\n  --use-async\n</code></pre>"},{"location":"CONNECTOR_USAGE/#what-gets-stored","title":"What Gets Stored","text":"<ul> <li>Repository metadata (name, URL, default branch)</li> <li>Commits (up to 100 most recent)</li> <li>Commit statistics (additions, deletions per file for last 50 commits)</li> </ul>"},{"location":"CONNECTOR_USAGE/#gitlab-connector-mode","title":"GitLab Connector Mode","text":"<p>Fetch project data directly from GitLab (including self-hosted instances). Fully supports both public and private projects.</p>"},{"location":"CONNECTOR_USAGE/#requirements_1","title":"Requirements","text":"<ul> <li>GitLab private token with appropriate permissions</li> <li>For public projects: Token optional (but recommended for rate limits)</li> <li>For private projects: Token must have <code>read_api</code> and <code>read_repository</code> scopes</li> <li>Project ID (numeric ID, not path)</li> </ul>"},{"location":"CONNECTOR_USAGE/#usage_1","title":"Usage","text":"<pre><code># Public project on GitLab.com with unified auth\npython cli.py sync git --provider gitlab \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --project-id 278964\n\n# Private project (token must have required scopes)\npython cli.py sync git --provider gitlab \\\n  --db \"postgresql://...\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --project-id 12345\n\n# Self-hosted GitLab\npython cli.py sync git --provider gitlab \\\n  --db \"postgresql://...\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --gitlab-url \"https://gitlab.example.com\" \\\n  --project-id 123\n\n# Token from environment variable (GITLAB_TOKEN)\nexport GITLAB_TOKEN=\"glpat-xxxxxxxxxxxx\"\npython cli.py sync git --provider gitlab \\\n  --db \"mongodb://localhost:27017\" \\\n  --project-id 278964\n</code></pre>"},{"location":"CONNECTOR_USAGE/#batch-processing-multiple-projects","title":"Batch Processing Multiple Projects","text":"<pre><code># Process projects matching a pattern\npython cli.py sync git --provider gitlab \\\n  --db \"sqlite+aiosqlite:///mergestat.db\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --gitlab-url \"https://gitlab.com\" \\\n  --group mygroup \\\n  -s \"mygroup/api-*\" \\\n  --batch-size 10 \\\n  --max-concurrent 4 \\\n  --max-repos 50 \\\n  --use-async\n</code></pre>"},{"location":"CONNECTOR_USAGE/#what-gets-stored_1","title":"What Gets Stored","text":"<ul> <li>Project metadata (name, URL, default branch)</li> <li>Commits (up to 100 most recent)</li> <li>Commit statistics (aggregate additions/deletions for last 50 commits)</li> </ul>"},{"location":"CONNECTOR_USAGE/#finding-your-project-id","title":"Finding Your Project ID","text":"<p>The project ID is the numeric identifier for your GitLab project:</p> <ol> <li>Go to your project page on GitLab</li> <li>Look under the project name - you'll see \"Project ID: 12345\"</li> <li>Or use the GitLab API: <code>curl \"https://gitlab.com/api/v4/projects/owner%2Fproject\" --header \"PRIVATE-TOKEN: &lt;token&gt;\"</code></li> </ol>"},{"location":"CONNECTOR_USAGE/#environment-variables","title":"Environment Variables","text":"<p>All modes support these environment variables:</p> Variable Description Default <code>DATABASE_URI</code> Database connection string None (required) <code>SECONDARY_DATABASE_URI</code> Secondary database for <code>--sink both</code> None <code>DB_ECHO</code> Enable SQL logging <code>false</code> <code>BATCH_SIZE</code> Records per batch insert <code>100</code> <code>MAX_WORKERS</code> Parallel workers <code>4</code> <code>GITHUB_TOKEN</code> GitHub personal access token None <code>GITLAB_TOKEN</code> GitLab private token None <code>REPO_PATH</code> Path to local repository <code>.</code>"},{"location":"CONNECTOR_USAGE/#command-line-arguments","title":"Command-Line Arguments","text":"<p>The CLI exposes target-specific sync jobs (<code>sync git</code>, <code>sync prs</code>, <code>sync blame</code>, <code>sync cicd</code>, <code>sync deployments</code>, <code>sync incidents</code>) with a <code>--provider</code> flag.</p>"},{"location":"CONNECTOR_USAGE/#local-provider-python-clipy-sync-target-provider-local","title":"Local provider (<code>python cli.py sync &lt;target&gt; --provider local</code>)","text":"<ul> <li><code>--db DB</code> (required): Database connection string (auto-detects the backend from the URL).</li> <li><code>--repo-path PATH</code>: Path to the repository (<code>.</code> by default).</li> <li><code>--since / --date / --backfill</code>: Time filters for commits and stats.</li> <li><code>--db-type {postgres,mongo,sqlite,clickhouse}</code>: Optional override when the scheme is ambiguous.</li> </ul>"},{"location":"CONNECTOR_USAGE/#github-provider-python-clipy-sync-target-provider-github","title":"GitHub provider (<code>python cli.py sync &lt;target&gt; --provider github</code>)","text":"<p>Single-repo execution (required: <code>--owner</code> and <code>--repo</code>):</p> <ul> <li><code>--db DB</code> (required).</li> <li><code>--auth TOKEN</code>: Token overrides <code>GITHUB_TOKEN</code>.</li> <li><code>--owner OWNER</code> and <code>--repo REPO</code>: Target repository.</li> </ul> <p>Batch processing:</p> <ul> <li><code>-s, --search PATTERN</code> (required for batch).</li> <li><code>--group NAME</code>: Organization/user that owns the repositories.</li> <li><code>--batch-size N</code>, <code>--max-concurrent N</code>, <code>--rate-limit-delay SECONDS</code>: Tune pagination and throughput.</li> <li><code>--max-repos N</code>: Stop after processing N repositories.</li> <li><code>--use-async</code>: Enable async workers.</li> <li>Shared options: <code>--max-commits-per-repo</code>.</li> </ul>"},{"location":"CONNECTOR_USAGE/#gitlab-provider-python-clipy-sync-target-provider-gitlab","title":"GitLab provider (<code>python cli.py sync &lt;target&gt; --provider gitlab</code>)","text":"<ul> <li><code>--db DB</code> (required).</li> <li><code>--auth TOKEN</code>: Token overrides <code>GITLAB_TOKEN</code>.</li> <li><code>--project-id ID</code>: Required for single-project mode.</li> <li><code>--gitlab-url URL</code>: Defaults to <code>https://gitlab.com</code>.</li> <li>Batch options mirror the GitHub CLI flags (<code>-s/--search</code>, <code>--group</code>, etc.).</li> <li>Shared tuning: <code>--batch-size</code>, <code>--max-concurrent</code>, <code>--rate-limit-delay</code>, <code>--max-repos</code>, <code>--use-async</code>, <code>--max-commits-per-repo</code>.</li> </ul>"},{"location":"CONNECTOR_USAGE/#examples","title":"Examples","text":""},{"location":"CONNECTOR_USAGE/#analyze-linux-kernel-from-github","title":"Analyze Linux Kernel from GitHub","text":"<pre><code>python cli.py sync git --provider github \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner torvalds \\\n  --repo linux\n</code></pre>"},{"location":"CONNECTOR_USAGE/#analyze-gitlabs-gitlab-from-gitlabcom","title":"Analyze GitLab's GitLab from GitLab.com","text":"<pre><code>python cli.py sync git --provider gitlab \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --project-id 278964\n</code></pre>"},{"location":"CONNECTOR_USAGE/#analyze-local-repository","title":"Analyze Local Repository","text":"<pre><code>python cli.py sync git --provider local \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --repo-path \"/home/user/my-project\"\n</code></pre>"},{"location":"CONNECTOR_USAGE/#batch-process-multiple-repositories","title":"Batch Process Multiple Repositories","text":"<pre><code># GitHub batch processing\npython cli.py sync git --provider github \\\n  --db \"sqlite+aiosqlite:///mergestat.db\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --group myorg \\\n  -s \"myorg/api-*\" \\\n  --batch-size 5 \\\n  --max-repos 20 \\\n  --use-async\n\n# GitLab batch processing\npython cli.py sync git --provider gitlab \\\n  --db \"sqlite+aiosqlite:///mergestat.db\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --group mygroup \\\n  -s \"mygroup/service-*\" \\\n  --batch-size 5 \\\n  --max-repos 20 \\\n  --use-async\n</code></pre>"},{"location":"CONNECTOR_USAGE/#limitations","title":"Limitations","text":""},{"location":"CONNECTOR_USAGE/#github-connector-mode_1","title":"GitHub Connector Mode","text":"<ul> <li>Fetches up to 100 most recent commits</li> <li>Commit stats limited to last 50 commits (API rate limits)</li> <li>Does not fetch blame data (requires per-file API calls)</li> <li>Does not fetch file contents</li> </ul>"},{"location":"CONNECTOR_USAGE/#gitlab-connector-mode_1","title":"GitLab Connector Mode","text":"<ul> <li>Fetches up to 100 most recent commits</li> <li>Commit stats limited to last 50 commits (API rate limits)</li> <li>Stores aggregate stats per commit (not per-file breakdowns)</li> <li>Does not fetch blame data (requires per-file API calls)</li> <li>Does not fetch file contents</li> </ul>"},{"location":"CONNECTOR_USAGE/#rate-limits","title":"Rate Limits","text":"<ul> <li>GitHub: 5,000 requests/hour for authenticated users</li> <li>GitLab: 10 requests/second (self-hosted may vary)</li> </ul> <p>Both connectors include automatic retry with exponential backoff for rate limit handling.</p> <p>Batch modes also coordinate concurrent workers using a shared backoff gate to reduce rate-limit stampedes, and honor server-provided <code>Retry-After</code>/reset delays when available.</p>"},{"location":"CONNECTOR_USAGE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"CONNECTOR_USAGE/#connectors-are-not-available","title":"\"Connectors are not available\"","text":"<p>The connectors require additional dependencies. Install them:</p> <pre><code>pip install PyGithub python-gitlab\n</code></pre> <p>Or install all requirements:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"CONNECTOR_USAGE/#github-authentication-errors","title":"GitHub Authentication Errors","text":"<p>Ensure your token has the appropriate scopes:</p> <ul> <li><code>repo</code> - Full control of private repositories (REQUIRED for private repos)</li> <li><code>read:org</code> - Read org and team membership (recommended for organization repos)</li> </ul> <p>For private repositories: The <code>repo</code> scope is mandatory. Without it, you'll receive 404 errors when trying to access private repositories.</p> <p>To verify your token scopes:</p> <ol> <li>Go to https://github.com/settings/tokens</li> <li>Find your token and check which scopes are selected</li> <li>If <code>repo</code> is not checked, generate a new token with this scope</li> </ol>"},{"location":"CONNECTOR_USAGE/#gitlab-authentication-errors","title":"GitLab Authentication Errors","text":"<p>Ensure your token has the appropriate scopes:</p> <ul> <li><code>read_api</code> - Read access to API (REQUIRED for private projects)</li> <li><code>read_repository</code> - Read repository data (REQUIRED for private projects)</li> </ul> <p>For private projects: Both scopes are mandatory. Without them, you'll receive authentication or permission errors.</p> <p>To verify your token permissions:</p> <ol> <li>Go to your GitLab instance Settings \u2192 Access Tokens</li> <li>Review the token's scopes</li> <li>If needed, create a new token with <code>read_api</code> and <code>read_repository</code> scopes</li> </ol>"},{"location":"CONNECTOR_USAGE/#finding-repositoryproject-ids","title":"Finding Repository/Project IDs","text":"<p>GitHub: Use owner/repo format (e.g., <code>torvalds/linux</code>) GitLab: Use the numeric project ID (e.g., <code>278964</code>)</p>"},{"location":"CONNECTOR_USAGE/#integration-with-existing-workflows","title":"Integration with Existing Workflows","text":"<p>The connector modes integrate seamlessly with the existing storage system:</p> <ul> <li>All data is stored in the same database schema</li> <li>Repository metadata tagged with source (<code>github</code> or <code>gitlab</code>)</li> <li>Can mix local and remote repositories in the same database</li> <li>Query data the same way regardless of source</li> </ul>"},{"location":"CONNECTOR_USAGE/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Local mode: Best for comprehensive analysis (files, blame, full history)</li> <li>GitHub/GitLab modes: Faster for basic commits and stats</li> <li>API rate limits: GitHub and GitLab have rate limits; local mode has none</li> <li>Network dependency: Connector modes require internet access</li> </ul> <p>Choose the appropriate mode based on your needs:</p> <ul> <li>Use local mode for complete repository analysis</li> <li>Use connector modes for quick commit and stats analysis without cloning</li> </ul>"},{"location":"PRIVATE_REPO_TESTING/","title":"Testing Private Repository Access","text":"<p>This document explains how to verify that the GitHub and GitLab connectors work correctly with private repositories and projects.</p>"},{"location":"PRIVATE_REPO_TESTING/#overview","title":"Overview","text":"<p>The connectors fully support private repositories when provided with tokens that have the appropriate permissions:</p> <ul> <li>GitHub: Requires <code>repo</code> scope for private repository access</li> <li>GitLab: Requires <code>read_api</code> and <code>read_repository</code> scopes for private project access</li> </ul>"},{"location":"PRIVATE_REPO_TESTING/#quick-test","title":"Quick Test","text":"<p>Run the example script to verify private repository access:</p> <pre><code># Set up GitHub credentials\nexport GITHUB_TOKEN=ghp_your_token_with_repo_scope\nexport GITHUB_PRIVATE_REPO=your-username/your-private-repo\n\n# Set up GitLab credentials\nexport GITLAB_TOKEN=glpat_your_token\nexport GITLAB_PRIVATE_PROJECT=your-group/your-private-project\n\n# Run the test\npython examples/private_repo_example.py\n</code></pre>"},{"location":"PRIVATE_REPO_TESTING/#detailed-testing","title":"Detailed Testing","text":""},{"location":"PRIVATE_REPO_TESTING/#github-private-repository-testing","title":"GitHub Private Repository Testing","text":""},{"location":"PRIVATE_REPO_TESTING/#1-create-a-github-token","title":"1. Create a GitHub Token","text":"<ol> <li>Go to https://github.com/settings/tokens</li> <li>Click \"Generate new token\" \u2192 \"Generate new token (classic)\"</li> <li>Give it a descriptive name: \"MergeStat Private Repo Test\"</li> <li>Select scopes:</li> <li>\u2705 repo (Full control of private repositories)</li> <li>\u2705 read:org (optional, for organization repos)</li> <li>Click \"Generate token\" and copy it immediately</li> </ol>"},{"location":"PRIVATE_REPO_TESTING/#2-set-environment-variables","title":"2. Set Environment Variables","text":"<pre><code># Your GitHub token (with 'repo' scope)\nexport GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n# Your private repository (format: owner/repo)\nexport GITHUB_PRIVATE_REPO=myusername/my-private-repo\n</code></pre>"},{"location":"PRIVATE_REPO_TESTING/#3-run-tests","title":"3. Run Tests","text":"<p>Option A: Run the example script</p> <pre><code>python examples/private_repo_example.py\n</code></pre> <p>Option B: Run integration tests</p> <pre><code>pytest tests/test_private_repo_access.py::TestGitHubPrivateRepoAccess -v\n</code></pre> <p>Option C: Test with actual data sync</p> <pre><code>python cli.py sync git --provider github \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner myusername \\\n  --repo my-private-repo\n</code></pre>"},{"location":"PRIVATE_REPO_TESTING/#4-verify-success","title":"4. Verify Success","text":"<p>The test should:</p> <ul> <li>\u2705 Successfully list the private repository</li> <li>\u2705 Fetch repository statistics</li> <li>\u2705 Get contributors list</li> <li>\u2705 Access pull requests</li> <li>\u2705 Check rate limit status</li> </ul>"},{"location":"PRIVATE_REPO_TESTING/#gitlab-private-project-testing","title":"GitLab Private Project Testing","text":""},{"location":"PRIVATE_REPO_TESTING/#1-create-a-gitlab-token","title":"1. Create a GitLab Token","text":"<ol> <li>Go to your GitLab instance Settings \u2192 Access Tokens</li> <li>Create a new token:</li> <li>Name: \"MergeStat Private Project Test\"</li> <li>Scopes:<ul> <li>\u2705 read_api (Read API access)</li> <li>\u2705 read_repository (Read repository content)</li> </ul> </li> <li>Click \"Create personal access token\" and copy it</li> </ol>"},{"location":"PRIVATE_REPO_TESTING/#2-set-environment-variables_1","title":"2. Set Environment Variables","text":"<pre><code># Your GitLab token (with required scopes)\nexport GITLAB_TOKEN=glpat-xxxxxxxxxxxxxxxxxxxx\n\n# Your private project (format: group/project or numeric ID)\nexport GITLAB_PRIVATE_PROJECT=mygroup/my-private-project\n# OR\nexport GITLAB_PRIVATE_PROJECT=12345\n\n# Optional: Custom GitLab instance URL\nexport GITLAB_URL=https://gitlab.example.com\n</code></pre>"},{"location":"PRIVATE_REPO_TESTING/#3-run-tests_1","title":"3. Run Tests","text":"<p>Option A: Run the example script</p> <pre><code>python examples/private_repo_example.py\n</code></pre> <p>Option B: Run integration tests</p> <pre><code>pytest tests/test_private_repo_access.py::TestGitLabPrivateProjectAccess -v\n</code></pre> <p>Option C: Test with actual data sync</p> <pre><code>python cli.py sync git --provider gitlab \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --project-id 12345\n</code></pre>"},{"location":"PRIVATE_REPO_TESTING/#4-verify-success_1","title":"4. Verify Success","text":"<p>The test should:</p> <ul> <li>\u2705 Successfully access the private project</li> <li>\u2705 Fetch project statistics</li> <li>\u2705 Get contributors list</li> <li>\u2705 Access merge requests</li> <li>\u2705 List accessible projects (including private)</li> </ul>"},{"location":"PRIVATE_REPO_TESTING/#troubleshooting","title":"Troubleshooting","text":""},{"location":"PRIVATE_REPO_TESTING/#github-issues","title":"GitHub Issues","text":"<p>Problem: <code>404 Not Found</code> error</p> <ul> <li>Cause: Token doesn't have access to the repository or lacks <code>repo</code> scope</li> <li>Solution:</li> <li>Verify the repository exists and you have access</li> <li>Check token has <code>repo</code> scope at https://github.com/settings/tokens</li> <li>Generate a new token with correct scopes if needed</li> </ul> <p>Problem: <code>401 Unauthorized</code> error</p> <ul> <li>Cause: Invalid or expired token</li> <li>Solution: Generate a new token</li> </ul> <p>Problem: Rate limit exceeded</p> <ul> <li>Cause: Too many API requests</li> <li>Solution: Wait for rate limit to reset or use a different token</li> </ul>"},{"location":"PRIVATE_REPO_TESTING/#gitlab-issues","title":"GitLab Issues","text":"<p>Problem: <code>404 Not Found</code> or <code>401 Unauthorized</code> error</p> <ul> <li>Cause: Token doesn't have access to the project or lacks required scopes</li> <li>Solution:</li> <li>Verify the project exists and you have access</li> <li>Check token has <code>read_api</code> and <code>read_repository</code> scopes</li> <li>Generate a new token with correct scopes if needed</li> </ul> <p>Problem: Project ID vs Project Path</p> <ul> <li>Cause: Using project path when numeric ID is expected or vice versa</li> <li>Solution:</li> <li>For <code>cli.py sync git --provider gitlab</code>: Use numeric project ID</li> <li>For connector methods: Can use either project name or ID</li> </ul> <p>Problem: Self-hosted GitLab connection issues</p> <ul> <li>Cause: Incorrect URL or network issues</li> <li>Solution: Verify <code>GITLAB_URL</code> is correct and accessible</li> </ul>"},{"location":"PRIVATE_REPO_TESTING/#verifying-token-scopes","title":"Verifying Token Scopes","text":""},{"location":"PRIVATE_REPO_TESTING/#github-token-verification","title":"GitHub Token Verification","text":"<pre><code>from connectors import GitHubConnector\n\ntoken = \"ghp_your_token\"\nconnector = GitHubConnector(token=token)\n\n# If this works, token is valid\nrate_limit = connector.get_rate_limit()\nprint(f\"Token is valid. Rate limit: {rate_limit['remaining']}/{rate_limit['limit']}\")\n\n# If you can list private repos, token has 'repo' scope\nrepos = connector.list_repositories(max_repos=5)\nprint(f\"Can access {len(repos)} repositories\")\n\nconnector.close()\n</code></pre>"},{"location":"PRIVATE_REPO_TESTING/#gitlab-token-verification","title":"GitLab Token Verification","text":"<pre><code>from connectors import GitLabConnector\n\ntoken = \"glpat_your_token\"\nconnector = GitLabConnector(url=\"https://gitlab.com\", private_token=token)\n\n# If this works, token is valid\ntry:\n    projects = connector.list_projects(max_projects=5)\n    print(f\"Token is valid. Can access {len(projects)} projects\")\nexcept Exception as e:\n    print(f\"Token validation failed: {e}\")\n\nconnector.close()\n</code></pre>"},{"location":"PRIVATE_REPO_TESTING/#best-practices","title":"Best Practices","text":"<ol> <li>Never commit tokens to version control</li> <li>Use environment variables</li> <li>Add tokens to <code>.gitignore</code></li> <li> <p>Use secret management tools in production</p> </li> <li> <p>Use minimal required scopes</p> </li> <li>GitHub: <code>repo</code> for private repos, no scope needed for public</li> <li> <p>GitLab: <code>read_api</code> + <code>read_repository</code> for private projects</p> </li> <li> <p>Rotate tokens regularly</p> </li> <li>Generate new tokens periodically</li> <li> <p>Revoke old tokens when no longer needed</p> </li> <li> <p>Test with both public and private repositories</p> </li> <li>Ensure your setup works for both cases</li> <li> <p>Verify error handling for insufficient permissions</p> </li> <li> <p>Monitor rate limits</p> </li> <li>Check rate limit status regularly</li> <li>Implement backoff strategies for production use</li> </ol>"},{"location":"PRIVATE_REPO_TESTING/#integration-tests","title":"Integration Tests","text":"<p>Run the full test suite:</p> <pre><code># Run all private repo tests\npytest tests/test_private_repo_access.py -v\n\n# Run specific test classes\npytest tests/test_private_repo_access.py::TestGitHubPrivateRepoAccess -v\npytest tests/test_private_repo_access.py::TestGitLabPrivateProjectAccess -v\n\n# Skip integration tests in CI/CD\nexport SKIP_INTEGRATION_TESTS=1\npytest tests/test_private_repo_access.py -v\n</code></pre>"},{"location":"PRIVATE_REPO_TESTING/#cicd-considerations","title":"CI/CD Considerations","text":"<p>When running tests in CI/CD pipelines:</p> <ol> <li>Store tokens as secrets</li> <li>GitHub Actions: Use repository secrets</li> <li>GitLab CI: Use CI/CD variables (masked)</li> <li> <p>Other CI: Use secure secret storage</p> </li> <li> <p>Skip integration tests by default</p> </li> </ol> <pre><code># GitHub Actions example\n- name: Run tests\n  env:\n    SKIP_INTEGRATION_TESTS: 1\n  run: pytest\n</code></pre> <ol> <li>Optional: Run integration tests with secrets</li> </ol> <pre><code># Only run on main branch or with specific label\n- name: Run integration tests\n  if: github.ref == 'refs/heads/main'\n  env:\n    GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n    GITHUB_PRIVATE_REPO: ${{ secrets.TEST_PRIVATE_REPO }}\n  run: pytest tests/test_private_repo_access.py -v\n</code></pre>"},{"location":"PRIVATE_REPO_TESTING/#support","title":"Support","text":"<p>If you encounter issues:</p> <ol> <li>Check this documentation for troubleshooting steps</li> <li>Verify your token scopes and permissions</li> <li>Review the connector documentation in <code>connectors/README.md</code></li> <li>Check example scripts in <code>examples/</code> directory</li> <li>Review test files for usage patterns</li> </ol>"},{"location":"architecture/","title":"Architecture","text":"<p>The project follows a pipeline-style architecture that separates data collection, processing, storage, and analysis.</p>"},{"location":"architecture/#pipeline-stages","title":"Pipeline stages","text":"<ol> <li>Connectors (<code>connectors/</code>)</li> <li>Fetch raw data from providers (GitHub, GitLab, Jira).</li> <li>Processors (<code>processors/</code>)</li> <li>Normalize and enrich connector payloads.</li> <li>Storage (<code>storage.py</code>, <code>models/</code>)</li> <li>Persist processed data into PostgreSQL, ClickHouse, MongoDB, or SQLite.</li> <li>Metrics (<code>metrics/</code>)</li> <li>Compute high-level metrics like throughput, cycle time, rework, and predictability.</li> <li>Visualization (<code>grafana/</code>)</li> <li>Provision dashboards for exploration and reporting.</li> </ol>"},{"location":"architecture/#storage-backends","title":"Storage backends","text":"<ul> <li>PostgreSQL for relational storage with Alembic migrations.</li> <li>ClickHouse for analytics-heavy queries.</li> <li>MongoDB for document storage.</li> <li>SQLite for local development.</li> </ul>"},{"location":"architecture/#cli-entry-points","title":"CLI entry points","text":"<p>The CLI is implemented with argparse in <code>cli.py</code> and orchestrates sync, metrics, and Grafana workflows.</p>"},{"location":"cli/","title":"CLI","text":"<p>The CLI is the primary way to run sync jobs, compute metrics, and manage dashboards.</p>"},{"location":"cli/#common-commands","title":"Common commands","text":""},{"location":"cli/#sync-local-git-data","title":"Sync local Git data","text":"<pre><code>python cli.py sync git --provider local --db \"&lt;DB_CONN&gt;\" --repo-path /path/to/repo\n</code></pre>"},{"location":"cli/#sync-teams","title":"Sync teams","text":"<pre><code>python cli.py sync teams --provider config --db \"&lt;DB_CONN&gt;\" --path /path/to/teams.yml\n</code></pre>"},{"location":"cli/#sync-work-items","title":"Sync work items","text":"<pre><code>python cli.py sync work-items --provider github --auth \"$GITHUB_TOKEN\" -s \"org/*\" --db \"&lt;DB_CONN&gt;\"\n</code></pre>"},{"location":"cli/#metrics","title":"Metrics","text":"<pre><code>python cli.py metrics daily --db \"&lt;DB_CONN&gt;\"\npython cli.py metrics complexity --repo-path . -s \"*\" --db \"&lt;DB_CONN&gt;\"\n</code></pre>"},{"location":"cli/#fixtures","title":"Fixtures","text":"<pre><code>python cli.py fixtures generate --db \"&lt;DB_CONN&gt;\" --days 30\n</code></pre>"},{"location":"cli/#grafana","title":"Grafana","text":"<pre><code>python cli.py grafana up\n</code></pre>"},{"location":"cli/#flags-and-overrides","title":"Flags and overrides","text":"<p>CLI flags override environment variables. Use <code>--db</code> or <code>DATABASE_URI</code> to target a specific database.</p>"},{"location":"getting-started/","title":"Getting started","text":""},{"location":"getting-started/#install","title":"Install","text":"<p>If you are using the project as a package:</p> <pre><code>pip install dev-health-ops\n</code></pre> <p>If you are working from source:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/#docs-site","title":"Docs site","text":"<pre><code>pip install -r requirements-docs.txt\nmkdocs serve\n</code></pre>"},{"location":"getting-started/#quick-start","title":"Quick start","text":""},{"location":"getting-started/#sync-a-local-repository","title":"Sync a local repository","text":"<pre><code>python cli.py sync git --provider local --db \"&lt;DB_CONN&gt;\" --repo-path /path/to/repo\n</code></pre>"},{"location":"getting-started/#sync-work-items-from-github","title":"Sync work items from GitHub","text":"<pre><code>python cli.py sync work-items --provider github --auth \"$GITHUB_TOKEN\" -s \"org/*\" --db \"&lt;DB_CONN&gt;\"\n</code></pre>"},{"location":"getting-started/#compute-daily-metrics","title":"Compute daily metrics","text":"<pre><code>python cli.py metrics daily --db \"&lt;DB_CONN&gt;\"\n</code></pre>"},{"location":"getting-started/#bring-up-grafana-dashboards","title":"Bring up Grafana dashboards","text":"<pre><code>python cli.py grafana up\n</code></pre>"},{"location":"getting-started/#environment-notes","title":"Environment notes","text":"<p>CLI flags override environment variables. Common env vars:</p> <ul> <li><code>DATABASE_URI</code></li> <li><code>SECONDARY_DATABASE_URI</code></li> <li><code>GITHUB_TOKEN</code></li> <li><code>GITLAB_TOKEN</code></li> <li><code>REPO_PATH</code></li> </ul>"},{"location":"grafana/","title":"Grafana","text":"<p>This repo ships Grafana provisioning for ClickHouse dashboards under <code>grafana/</code>.</p>"},{"location":"grafana/#start-grafana-clickhouse-dev","title":"Start Grafana + ClickHouse (dev)","text":"<p>From the repo root:</p> <pre><code>python cli.py grafana up\n</code></pre> <ul> <li>Grafana: <code>http://localhost:3000</code> (default <code>admin</code> / <code>admin</code>)</li> <li>ClickHouse HTTP: <code>http://localhost:8123</code></li> </ul>"},{"location":"grafana/#datasource-provisioning","title":"Datasource provisioning","text":"<p>Provisioning file: - <code>grafana/datasources/clickhouse.yaml</code></p> <p>It reads connection settings from environment variables inside the Grafana container: - <code>CLICKHOUSE_HOST</code> (defaults to <code>clickhouse</code> in the provided docker compose) - <code>CLICKHOUSE_PORT</code> (defaults to <code>8123</code>) - <code>CLICKHOUSE_DB</code> - <code>CLICKHOUSE_USER</code> - <code>CLICKHOUSE_PASSWORD</code></p>"},{"location":"grafana/#dashboards","title":"Dashboards","text":"<p>Dashboards are provisioned automatically into the \u201cDeveloper Health\u201d folder: - Repo Health: <code>grafana/dashboards/repo_health.json</code> - Code Hotspots: <code>grafana/dashboards/code_hotspots.json</code> - Work Tracking: <code>grafana/dashboards/work_tracking.json</code> - Advanced Work Tracking: <code>grafana/dashboards/advanced_work_tracking.json</code> - Collaboration: <code>grafana/dashboards/collaboration.json</code> - Quality &amp; Risk: <code>grafana/dashboards/quality_risk.json</code> - Well-being (team-level): <code>grafana/dashboards/wellbeing.json</code></p> <p>The Work Tracking dashboard supports filtering by <code>provider</code>, <code>team_id</code>, and <code>work_scope_id</code> (Jira project key / GitHub repo or project board / GitLab project path).</p>"},{"location":"grafana/#expected-clickhouse-tables","title":"Expected ClickHouse tables","text":"<p>Git facts (synced via the CLI, e.g., <code>python cli.py sync ...</code>): - <code>repos</code> - <code>git_commits</code> - <code>git_commit_stats</code> - <code>git_pull_requests</code></p> <p>Derived metrics (computed by <code>scripts/compute_metrics_daily.py</code>): - <code>repo_metrics_daily</code> - <code>user_metrics_daily</code> - <code>commit_metrics</code> - <code>team_metrics_daily</code> - <code>work_item_metrics_daily</code> - <code>work_item_user_metrics_daily</code> - <code>work_item_cycle_times</code></p>"},{"location":"grafana/#populate-data","title":"Populate data","text":"<p>1) Sync git data into ClickHouse:</p> <pre><code>python cli.py sync git --provider local --db \"clickhouse://localhost:8123/default\" --repo-path .\n</code></pre> <p>2) Compute derived metrics:</p> <pre><code>python cli.py sync work-items --provider all --date 2025-02-01 --backfill 30 --db \"clickhouse://localhost:8123/default\"\npython cli.py metrics daily --date 2025-02-01 --backfill 30 --db \"clickhouse://localhost:8123/default\"\n</code></pre> <p>Work tracking providers require credentials; see <code>docs/task_trackers.md</code>.</p>"},{"location":"grafana/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>The provisioned dashboards use repo names (e.g. <code>org/repo</code>) in the Repo picker and support multi-select + \u201cAll\u201d. Repo filtering is applied by resolving selected repo names to <code>repos.id</code> in ClickHouse.</li> </ul>"},{"location":"metrics-inventory/","title":"Metrics Inventory","text":"<p>This inventory tracks the implementation status of all metrics defined in the <code>dev-health-ops</code> project. Work item metrics assume provider data has been synced via <code>python cli.py sync work-items ...</code> (use <code>-s</code> to filter repos; <code>--auth</code> to override GitHub/GitLab tokens when needed; tags/settings filtering planned).</p>"},{"location":"metrics-inventory/#1-delivery-velocity-flow-dora","title":"1. Delivery &amp; Velocity (Flow &amp; DORA)","text":"Metric Status Source/Implementation Cycle Time (PR) [x] metrics/compute.py Coding Time [x] metrics/compute.py Review Time [x] metrics/compute.py Pickup Time [x] metrics/compute.py Deploy Time [x] GitHub/GitLab: Synced via <code>sync deployments --provider github|gitlab</code>. Computed in compute_deployments.py Deployment Frequency [x] Derived from deployments table + fallback to <code>prs_merged</code> Lead Time for Changes [x] DORA metric in compute.py MTTR [x] Calculated from Bug work items + Incident records Change Failure Rate [x] Calculated from \"revert\" PRs in compute.py Work Item Cycle Time [x] metrics/compute_work_items.py Work Item Lead Time [x] metrics/compute_work_items.py WIP Count/Age [x] metrics/compute_work_items.py Flow Efficiency [x] metrics/compute_work_items.py CI/CD Pipeline Metrics [x] GitHub/GitLab: Synced via <code>sync cicd --provider github|gitlab</code>. Computed in compute_cicd.py Incident MTTR [x] GitHub/GitLab: Issues labeled 'incident'. Computed in compute_incidents.py"},{"location":"metrics-inventory/#2-code-quality-risk","title":"2. Code Quality &amp; Risk","text":"Metric Status Source/Implementation Churn [x] Total LOC touched in compute.py Rework Rate (30-day) [x] Computed in quality.py and wired in job_daily.py File Hotspot Score [x] metrics/hotspots.py combining churn and contributor count PR Size [x] <code>avg_commit_size_loc</code> and <code>large_pr_ratio</code> in compute.py PR Rework Ratio [x] <code>pr_rework_ratio</code> in compute.py (based on changes requested) Defect Introduction Rate [x] metrics/compute_work_items.py (bugs created vs items closed) Single Owner File Ratio [x] Computed in quality.py and wired in job_daily.py Cyclomatic Complexity [x] Computed via <code>metrics complexity</code> using <code>radon</code>. Stored in <code>file_complexity_snapshots</code> (per-file) and <code>repo_complexity_daily</code>; loaded in <code>metrics daily</code> via <code>store.get_complexity_snapshots()</code>."},{"location":"metrics-inventory/#3-investment-portfolio","title":"3. Investment &amp; Portfolio","text":"Metric Status Source/Implementation Investment Allocation [x] Classified via <code>InvestmentClassifier</code> in <code>analytics/investment.py</code>. Rules in <code>config/investment_areas.yaml</code>. Project Stream Churn [x] LOC churn aggregated by project stream and investment area. Strategic vs KTLO % [x] Derived from Investment Allocation daily rollups."},{"location":"metrics-inventory/#4-ic-metrics-landscape-new","title":"4. IC Metrics &amp; Landscape (New)","text":"Metric Status Source/Implementation IC Throughput [x] <code>delivery_units</code> (PRs merged + Items completed) in metrics/compute_ic.py. Relies on <code>identity_mapping.yaml</code>. IC Churn [x] <code>loc_touched</code> (Additions + Deletions) in metrics/compute_ic.py IC Cycle Time [x] <code>cycle_p50_hours</code> (PR median cycle time) in metrics/compute_ic.py IC Active WIP [x] <code>work_items_active</code> (Items in progress/review/blocked) in metrics/compute_ic.py Landscape: Churn vs Throughput [x] 2D map with team percentile normalization in metrics/compute_ic.py Landscape: Cycle vs Throughput [x] 2D map with team percentile normalization in metrics/compute_ic.py Landscape: WIP vs Throughput [x] 2D map with team percentile normalization in metrics/compute_ic.py"},{"location":"metrics-inventory/#4-collaboration-team-dynamics","title":"4. Collaboration &amp; Team Dynamics","text":"Metric Status Source/Implementation Review Responsiveness [x] <code>pr_pickup_time_p50_hours</code> in compute.py Review Load [x] <code>reviews_given</code> in compute.py Review Reciprocity [x] Ratio of reviews given vs received in compute.py Bus Factor [x] metrics/knowledge.py (Truck Factor: min authors for 50% churn) Knowledge Distribution [x] metrics/knowledge.py (Gini Coefficient of churn ownership)"},{"location":"metrics-inventory/#5-developer-well-being-cognitive-load","title":"5. Developer Well-being &amp; Cognitive Load","text":"Metric Status Source/Implementation Late-night Activity [x] metrics/compute_wellbeing.py Weekend Activity [x] metrics/compute_wellbeing.py Burnout Risk Score [x] Combined after-hours/weekend activity per developer Flow Score [ ] Requires IDE telemetry Cognitive Load [ ] Requires IDE telemetry"},{"location":"metrics-inventory/#6-systemic-process-health","title":"6. Systemic &amp; Process Health","text":"Metric Status Source/Implementation Bottleneck Index [x] metrics/compute_work_item_state_durations.py WIP Congestion [x] metrics/compute_work_items.py (WIP / Weekly Throughput) Predictability Index [x] metrics/compute_work_items.py (Completion Rate: Completed / (Completed + WIP)) Pipeline Success Rate [x] metrics/compute_cicd.py (GitHub only, GitLab pending) Deploy Success Rate [x] metrics/compute_deployments.py"},{"location":"metrics/","title":"Developer Health Metrics (v2)","text":"<p>This repository computes daily \u201cdeveloper health\u201d metrics from:</p> <ul> <li>synced Git facts (<code>git_commits</code>, <code>git_commit_stats</code>, <code>git_pull_requests</code>)</li> <li>optional work tracking data (Jira issues, GitHub issues/Projects v2, GitLab issues)</li> </ul> <p>Derived time series are written to ClickHouse (preferred) and/or MongoDB (optional) for Grafana.</p> <p>All timestamps are treated as UTC (ClickHouse stores DateTime in UTC; Mongo stores naive datetimes as UTC by convention).</p> <p>Important: Jira (and other issue trackers) is not a replacement for pull request data. Work items are used for planning/throughput/WIP metrics, while PR metrics (cycle time, merge frequency, review metrics when available) come from the Git provider sync.</p>"},{"location":"metrics/#source-data","title":"Source Data","text":"<p>Git facts must already exist in the backend you point the job at:</p> <ul> <li><code>git_commits</code></li> <li><code>git_commit_stats</code></li> <li><code>git_pull_requests</code></li> </ul> <p>Work tracking facts should be synced from provider APIs via <code>python cli.py sync work-items ...</code>. See <code>docs/task_trackers.md</code> for configuration. (<code>metrics daily --provider ...</code> still exists as a convenience/backward-compatible path.)</p> <p>CI/CD pipeline facts are synced from GitHub/GitLab via the <code>sync cicd</code> job:</p> <pre><code>python cli.py sync cicd --provider github --db \"&lt;DB_CONN&gt;\" --auth \"$GITHUB_TOKEN\" --owner \"&lt;org&gt;\" --repo \"&lt;repo&gt;\"\npython cli.py sync cicd --provider gitlab --db \"&lt;DB_CONN&gt;\" --auth \"$GITLAB_TOKEN\" --gitlab-url \"&lt;URL&gt;\" --project-id &lt;ID&gt;\n</code></pre> <p>Deployments and incident facts are synced via their own jobs:</p> <pre><code>python cli.py sync deployments --provider github --db \"&lt;DB_CONN&gt;\" --auth \"$GITHUB_TOKEN\" --owner \"&lt;org&gt;\" --repo \"&lt;repo&gt;\"\npython cli.py sync incidents --provider github --db \"&lt;DB_CONN&gt;\" --auth \"$GITHUB_TOKEN\" --owner \"&lt;org&gt;\" --repo \"&lt;repo&gt;\"\npython cli.py sync deployments --provider gitlab --db \"&lt;DB_CONN&gt;\" --auth \"$GITLAB_TOKEN\" --gitlab-url \"&lt;URL&gt;\" --project-id &lt;ID&gt;\npython cli.py sync incidents --provider gitlab --db \"&lt;DB_CONN&gt;\" --auth \"$GITLAB_TOKEN\" --gitlab-url \"&lt;URL&gt;\" --project-id &lt;ID&gt;\n</code></pre> <p>Note: Jira Ops/Service Desk incidents are planned once project-to-repo or deployment mapping is defined.</p>"},{"location":"metrics/#derived-tables-collections","title":"Derived Tables / Collections","text":""},{"location":"metrics/#git-repo-user","title":"Git / Repo / User","text":"<ul> <li><code>repo_metrics_daily</code></li> <li><code>user_metrics_daily</code></li> <li><code>commit_metrics</code></li> </ul>"},{"location":"metrics/#work-tracking","title":"Work Tracking","text":"<ul> <li><code>work_item_metrics_daily</code> (daily aggregates, by provider/team/repo)</li> <li><code>work_item_user_metrics_daily</code> (daily aggregates, by provider/user/team)</li> <li><code>work_item_cycle_times</code> (per-work-item fact rows for completed items)</li> </ul>"},{"location":"metrics/#team-well-being-team-level-only","title":"Team Well-being (team-level only)","text":"<ul> <li><code>team_metrics_daily</code></li> </ul>"},{"location":"metrics/#metric-definitions","title":"Metric Definitions","text":""},{"location":"metrics/#commit-size-bucketing","title":"Commit size bucketing","text":"<ul> <li><code>total_loc = additions + deletions</code> (summed from <code>git_commit_stats</code>)</li> <li>bucket:</li> <li><code>small</code>: <code>total_loc &lt;= 50</code></li> <li><code>medium</code>: <code>51..300</code></li> <li><code>large</code>: <code>&gt; 300</code></li> </ul>"},{"location":"metrics/#pr-cycle-time","title":"PR cycle time","text":"<p>For PRs with <code>merged_at</code> on day D:</p> <ul> <li><code>cycle_time_hours = (merged_at - created_at) / 3600</code></li> <li>Distribution fields (per repo/day and user/day):</li> <li><code>median_pr_cycle_hours</code> (p50)</li> <li><code>pr_cycle_p75_hours</code></li> <li><code>pr_cycle_p90_hours</code></li> </ul>"},{"location":"metrics/#large-change-thresholds","title":"Large change thresholds","text":"<ul> <li>Large commits: <code>total_loc &gt; 300</code></li> <li>Large PRs: <code>additions + deletions &gt;= LARGE_PR_LOC_THRESHOLD</code> (default: <code>1000</code>)</li> <li>Note: PR size facts are best-effort and may be missing depending on the connector.</li> </ul>"},{"location":"metrics/#daily-user-metrics-user_metrics_daily","title":"Daily user metrics (<code>user_metrics_daily</code>)","text":"<p>Keyed by <code>(repo_id, author_email, day)</code> where <code>author_email</code> falls back to <code>author_name</code> when email is missing.</p> <ul> <li>Commits: counts, LOC added/deleted, distinct files changed (union across the day), large commits, avg commit size</li> <li>PRs: authored (created that day), merged (merged that day), avg/p50/p75/p90 PR cycle hours (for PRs merged that day)</li> <li>Collaboration (nullable when not available):</li> <li><code>pr_pickup_time_p50_hours</code>: PR created \u2192 first interaction (comment/review)</li> <li><code>pr_first_review_p50_hours</code> / <code>p90</code>: PR created \u2192 first review</li> <li><code>pr_review_time_p50_hours</code>: first review \u2192 merge</li> <li><code>reviews_given</code>, <code>changes_requested_given</code>: counts of review submissions for the day</li> </ul> <p>Null behavior:</p> <ul> <li>if review/comment facts are unavailable, pickup/first-review/review-time fields remain <code>NULL</code></li> </ul>"},{"location":"metrics/#daily-repo-metrics-repo_metrics_daily","title":"Daily repo metrics (<code>repo_metrics_daily</code>)","text":"<p>Keyed by <code>(repo_id, day)</code>.</p> <ul> <li>Commits: count, LOC touched, avg size, large commit ratio</li> <li>PRs: merged count, p50/p75/p90 PR cycle hours</li> <li>Quality (best-effort): <code>large_pr_ratio</code>, <code>pr_rework_ratio</code> (requires PR size and review facts)</li> <li>Knowledge:</li> <li><code>bus_factor</code>: smallest number of developers accounting for \u2265 50% of code churn</li> <li><code>code_ownership_gini</code>: Gini coefficient of code contribution inequality (0..1)</li> </ul>"},{"location":"metrics/#optional-per-commit-metrics-commit_metrics","title":"Optional per-commit metrics (<code>commit_metrics</code>)","text":"<p>Keyed by <code>(repo_id, day, author_email, commit_hash)</code>.</p>"},{"location":"metrics/#work-item-normalization-cycle-times","title":"Work item normalization + cycle times","text":"<p>Work items are normalized into a unified <code>WorkItem</code> abstraction (<code>models/work_items.py</code>).</p> <p>Best-effort fields:</p> <ul> <li><code>started_at</code>: first transition into normalized <code>in_progress</code></li> <li><code>completed_at</code>: first transition into normalized <code>done</code> or <code>canceled</code></li> </ul> <p>Time metrics:</p> <ul> <li><code>lead_time_hours = completed_at - created_at</code> (when completed)</li> <li><code>cycle_time_hours = completed_at - started_at</code> (when started + completed)</li> </ul> <p>Null behavior:</p> <ul> <li>items missing <code>started_at</code> are excluded from cycle-time distributions</li> </ul>"},{"location":"metrics/#work-tracking-daily-metrics-work_item_metrics_daily","title":"Work tracking daily metrics (<code>work_item_metrics_daily</code>)","text":"<p>Keyed by <code>(day, provider, team_id, work_scope_id)</code>.</p> <p><code>work_scope_id</code> is provider-native and is used to avoid assuming a repo UUID exists for work items:</p> <ul> <li>Jira: Jira project key (e.g. <code>ABC</code>)</li> <li>GitHub: repository full name (e.g. <code>owner/repo</code>) when sourced from issues; Projects v2 uses <code>ghprojv2:&lt;org&gt;#&lt;number&gt;</code></li> <li>GitLab: project full path (e.g. <code>group/project</code>)</li> <li>Throughput: <code>items_started</code>, <code>items_completed</code></li> <li>Ownership gap: <code>items_completed_unassigned</code> (subset of completed items that had no assignee at completion time)</li> <li>WIP: <code>wip_count_end_of_day</code></li> <li>Cycle/lead time distributions (nullable if no samples): p50/p90</li> <li>WIP age distributions (nullable if no WIP samples): p50/p90</li> <li><code>bug_completed_ratio</code>: completed bugs / total completed</li> <li><code>story_points_completed</code>: sum of story points completed (Jira only, when configured)</li> <li><code>predictability_score</code>: Completion Rate = <code>items_completed / (items_completed + wip_count_end_of_day)</code></li> </ul>"},{"location":"metrics/#work-item-facts-work_item_cycle_times","title":"Work item facts (<code>work_item_cycle_times</code>)","text":"<p>Keyed by <code>(provider, work_item_id)</code>; stored as ClickHouse <code>ReplacingMergeTree</code> by <code>computed_at</code> and as Mongo upserts.</p>"},{"location":"metrics/#work-item-time-in-state-work_item_state_durations_daily","title":"Work item time-in-state (<code>work_item_state_durations_daily</code>)","text":"<p>Keyed by <code>(day, provider, work_scope_id, team_id, status)</code> and computed from provider status transitions (Jira changelog when available).</p> <p>Null behavior:</p> <ul> <li>items without status transition history contribute no rows</li> </ul>"},{"location":"metrics/#unassigned-work","title":"Unassigned work","text":"<p>If a work item has no assignee, daily rollups:</p> <ul> <li>count it under <code>team_id=''</code> (shown as \u201cunassigned\u201d in dashboards)</li> <li>emit a <code>work_item_user_metrics_daily</code> row with <code>user_identity='unassigned'</code> so you can chart \u201cclosed unassigned\u201d work over time</li> </ul>"},{"location":"metrics/#team-well-being-team-level-only_1","title":"Team well-being (team-level only)","text":"<p>Computed from commits (deduplicated by commit hash) and a team mapping.</p> <ul> <li><code>after_hours_commit_ratio</code>: commits outside business hours on weekdays</li> <li><code>weekend_commit_ratio</code>: commits on weekends</li> </ul> <p>Configuration:</p> <ul> <li><code>BUSINESS_TIMEZONE</code> (default: <code>UTC</code>)</li> <li><code>BUSINESS_HOURS_START</code> (default: <code>9</code>)</li> <li><code>BUSINESS_HOURS_END</code> (default: <code>17</code>)</li> </ul>"},{"location":"metrics/#storage-targets","title":"Storage Targets","text":""},{"location":"metrics/#clickhouse-tables","title":"ClickHouse (tables)","text":"<p>Tables are created automatically if missing:</p> <ul> <li><code>repo_metrics_daily</code></li> <li><code>user_metrics_daily</code></li> <li><code>commit_metrics</code></li> <li><code>team_metrics_daily</code></li> <li><code>work_item_metrics_daily</code></li> <li><code>work_item_user_metrics_daily</code></li> <li><code>work_item_cycle_times</code></li> </ul> <p>They are <code>MergeTree</code> tables partitioned by <code>toYYYYMM(day)</code> and ordered by the natural keys for Grafana queries.</p>"},{"location":"metrics/#clickhouse-query-notes","title":"ClickHouse query notes","text":"<p>These derived tables are append-only with a <code>computed_at</code> version. To query the latest value per key, use <code>argMax(&lt;metric&gt;, computed_at)</code> grouped by the key columns. If you need a daily total, do it in two steps (no nested aggregates):</p> <pre><code>SELECT\n  day,\n  sum(items_completed_unassigned_latest) AS items_completed_unassigned\nFROM\n(\n  SELECT\n    day,\n    provider,\n    work_scope_id,\n    team_id,\n    argMax(items_completed_unassigned, computed_at) AS items_completed_unassigned_latest\n  FROM work_item_metrics_daily\n  WHERE provider = 'jira'\n  GROUP BY day, provider, work_scope_id, team_id\n)\nGROUP BY day\nORDER BY day;\n</code></pre> <p>Re-computations are append-only and distinguished by <code>computed_at</code>. To query the latest metrics for a key/day, use <code>argMax(..., computed_at)</code> in ClickHouse.</p>"},{"location":"metrics/#mongodb-collections","title":"MongoDB (collections)","text":"<p>Collections are created automatically:</p> <ul> <li><code>repo_metrics_daily</code></li> <li><code>user_metrics_daily</code></li> <li><code>commit_metrics</code></li> <li><code>team_metrics_daily</code></li> <li><code>work_item_metrics_daily</code></li> <li><code>work_item_user_metrics_daily</code></li> <li><code>work_item_cycle_times</code></li> </ul> <p>Documents use stable compound <code>_id</code> keys and are written via upserts, so recomputation is safe.</p>"},{"location":"metrics/#sqlite-tables","title":"SQLite (tables)","text":"<p>Tables are created automatically in the same <code>.db</code> file:</p> <ul> <li><code>repo_metrics_daily</code></li> <li><code>user_metrics_daily</code></li> <li><code>commit_metrics</code></li> <li><code>team_metrics_daily</code></li> <li><code>work_item_metrics_daily</code></li> <li><code>work_item_user_metrics_daily</code></li> <li><code>work_item_cycle_times</code></li> </ul>"},{"location":"metrics/#running-the-daily-job","title":"Running The Daily Job","text":"<p>The job reads source data from the same backend you point it at (ClickHouse or MongoDB), using the synced tables/collections:</p> <ul> <li><code>git_commits</code></li> <li><code>git_commit_stats</code></li> <li><code>git_pull_requests</code>   It also supports SQLite, reading the same tables and writing metrics tables into the same <code>.db</code> file.</li> </ul>"},{"location":"metrics/#environment-variables","title":"Environment variables","text":"<ul> <li><code>DATABASE_URI</code> (or <code>DATABASE_URL</code>): ClickHouse, MongoDB, SQLite, or PostgreSQL URI for both reading source data and writing derived metrics.</li> <li><code>SECONDARY_DATABASE_URI</code>: Required when running with <code>--sink both</code> to write to a secondary backend.</li> </ul>"},{"location":"metrics/#examples","title":"Examples","text":"<ul> <li>Compute one day (backend inferred from <code>--db</code> or <code>DATABASE_URI</code>):</li> <li><code>python cli.py metrics daily --date 2025-02-01 --db clickhouse://localhost:8123/default</code></li> <li><code>python cli.py metrics daily --date 2025-02-01 --db mongodb://localhost:27017/mergestat</code></li> <li><code>python cli.py metrics daily --date 2025-02-01 --db sqlite:///./mergestat.db</code></li> <li>Compute 7-day backfill ending at a date:</li> <li><code>python cli.py metrics daily --date 2025-02-01 --backfill 7 --db clickhouse://localhost:8123/default</code></li> <li>Filter to one repository:</li> <li><code>python cli.py metrics daily --date 2025-02-01 --repo-id &lt;uuid&gt; --db clickhouse://localhost:8123/default</code></li> <li>Compute git + work item metrics (requires provider credentials; see <code>docs/task_trackers.md</code>):</li> <li><code>python cli.py sync work-items --provider all --date 2025-02-01 --backfill 30 --db clickhouse://localhost:8123/default</code></li> <li><code>python cli.py metrics daily --date 2025-02-01 --backfill 30 --db clickhouse://localhost:8123/default</code></li> </ul>"},{"location":"metrics/#dependencies","title":"Dependencies","text":"<ul> <li>ClickHouse uses <code>clickhouse-connect</code> (already in <code>requirements.txt</code>).</li> <li>MongoDB uses <code>pymongo</code> (available via the <code>motor</code> dependency in <code>requirements.txt</code>).</li> <li>SQLite uses <code>sqlalchemy</code> (already in <code>requirements.txt</code>).</li> </ul>"},{"location":"metrics/#ic-metrics-landscape-v3","title":"IC Metrics &amp; Landscape (v3)","text":"<p>We compute canonical Individual Contributor (IC) metrics and \"Developer Landscape\" maps to visualize patterns in churn, throughput, and work-in-progress.</p> <p>Note: These metrics are designed for identifying signals and patterns, not for ranking individuals.</p>"},{"location":"metrics/#identity-resolution","title":"Identity Resolution","text":"<ul> <li>Users are mapped to a canonical <code>identity_id</code> (preferring email) across providers.</li> <li>Configuration: <code>config/teams.yaml</code> or <code>config/team_mapping.yaml</code>.</li> </ul>"},{"location":"metrics/#ic-metrics-user_metrics_daily","title":"IC Metrics (<code>user_metrics_daily</code>)","text":"<p>Extends the daily user metrics with work tracking and unified throughput signals:</p> <ul> <li><code>identity_id</code>: Canonical user identity.</li> <li><code>loc_touched</code>: Sum of additions + deletions.</li> <li><code>delivery_units</code>: <code>prs_merged</code> + <code>work_items_completed</code>.</li> <li><code>work_items_active</code>: Number of items in progress/review/blocked at end of day.</li> <li><code>cycle_p50_hours</code>: Median cycle time (PRs).</li> </ul>"},{"location":"metrics/#landscape-maps-ic_landscape_rolling_30d","title":"Landscape Maps (<code>ic_landscape_rolling_30d</code>)","text":"<p>Rolling 30-day metrics normalized by team percentiles (0..1). Stored in ClickHouse table <code>ic_landscape_rolling_30d</code>.</p>"},{"location":"metrics/#map-1-churn-vs-throughput","title":"Map 1: Churn vs Throughput","text":"<ul> <li>X: <code>log(churn_loc_30d)</code> (rolling 30d sum of LOC touched)</li> <li>Y: <code>delivery_units_30d</code> (rolling 30d sum of delivery units)</li> </ul>"},{"location":"metrics/#map-2-cycle-time-vs-throughput","title":"Map 2: Cycle Time vs Throughput","text":"<ul> <li>X: <code>log(cycle_p50_30d_hours)</code> (median of daily median PR cycle times over 30d)</li> <li>Y: <code>delivery_units_30d</code></li> </ul>"},{"location":"metrics/#map-3-wip-vs-throughput","title":"Map 3: WIP vs Throughput","text":"<ul> <li>X: <code>wip_max_30d</code> (max active work items over 30d)</li> <li>Y: <code>delivery_units_30d</code></li> </ul> <p>Each coordinate (<code>x_raw</code>, <code>y_raw</code>) is normalized per team into (<code>x_norm</code>, <code>y_norm</code>) representing the percentile rank within the team.</p>"},{"location":"metrics/#code-complexity-repo_complexity_daily","title":"Code Complexity (<code>repo_complexity_daily</code>)","text":"<p>Complexity metrics are computed by scanning local git clones at specific historical references using <code>radon</code>.</p> <ul> <li>Cyclomatic Complexity (CC): Measures the number of linearly independent paths through a program's source code.</li> <li>Metric fields:</li> <li><code>cyclomatic_total</code>: Sum of CC for all functions/classes in the repo.</li> <li><code>cyclomatic_avg</code>: Mean CC per function.</li> <li><code>high_complexity_functions</code>: Count of functions with CC &gt; 15 (configurable).</li> <li><code>very_high_complexity_functions</code>: Count of functions with CC &gt; 25.</li> <li>Backfilling: Uses <code>git checkout</code> (via <code>GitPython</code>) to analyze historical state.</li> </ul>"},{"location":"metrics/#investment-metrics-investment_metrics_daily","title":"Investment Metrics (<code>investment_metrics_daily</code>)","text":"<p>Categorizes engineering effort into investment areas (e.g., \"New Value\", \"Security\", \"Infrastructure\") using a rule-based classifier.</p> <ul> <li>Artifacts Classified:</li> <li>Work Items: Based on labels, components, and title keywords.</li> <li>Commits (Churn): Based on file path patterns (e.g., <code>infra/</code> or <code>tests/</code>).</li> <li>Metric fields:</li> <li><code>investment_area</code>: The assigned category.</li> <li><code>project_stream</code>: A secondary grouping (e.g., \"Project Phoenix\").</li> <li><code>delivery_units</code>: Story points or count of work items completed.</li> <li><code>churn_loc</code>: Sum of additions + deletions associated with the area.</li> <li>Configuration: <code>config/investment_areas.yaml</code> defines the matching rules and priorities.</li> </ul>"},{"location":"project/","title":"Part 1 \u2014 Existing Platforms: What They Measure","text":""},{"location":"project/#system-flow-connect-sync-calculate","title":"System Flow (Connect \u2192 Sync \u2192 Calculate)","text":"<ul> <li>connect: Github, GitLab, IDEs, CI/CD, SCM, Databases</li> <li>sync: code events, PRs, reviews, issues, deployments</li> <li>calculate: metrics, scores, trends, risk indicators, health signals</li> </ul>"},{"location":"project/#linearb-flow-team-efficiency","title":"LinearB \u2014 Flow &amp; Team Efficiency","text":"<p>Focus: flow, predictability, PR health, resource allocation.</p> <p>Metrics</p> <ul> <li> <p>Cycle Time</p> </li> <li> <p>Coding Time</p> </li> <li>Pickup Time</li> <li>Review Time</li> <li> <p>Deploy Time</p> </li> <li> <p>Deployment Frequency</p> </li> <li>Lead Time for Changes</li> <li> <p>Work Breakdown</p> </li> <li> <p>New work vs rework vs unplanned vs refactor</p> </li> <li> <p>Investment Profile</p> </li> <li> <p>Strategic work %</p> </li> <li>Maintenance %</li> <li> <p>Unplanned work %</p> </li> <li> <p>PR Process Health</p> </li> <li> <p>PR size</p> </li> <li>Review depth</li> <li>Review response time</li> <li> <p>Time-to-approve</p> </li> <li> <p>Bottlenecks</p> </li> <li> <p>Review congestion</p> </li> <li> <p>Idle WIP</p> </li> <li> <p>Review Silos / Single-reviewer dependencies</p> </li> </ul>"},{"location":"project/#gitprime-pluralsight-flow-engineering-behavior","title":"GitPrime / Pluralsight Flow \u2014 Engineering Behavior","text":"<p>Focus: individual contribution patterns and risk.</p> <p>Metrics</p> <ul> <li>Impact Score (weighted contribution proxy)</li> <li>Efficiency (merged vs reworked code)</li> <li> <p>Code Fundamentals</p> </li> <li> <p>Active days</p> </li> <li>Commits/day</li> <li>PRs opened/merged</li> <li>Lines changed</li> <li>Rework rate</li> <li> <p>Churn %</p> </li> <li> <p>Collaboration</p> </li> <li> <p>Review load</p> </li> <li> <p>Review turnaround</p> </li> <li> <p>Risk Indicators</p> </li> <li> <p>High-churn files</p> </li> <li> <p>Ownership hotspots</p> </li> <li> <p>Health Signals</p> </li> <li> <p>Sustained intensity</p> </li> <li>Weekend / off-hours work</li> <li>Irregular contribution patterns</li> </ul>"},{"location":"project/#gitlab-analytics-value-stream-devops","title":"GitLab Analytics \u2014 Value Stream &amp; DevOps","text":"<p>Focus: delivery pipeline and DORA.</p> <p>Metrics</p> <ul> <li>Lead Time for Changes</li> <li> <p>Value Stream Stages</p> </li> <li> <p>Issue \u2192 Code \u2192 Review \u2192 Merge \u2192 Deploy</p> </li> <li> <p>Merge Request Analytics</p> </li> <li> <p>MR size</p> </li> <li>Approval count</li> <li>Discussion count</li> <li> <p>Time to merge</p> </li> <li> <p>CI/CD</p> </li> <li> <p>Pipeline duration</p> </li> <li>Success rate</li> <li>MTTR</li> <li> <p>Change failure rate</p> </li> <li> <p>Contribution Analytics</p> </li> <li>Security / Vulnerability introduction rate</li> </ul>"},{"location":"project/#typeappapp-cognitive-load-well-being","title":"typeapp.app \u2014 Cognitive Load &amp; Well-being","text":"<p>Focus: IDE-level behavior and burnout risk.</p> <p>Metrics</p> <ul> <li>Flow State Duration</li> <li> <p>Context Switching</p> </li> <li> <p>File switches</p> </li> <li>Tab switches</li> <li> <p>Project switches</p> </li> <li> <p>Typing Behavior</p> </li> <li> <p>Error rate</p> </li> <li> <p>Undo/redo density</p> </li> <li> <p>Distraction Index</p> </li> <li> <p>Wellness Signals</p> </li> <li> <p>Late-night streaks</p> </li> <li> <p>Burst\u2013burnout cycles</p> </li> <li> <p>Cognitive Load Index</p> </li> </ul>"},{"location":"project/#part-2-unified-metrics-framework","title":"Part 2 \u2014 Unified Metrics Framework","text":""},{"location":"project/#1-delivery-velocity","title":"1. Delivery &amp; Velocity","text":"<ul> <li>Coding Time</li> <li>Review Time</li> <li>Rework Time</li> <li>Deploy Time</li> <li>Deployment Frequency</li> <li>Throughput (PRs, issues, story points)</li> <li>Work Composition</li> <li>Investment Profile (Strategic vs Maintenance)</li> </ul>"},{"location":"project/#2-code-quality-risk","title":"2. Code Quality &amp; Risk","text":"<ul> <li>Code Risk Index</li> <li>Churn</li> <li>Ownership concentration</li> <li>Hotspots</li> <li>Cyclomatic Complexity (radon; snapshots persisted and loaded via <code>store.get_complexity_snapshots()</code>)</li> <li>Rework Rate</li> <li> <p>PR Quality</p> </li> <li> <p>Size distribution</p> </li> <li>Comment density</li> <li> <p>Rejection rate</p> </li> <li> <p>Stability</p> </li> <li> <p>Bugs introduced</p> </li> <li>Defect escape rate</li> </ul>"},{"location":"project/#3-collaboration-team-dynamics","title":"3. Collaboration &amp; Team Dynamics","text":"<ul> <li>Review Responsiveness</li> <li>Reviewer Load</li> <li>Review Network Graph</li> <li> <p>Knowledge Distribution</p> </li> <li> <p>Bus factor</p> </li> <li> <p>Contribution entropy</p> </li> <li> <p>Communication Density</p> </li> </ul>"},{"location":"project/#4-developer-health-cognitive-load","title":"4. Developer Health &amp; Cognitive Load","text":"<ul> <li>Context Switching Score</li> <li>Focus Time Index</li> <li> <p>Work Pattern Health</p> </li> <li> <p>Late-night activity</p> </li> <li> <p>Weekend work</p> </li> <li> <p>Editing &amp; Typing Behavior</p> </li> </ul>"},{"location":"project/#5-systemic-process-health","title":"5. Systemic &amp; Process Health","text":"<ul> <li>Bottleneck Index</li> <li>WIP Congestion</li> <li>Stale Work Detection</li> <li>Predictability Index</li> </ul>"},{"location":"project/#part-3-developer-health-model","title":"Part 3 \u2014 Developer Health Model","text":"<p>Four Dimensions</p> <ol> <li>Delivery \u2014 speed, throughput, predictability</li> <li>Durability \u2014 quality, stability, long-term risk</li> <li>Developer Well-being \u2014 flow, burnout risk, load</li> <li>Dynamics \u2014 collaboration, knowledge spread</li> </ol> <p>This framework fully covers and extends LinearB, GitPrime, GitLab, and typeapp.app with a single coherent analytics model.</p> <p>Absolutely \u2014 I\u2019ll continue without pause and expand the entire system:</p> <ul> <li>Data model</li> <li>Events and ingestion</li> <li>Scoring &amp; formulas</li> <li>Detailed metric definitions</li> <li>Architecture</li> <li>Dashboards &amp; views</li> <li>Roadmap</li> </ul> <p>This will give you a complete draft developer health platform capable of rivaling LinearB, GitPrime, TypeApp, and GitLab Analytics in one unified product.</p>"},{"location":"project/#part-4-detailed-data-model","title":"\ud83d\udea7 PART 4 \u2014 Detailed Data Model","text":"<ol> <li>Git Events (commits, branches, PRs, reviews)</li> <li>Issue/Task Events (create, update, close, transitions)</li> <li>CI/CD Events (pipelines, deploys, failures, restores)</li> <li>IDE Telemetry Events (typing, context switching, focus time)</li> </ol>"},{"location":"project/#part-5-metric-definitions-formulas","title":"\ud83e\uddee PART 5 \u2014 Metric Definitions &amp; Formulas","text":"<p>Everything the earlier platforms measure + extended metrics.</p>"},{"location":"project/#1-delivery-metrics-flow-velocity","title":"1\ufe0f\u20e3 DELIVERY METRICS (Flow &amp; Velocity)","text":""},{"location":"project/#cycle-time","title":"Cycle Time","text":"<pre><code>cycle_time = merged_at - first_commit_timestamp_in_branch\n</code></pre>"},{"location":"project/#coding-time","title":"Coding Time","text":"<pre><code>coding_time = pr_created_at - first_commit_timestamp\n</code></pre>"},{"location":"project/#review-time","title":"Review Time","text":"<pre><code>review_time = first_review_timestamp - pr_created_at\n</code></pre>"},{"location":"project/#rework-time","title":"Rework Time","text":"<pre><code>rework_time = merged_at - first_approval_timestamp\n</code></pre>"},{"location":"project/#deploy-time","title":"Deploy Time","text":"<pre><code>deploy_time = first_prod_deploy_timestamp - merged_at\n</code></pre>"},{"location":"project/#throughput","title":"Throughput","text":"<pre><code>features_completed = count(tasks.type==\"feature\")\nbugs_fixed = count(tasks.type==\"bug\")\nprs_merged = count(prs where state=\"merged\")\n</code></pre>"},{"location":"project/#work-mix","title":"Work Mix","text":"<pre><code>rework_percent = churn_loc_last_30_days / total_loc_last_30_days\nnew_work_percent = new_features_loc / total_loc\nbugfix_percent = bugfix_loc / total_loc\nrefactor_percent = refactor_loc / total_loc\n</code></pre>"},{"location":"project/#dora-metrics","title":"DORA Metrics","text":"<ul> <li>Deployment frequency = deploy_count / time_period</li> <li>Lead time for changes = average(coding_time + review_time + deploy_time)</li> <li>MTTR = mean(time_from_incident_to_restore)</li> <li>Change failure rate = failed_deploys / total_deploys</li> </ul>"},{"location":"project/#2-code-quality-risk-metrics","title":"2\ufe0f\u20e3 CODE QUALITY &amp; RISK METRICS","text":""},{"location":"project/#churn","title":"Churn","text":"<pre><code>commit_churn = (loc_added + loc_deleted)\nrework_rate = churn_in_30_days / total_loc_touched\n</code></pre>"},{"location":"project/#hotspot-risk","title":"Hotspot Risk","text":"<p>A weighted model:</p> <pre><code>hotspot_score = file_churn * number_of_contributors * commit_frequency\n</code></pre>"},{"location":"project/#pr-size","title":"PR Size","text":"<pre><code>pr_size = lines_added + lines_deleted\n</code></pre>"},{"location":"project/#review-quality","title":"Review Quality","text":"<pre><code>comments_per_review = total_review_comments / number_of_reviews\nreview_coverage = files_reviewed / files_changed\n</code></pre>"},{"location":"project/#3-collaboration-team-dynamics_1","title":"3\ufe0f\u20e3 COLLABORATION &amp; TEAM DYNAMICS","text":""},{"location":"project/#review-responsiveness","title":"Review Responsiveness","text":"<pre><code>time_to_first_review = min(review.submitted_at) - pr_created_at\n</code></pre>"},{"location":"project/#collaboration-network","title":"Collaboration Network","text":"<p>Track edges: <code>author \u2192 reviewer</code></p> <p>Metrics:</p> <ul> <li>reciprocity</li> <li>centrality</li> <li>isolated contributors</li> <li>bottleneck reviewers</li> </ul>"},{"location":"project/#knowledge-distribution-bus-factor","title":"Knowledge Distribution (Bus Factor)","text":"<pre><code>ownership_score(file) = commits_by_user / total_commits_on_file\nbus_factor = number_of_files_with_ownership&gt;0.75\n</code></pre>"},{"location":"project/#4-developer-health-cognitive-load_1","title":"4\ufe0f\u20e3 DEVELOPER HEALTH &amp; COGNITIVE LOAD","text":""},{"location":"project/#flow-score-0100","title":"Flow Score (0\u2013100)","text":"<p>Uses editor telemetry:</p> <pre><code>focus_blocks = sequences of 10+ min uninterrupted editing\ncontext_switch_penalty = file_switches + tab_switches\nflow_score = focus_blocks * 10 - context_switch_penalty * 2\n</code></pre>"},{"location":"project/#cognitive-load","title":"Cognitive Load","text":"<pre><code>load_index = (avg_time_between_edits + tab_switch_rate + undo_density)\n</code></pre>"},{"location":"project/#burnout-indicators","title":"Burnout Indicators","text":"<pre><code>late_night_activity = activity between 12am\u20135am\nweekend_activity = sat/sun commits\nburst_cycles = # of commit storms within &lt;2h\nrisk_score = weighted_sum(late_night, weekend, bursts)\n</code></pre>"},{"location":"project/#5-system-health-metrics","title":"5\ufe0f\u20e3 SYSTEM HEALTH METRICS","text":""},{"location":"project/#wip-congestion","title":"WIP Congestion","text":"<pre><code>stale_prs = prs open &gt; X days\nqueued_reviews = prs waiting for review &gt; Y hours\nbuild_queue_length = count(pipelines waiting)\n</code></pre>"},{"location":"project/#predictability","title":"Predictability","text":"<pre><code>estimate_accuracy = |estimate - actual| / estimate\ncycle_time_variance = variance(cycle_time)\n</code></pre>"},{"location":"project/#part-7-dashboard-drafts","title":"\ud83d\udcca PART 7 \u2014 Dashboard Drafts","text":""},{"location":"project/#1-org-dashboard","title":"1. Org Dashboard","text":"<ul> <li>Cycle Time</li> <li>Deployment Frequency</li> <li>Hotspot files</li> <li>Bottleneck teams</li> <li>Investment Profile</li> <li>Predictability Index</li> </ul>"},{"location":"project/#2-team-dashboard","title":"2. Team Dashboard","text":"<ul> <li>PR review health</li> <li>Collaboration graph</li> <li>Throughput</li> <li>Roadmap completion forecast</li> <li>Team burnout risk</li> </ul>"},{"location":"project/#3-developer-dashboard","title":"3. Developer Dashboard","text":"<ul> <li>Flow score</li> <li>Deep work patterns</li> <li>Impact score</li> <li>Review contribution</li> <li>Churn &amp; rework (healthy or high)</li> </ul>"},{"location":"project/#4-repo-dashboard","title":"4. Repo Dashboard","text":"<ul> <li>Hotspots</li> <li>Bus factor</li> <li>Risky files</li> <li>Churn trends</li> <li>Contribution activity</li> </ul>"},{"location":"project/#developer-health-platform-full-draft-specification","title":"Developer Health Platform \u2013 Full Draft Specification","text":""},{"location":"project/#1-metric-formulations-weights","title":"1. Metric formulations &amp; weights","text":""},{"location":"project/#11-normalization-scoring-pattern","title":"1.1 Normalization &amp; scoring pattern","text":"<p>General pattern (so everything fits into 0\u2013100):</p> <ul> <li>Raw metric \u2192 normalize into [0,1] using either</li> <li>min\u2013max based on historical window, or</li> <li>percentile within org/team.</li> <li>Direction: some metrics \u201clower is better\u201d.</li> <li>If lower is better: <code>score = 1 - normalized_value</code></li> <li>If higher is better: <code>score = normalized_value</code></li> <li>Metric Score (0\u2013100): <code>metric_score = score * 100</code></li> </ul> <p>All metrics below follow: raw formula \u2192 normalized score and then roll into dimension scores.</p>"},{"location":"project/#12-delivery-metrics-flow-dora","title":"1.2 Delivery metrics (Flow &amp; DORA)","text":""},{"location":"project/#121-cycle-time","title":"1.2.1 Cycle time","text":"<p>Per PR:</p> <pre><code>cycle_time(pr) = merged_at - first_commit_time_on_branch(pr)\n</code></pre> <p>Normalized (lower = better):</p> <pre><code>ct_norm = clip( (cycle_time - CT_min) / (CT_max - CT_min), 0, 1 )\nct_score = (1 - ct_norm) * 100\n</code></pre>"},{"location":"project/#122-coding-review-deploy-times","title":"1.2.2 Coding / Review / Deploy times","text":"<p>For each PR:</p> <pre><code>coding_time  = pr_created_at - first_commit_time\nreview_time  = time_of_first_review - pr_created_at\nrework_time  = merged_at - time_of_first_approval\ndeploy_time  = first_prod_deploy_at - merged_at\n</code></pre> <p>Flow Balance score (detect one-stage dominance):</p> <pre><code>flow_balance_ratio = max(coding_time, review_time, rework_time, deploy_time) / cycle_time\n# If one stage &gt; 60% of total, that\u2019s a bottleneck\nbalance_norm = clip( (flow_balance_ratio - 0.25) / (0.75 - 0.25), 0, 1 )\nflow_balance_score = (1 - balance_norm) * 100\n</code></pre>"},{"location":"project/#123-deployment-frequency","title":"1.2.3 Deployment frequency","text":"<p>For a team in a period T (e.g., 14 days):</p> <pre><code>deploy_freq = number_of_prod_deploys / T_days\n</code></pre> <p>Normalize vs org history:</p> <pre><code>df_norm = clip( (deploy_freq - DF_min) / (DF_max - DF_min), 0, 1 )\ndeploy_freq_score = df_norm * 100\n</code></pre>"},{"location":"project/#124-dora-metrics","title":"1.2.4 DORA metrics","text":"<ul> <li> <p>Lead time for changes = average cycle_time for prod-bound PRs \u2192 same pattern as cycle time.</p> </li> <li> <p>MTTR (Mean Time To Restore):</p> </li> </ul> <pre><code>MTTR = avg(incident_resolved_at - incident_detected_at)\nmttr_norm, mttr_score = lower-is-better normalization\n</code></pre> <ul> <li>Change failure rate:</li> </ul> <pre><code>change_failure_rate = failed_deploys / total_deploys\ncfr_norm = clip( (change_failure_rate - CFR_min) / (CFR_max - CFR_min), 0, 1 )\ncfr_score = (1 - cfr_norm) * 100\n</code></pre>"},{"location":"project/#125-delivery-dimension-score","title":"1.2.5 Delivery Dimension Score","text":"<p>For a team or org:</p> <ul> <li><code>Cycle Time score</code> (CT)</li> <li><code>Deploy Frequency score</code> (DF)</li> <li><code>Lead Time score</code> (LT)</li> <li><code>MTTR score</code> (MT)</li> <li><code>Change Failure Rate score</code> (CF)</li> </ul> <p>Weights (example):</p> <pre><code>DeliveryScore = 0.30*CT + 0.20*DF + 0.20*LT + 0.15*MT + 0.15*CF\n</code></pre>"},{"location":"project/#13-code-quality-risk-metrics","title":"1.3 Code quality &amp; risk metrics","text":""},{"location":"project/#131-churn-rework","title":"1.3.1 Churn &amp; rework","text":"<p>Per file over window W (e.g., 30 days):</p> <pre><code>loc_touched = \u03a3(|loc_added| + |loc_deleted|)\nloc_reworked_soon = \u03a3(loc_modified_within_30d_of_being_added)\n\nrework_rate = loc_reworked_soon / loc_touched\n</code></pre> <p>Normalize (lower is better):</p> <pre><code>rw_norm = clip( (rework_rate - RW_min) / (RW_max - RW_min), 0, 1 )\nrework_score = (1 - rw_norm) * 100\n</code></pre>"},{"location":"project/#132-file-hotspot-score","title":"1.3.2 File hotspot score","text":"<p>For each file f:</p> <pre><code>churn_f        = loc_touched_in_W\ncontributors_f = number_of_distinct_authors_in_W\ncommit_freq_f  = commits_touching_file_in_W / days_in_W\n\nhotspot_raw = \u03b1*log(1 + churn_f) + \u03b2*contributors_f + \u03b3*commit_freq_f\n# \u03b1,\u03b2,\u03b3 ~ 0.4, 0.3, 0.3 initially\n</code></pre> <p>Normalize across files:</p> <pre><code>hs_norm = (hotspot_raw - HS_min) / (HS_max - HS_min)\nhotspot_score_file = hs_norm * 100\n</code></pre> <p>Team-level Code Risk score can be e.g. 80th percentile of hotspot scores in that team\u2019s modules, inverted:</p> <pre><code>risk_raw = P80(hotspot_score_file_for_team)\nrisk_score = 100 - risk_raw\n</code></pre> <p>Ownership concentration (for hotspot drivers) is derived from git blame data:</p> <pre><code>ownership_concentration = max(lines_by_author) / total_lines\n</code></pre> <p>Synthetic fixtures include an expanded file set to improve blame-driven ownership coverage. Blame-only sync is available via <code>cli.py sync blame --provider &lt;local|github|gitlab&gt;</code>.</p>"},{"location":"project/#133-pr-size","title":"1.3.3 PR Size","text":"<pre><code>pr_size = loc_added + loc_deleted\n</code></pre> <p>Normalize with a sweet spot (e.g. 20\u2013400 LOC):</p> <pre><code>if pr_size &lt; target_min:\n    size_score = 40 + 60*(pr_size / target_min)   # Very tiny PRs not ideal\nelif pr_size &lt;= target_max:\n    size_score = 100\nelse:\n    overflow = min(pr_size - target_max, cap)\n    size_score = max(40, 100 - overflow / scale) # penalize very large ones\n</code></pre>"},{"location":"project/#134-review-depth-coverage","title":"1.3.4 Review depth &amp; coverage","text":"<pre><code>review_comments_per_pr = total_comments_on_pr / 1\nreview_coverage = reviewed_files_count / files_changed\n</code></pre> <p>Combined PR Review Health score:</p> <pre><code>depth_score = sigmoid( a * (review_comments_per_pr - target_comments) )\ncoverage_score = review_coverage * 100\n\nreview_health_score = 0.4*size_score + 0.3*depth_score + 0.3*coverage_score\n</code></pre> <p>Where <code>sigmoid(x) = 100 / (1 + exp(-x))</code> scaled to [0,100].</p>"},{"location":"project/#135-quality-durability-dimension-score","title":"1.3.5 Quality &amp; Durability Dimension Score","text":"<p>Metrics:</p> <ul> <li>Rework score</li> <li>Code risk score</li> <li>Review health score</li> <li>Defect introduction rate score</li> </ul> <p>Weights:</p> <pre><code>DurabilityScore = 0.35*ReworkScore + 0.30*CodeRiskScore\n                  + 0.20*ReviewHealthScore + 0.15*DefectRateScore\n</code></pre>"},{"location":"project/#14-collaboration-team-dynamics-metrics","title":"1.4 Collaboration &amp; team dynamics metrics","text":""},{"location":"project/#141-review-responsiveness","title":"1.4.1 Review responsiveness","text":"<pre><code>time_to_first_review = avg( first_review_timestamp - pr_created_at )\n\n# lower better\nresp_norm = (time_to_first_review - RESP_min) / (RESP_max - RESP_min)\nReviewResponsivenessScore = (1 - resp_norm) * 100\n</code></pre>"},{"location":"project/#142-review-load-reciprocity","title":"1.4.2 Review load &amp; reciprocity","text":"<p>For each user u in window W:</p> <pre><code>reviews_given_u  = count(reviews where reviewer_id = u)\nreviews_received_u = count(prs authored_by_u that had reviews)\nreview_balance_u = reviews_given_u / (reviews_received_u + 1)\n</code></pre> <p>Team reciprocity via dispersion:</p> <pre><code>team_balance = variance(review_balance_u across team)\nReciprocityScore = 100 - normalized_variance(team_balance)\n</code></pre>"},{"location":"project/#143-knowledge-distribution-bus-factor","title":"1.4.3 Knowledge distribution / Bus factor","text":"<ul> <li>Bus Factor (Truck Factor): The smallest number of developers that account for &gt;= 50% of the total code churn in the window.</li> <li>Code Ownership Gini: Gini coefficient of code contribution (churn) distribution. 0.0 = perfect equality, 1.0 = perfect inequality.</li> </ul> <pre><code>bus_factor = number_of_devs_contributing_50_percent_churn\ngini = (2 * sum(i * y_i) / (n * sum(y_i))) - (n + 1) / n\n</code></pre>"},{"location":"project/#144-dynamics-dimension-score","title":"1.4.4 Dynamics Dimension Score","text":"<p>Use:</p> <ul> <li>ReviewResponsivenessScore</li> <li>ReciprocityScore</li> <li>BusFactorScore</li> <li>Cross-team-review score (optional)</li> </ul> <pre><code>DynamicsScore = 0.30*ReviewResponsivenessScore\n              + 0.25*ReciprocityScore\n              + 0.30*BusFactorScore\n              + 0.15*CrossTeamReviewScore\n</code></pre>"},{"location":"project/#15-developer-well-being-cognitive-load-metrics","title":"1.5 Developer well-being &amp; cognitive load metrics","text":""},{"location":"project/#151-flow-score","title":"1.5.1 Flow Score","text":"<p>From editor events:</p> <pre><code>focus_block = a continuous period \u2265 10 min\n              with no context_switch events (tab/file/project) and\n              active edits every \u2264 2 min\n\nnum_blocks   = count(focus_block) in day\navg_block_len = avg(duration of focus_block)\ncontext_switches = count(context_switch events per hour)\ninterruptions = count(non-editor-window-focus events per hour)\n\nflow_raw = w1*num_blocks + w2*avg_block_len - w3*context_switches - w4*interruptions\n</code></pre> <p>Normalize:</p> <pre><code>flow_norm = (flow_raw - FLOW_min) / (FLOW_max - FLOW_min)\nFlowScore = clip(flow_norm, 0, 1) * 100\n</code></pre> <p>Start with <code>w1=2, w2=0.1, w3=1, w4=1</code> as tunables.</p>"},{"location":"project/#152-cognitive-load-index","title":"1.5.2 Cognitive load index","text":"<p>Signals:</p> <ul> <li>avg time between edits in same file</li> <li>undo/redo density</li> <li>error bursts (rapid changes + reverts)</li> </ul> <p>Example:</p> <pre><code>avg_edit_gap = avg(time_between_edits_same_file)\nundo_density = total_undo_ops / total_edits\nswitch_density = tab_switches / hour\n\nload_raw = a1*avg_edit_gap + a2*undo_density + a3*switch_density\nLoadScore = (1 - normalized(load_raw)) * 100\n</code></pre> <p>Higher score = better (manageable load).</p>"},{"location":"project/#153-burnout-risk-score","title":"1.5.3 Burnout risk score","text":"<p>Signals in last 14\u201330 days:</p> <pre><code>late_night_ratio = commits_or_edits_between_00_05 / total_commits_or_edits\nweekend_ratio    = weekend_commits / total_commits\nstreak_length    = longest_consecutive_days_with_activity\nburst_index      = fraction_of_commits_in_top_20% busiest_hours\n</code></pre> <p>Combine:</p> <pre><code>burnout_raw = b1*late_night_ratio + b2*weekend_ratio + b3*streak_length_norm + b4*burst_index\nBurnoutRiskScore = (1 - normalized(burnout_raw)) * 100\n</code></pre> <p>(High score = low risk.)</p>"},{"location":"project/#154-well-being-dimension-score","title":"1.5.4 Well-being Dimension Score","text":"<p>Combine:</p> <pre><code>WellBeingScore = 0.40*FlowScore + 0.25*LoadScore + 0.35*BurnoutRiskScore\n</code></pre>"},{"location":"project/#16-system-process-health-metrics","title":"1.6 System &amp; process health metrics","text":""},{"location":"project/#161-bottleneck-index","title":"1.6.1 Bottleneck index","text":"<p>For each stage s in {coding, review, qa, deploy}:</p> <pre><code>stage_time_s = avg(time_spent_in_stage_s)\nstage_fraction_s = stage_time_s / total_cycle_time\nbottleneck_raw = max(stage_fraction_s)\nBottleneckScore = (1 - normalized(bottleneck_raw)) * 100\n</code></pre>"},{"location":"project/#162-wip-congestion","title":"1.6.2 WIP congestion","text":"<pre><code>stale_prs_ratio = stale_prs / total_open_prs\nqueue_length    = queued_reviews / team_size\nbuild_queue     = queued_builds / historical_median\n\ncongestion_raw = c1*stale_prs_ratio + c2*queue_length + c3*build_queue\nCongestionScore = (1 - normalized(congestion_raw)) * 100\n</code></pre>"},{"location":"project/#163-predictability","title":"1.6.3 Predictability","text":"<p>Defined as Completion Rate (how well the team clears its plate).</p> <pre><code>predictability_score = items_completed / (items_completed + wip_count_end_of_day)\n</code></pre>"},{"location":"project/#164-system-health-dimension-score","title":"1.6.4 System Health Dimension Score","text":"<pre><code>SystemHealthScore = 0.40*BottleneckScore\n                  + 0.30*CongestionScore\n                  + 0.30*PredictabilityScore\n</code></pre>"},{"location":"project/#2-healthy-vs-unhealthy-thresholds-initial-draft","title":"2. Healthy vs unhealthy thresholds (initial draft)","text":"<p>These are initial org-agnostic defaults; tune them to your context.</p>"},{"location":"project/#delivery","title":"Delivery","text":"<ul> <li> <p>Cycle time (PR \u2192 deploy)</p> </li> <li> <p>Excellent: &lt; 24h</p> </li> <li>Healthy: 24\u201372h</li> <li>At risk: 3\u20137 days</li> <li> <p>Unhealthy: &gt; 7 days</p> </li> <li> <p>Time to first review</p> </li> <li> <p>Excellent: &lt; 2h</p> </li> <li>Healthy: 2\u20138h</li> <li>At risk: 8\u201324h</li> <li> <p>Unhealthy: &gt; 24h</p> </li> <li> <p>Deployment frequency</p> </li> <li> <p>Excellent: multiple times per day</p> </li> <li>Healthy: daily\u2013few times/week</li> <li>At risk: &lt; 1/week</li> <li> <p>Unhealthy: &lt; 1/month</p> </li> <li> <p>Change failure rate</p> </li> <li>Excellent: &lt; 10%</li> <li>Healthy: 10\u201320%</li> <li>At risk: 20\u201330%</li> <li>Unhealthy: &gt; 30%</li> </ul>"},{"location":"project/#code-quality-risk","title":"Code Quality &amp; Risk","text":"<ul> <li> <p>Rework rate (loc re-touched within 30 days)</p> </li> <li> <p>Excellent: &lt; 10%</p> </li> <li>Healthy: 10\u201320%</li> <li>At risk: 20\u201335%</li> <li> <p>Unhealthy: &gt; 35%</p> </li> <li> <p>Average PR size (median)</p> </li> <li> <p>Healthy: 50\u2013300 LOC</p> </li> <li>At risk: 300\u2013600 LOC</li> <li> <p>Unhealthy: &gt; 600 LOC regularly</p> </li> <li> <p>Hotspot concentration (files with high risk)</p> </li> <li>Healthy: &lt; 5% of files</li> <li>At risk: 5\u201315%</li> <li>Unhealthy: &gt; 15%</li> </ul>"},{"location":"project/#dynamics-collaboration","title":"Dynamics &amp; Collaboration","text":"<ul> <li> <p>Bus factor (single-owner files &gt;75%)</p> </li> <li> <p>Healthy: &lt; 25% of team files</p> </li> <li>At risk: 25\u201350%</li> <li> <p>Unhealthy: &gt; 50%</p> </li> <li> <p>Review reciprocity</p> </li> <li>Healthy: most devs in [.5, 2] given/received ratio</li> <li>Unhealthy: many devs only receiving or only giving</li> </ul>"},{"location":"project/#well-being","title":"Well-being","text":"<ul> <li> <p>Late-night ratio</p> </li> <li> <p>Healthy: &lt; 5%</p> </li> <li>At risk: 5\u201315%</li> <li> <p>Unhealthy: &gt; 15%</p> </li> <li> <p>Weekend ratio</p> </li> <li> <p>Healthy: &lt; 5\u201310%</p> </li> <li>At risk: 10\u201325%</li> <li> <p>Unhealthy: &gt; 25%</p> </li> <li> <p>Flow time (uninterrupted focus per day)</p> </li> <li>Excellent: \u2265 2\u20133 hours</li> <li>Healthy: 1\u20132 hours</li> <li>At risk: &lt; 1 hour</li> <li>Unhealthy: mostly fragmented 10\u201315 min blocks</li> </ul>"},{"location":"project/#3-org-wide-developer-health-score","title":"3. Org-wide \u201cDeveloper Health Score\u201d","text":"<p>Use the 4D model:</p> <ol> <li>DeliveryScore</li> <li>DurabilityScore</li> <li>WellBeingScore</li> <li>DynamicsScore</li> </ol> <p>Optionally add SystemHealthScore as a 5th dimension.</p>"},{"location":"project/#31-dimension-aggregation","title":"3.1 Dimension aggregation","text":"<p>Example weights:</p> <pre><code>DH_Delivery   = DeliveryScore\nDH_Durability = DurabilityScore\nDH_WellBeing  = WellBeingScore\nDH_Dynamics   = DynamicsScore\nDH_System     = SystemHealthScore\n\nDeveloperHealthScore = 0.25*DH_Delivery\n                      + 0.25*DH_Durability\n                      + 0.20*DH_WellBeing\n                      + 0.20*DH_Dynamics\n                      + 0.10*DH_System\n</code></pre> <p>Compute per org, per team, per repo, per individual (with some metric substitutions).</p>"},{"location":"project/#7-predictive-ai-models","title":"7. Predictive AI models","text":""},{"location":"project/#71-delivery-risk-prediction-pr-level","title":"7.1 Delivery risk prediction (PR-level)","text":"<p>Goal: Predict whether a PR will be \u201cslow\u201d or \u201cproblematic\u201d at creation time.</p> <ul> <li>Label: <code>slow_pr = cycle_time &gt; org_p75_cycle_time</code> (binary)</li> <li>Features: PR size, file count, hotspot involvement, author historical cycle time, team WIP, time-of-day/day-of-week, reviewer count</li> <li>Model: Gradient boosting (XGBoost/LightGBM) or logistic regression</li> <li>Output: <code>P(slow_pr)</code> + suggested mitigations (split PR, add reviewer, etc.)</li> </ul>"},{"location":"project/#72-burnout-risk-prediction-user-level","title":"7.2 Burnout risk prediction (user-level)","text":"<p>Goal: Predict burnout risk for each dev next 2\u20134 weeks.</p> <ul> <li>Label: Proxy: future spike in late-night/weekend + drop in FlowScore (or HR flag if integrated)</li> <li>Features: late-night ratio, weekend ratio, FlowScore mean/variance, streak length, review load, context switching, team stress/incident load</li> <li>Model: time-series classification or GBM on rolling window features</li> <li>Output: <code>BurnoutRiskProbability</code> 0\u20131 \u2192 0\u2013100 display</li> </ul>"},{"location":"project/#73-expected-cycle-time-delivery-date","title":"7.3 Expected cycle time / delivery date","text":"<p>Goal: Predict cycle time for a new PR or task.</p> <ul> <li>Label: numeric cycle time</li> <li>Features: 7.1 + repo/component + team throughput context</li> <li>Model: regression (GBM, random forest, or linear w/ interactions)</li> <li>Output: predicted cycle time + confidence interval</li> </ul>"},{"location":"project/#74-hotspot-evolution","title":"7.4 Hotspot evolution","text":"<p>Goal: Predict which files will become high-risk hotspots soon.</p> <ul> <li>Label: <code>future_hotspot = hotspot_score_file &gt; threshold in next W</code></li> <li>Features: churn, entropy, commit frequency, bug density, ownership volatility</li> <li>Model: GBM / logistic regression</li> <li>Output: \u201cemerging risky modules\u201d highlights</li> </ul>"},{"location":"project/#8-dashboard-mockups-textual-figma","title":"8. Dashboard mockups (textual Figma)","text":""},{"location":"project/#81-org-dashboard-executive-view","title":"8.1 Org Dashboard (\u201cExecutive View\u201d)","text":"<p>Top bar</p> <ul> <li>Time range selector (Last 7 / 30 / 90 days)</li> <li>Org dropdown</li> <li>Overall Developer Health Score pill (e.g. 82/100) + sparkline</li> </ul> <p>Row 1: 4 dimension cards</p> <ul> <li>Delivery: score + median cycle time, deploy frequency, change failure rate</li> <li>Durability: score + rework %, hotspot count, defect rate</li> <li>Well-being: score + flow hrs/dev/day, burnout risk index</li> <li>Dynamics: score + review responsiveness, bus factor</li> </ul> <p>Row 2: Charts</p> <ul> <li>Cycle Time Breakdown (stacked: coding, review, deploy) over time</li> <li>Deploy Frequency vs Change Failure Rate (dual-axis)</li> </ul> <p>Row 3: Tables</p> <ul> <li>Teams ranked by Developer Health Score</li> <li>Top 10 hotspot repos/services</li> </ul>"},{"location":"project/#82-team-dashboard-eng-lead-view","title":"8.2 Team Dashboard (\u201cEng Lead View\u201d)","text":"<p>Header</p> <ul> <li>Team name + date range + Team Health Score</li> </ul> <p>Row 1: KPIs</p> <ul> <li>Delivery: cycle time, time to first review, deploys/week</li> <li>Quality: rework %, defect rate, top hotspots</li> <li>Well-being: flow hrs/dev/day, late-night %, burnout risk</li> <li>Dynamics: review delay, bus factor, review balance</li> </ul> <p>Row 2: Charts</p> <ul> <li>PR Flow Timeline (scatter: PRs by age vs cycle time, colored by size)</li> <li>Team Review Network (graph: nodes=devs, edges=reviews)</li> </ul> <p>Row 3: People table</p> <ul> <li>Dev, Impact proxy, FlowScore, BurnoutRisk, ReviewLoad, Churn%</li> <li>Flags: high review load, high late-night work, isolation</li> </ul>"},{"location":"project/#83-developer-dashboard-personal-view","title":"8.3 Developer Dashboard (\u201cPersonal View\u201d)","text":"<p>Header</p> <ul> <li>Dev name + \u201cYour Health this month: 78/100\u201d</li> </ul> <p>Row 1: Summary</p> <ul> <li>Delivery vs team, Quality vs team, Flow, Burnout risk</li> </ul> <p>Row 2: Charts</p> <ul> <li>Flow over time</li> <li>Work mix (new vs bugfix vs refactor)</li> </ul> <p>Row 3: Suggestions</p> <ul> <li>Auto-generated coaching insights (split PRs, protect focus blocks, reduce late-night pattern, etc.)</li> </ul>"},{"location":"project/#84-repo-service-dashboard","title":"8.4 Repo / Service Dashboard","text":"<p>Header</p> <ul> <li>Repo name + risk indicator</li> </ul> <p>Row 1: Risk &amp; Quality</p> <ul> <li>Risk gauge</li> <li>Hotspot file count</li> <li>Defects tied to repo</li> </ul> <p>Row 2: Hotspots table</p> <ul> <li>file path, risk, churn, contributors, linked bugs</li> </ul> <p>Row 3: Ownership &amp; Bus factor</p> <ul> <li>contributor distribution</li> <li>single-owner file visualization</li> </ul>"},{"location":"roadmap/","title":"Roadmap &amp; Remaining Tasks","text":"<p>This checklist tracks what is complete and what remains to finalize <code>dev-health-ops</code>.</p>"},{"location":"roadmap/#completed","title":"Completed","text":"<ul> <li>[x] Core Git + PR Metrics: Commit size, churn, PR cycle/pickup/review times, review load, PR size stats.</li> <li>[x] Work Item Flow Metrics: Cycle/lead time p50/p90, WIP count/age, flow efficiency, state duration + avg WIP.</li> <li>[x] Quality + Risk Metrics: Defect introduction rate, WIP congestion, rework churn ratio, single-owner file ratio proxy.</li> <li>[x] DORA Metrics: MTTR, change failure rate, deployment/incident daily rollups.</li> <li>[x] Wellbeing Signals: After-hours and weekend commit ratios; weekend active users (derived from user activity).</li> <li>[x] Connectors + Pipelines: GitHub/GitLab CI/CD + deployments + incidents ingestion; async batch helpers.</li> <li>[x] Storage + Schema: ClickHouse migrations and sink support for new metrics tables/columns.</li> <li>[x] SQLite test cleanup: Dispose SQLAlchemyStore engines to avoid aiosqlite event-loop teardown warnings.</li> <li>[x] CLI Controls: Dedicated <code>sync cicd</code>, <code>sync deployments</code>, and <code>sync incidents</code> targets.</li> <li>[x] Complexity Metrics (Batch): Radon-based cyclomatic complexity scanning with DB-driven batch mode (<code>-s \"*\"</code>).</li> <li>[x] Unified Complexity Snapshot Loading: <code>metrics daily</code> loads complexity snapshots via storage abstraction (<code>get_complexity_snapshots</code>).</li> <li>[x] Investment Area Classification: Automated work item and churn classification based on configurable rules.</li> <li>[x] Work Item Sync Command: Dedicated <code>sync work-items</code> flow to fetch provider work items separately from <code>metrics daily</code>.</li> <li>[x] Work Item Auth Override: <code>sync work-items --auth</code> can override GitHub/GitLab tokens for provider sync.</li> <li>[x] Synthetic fixtures: CI/CD + deployments + incidents with metrics rollups for ClickHouse.</li> <li>[x] IC Metrics + Landscape: Identity resolution, unified UserMetricsDailyRecord, and landscape rolling stats (Churn/Cycle/WIP vs Throughput).</li> <li>[x] Grafana Dev Health Panels: Panel plugin with Developer Landscape, Hotspot Explorer, and Investment Flow views.</li> <li>[x] Dev Health panel theming: Plugin-local Grafana theme with a custom visualization palette.</li> <li>[x] Dev Health panel selector: Panel settings dropdown to switch between Developer Landscape, Hotspot Explorer, and Investment Flow.</li> <li>[x] Grafana Panel ClickHouse Contracts: Stats schema views for landscape, hotspot, and investment flow panels (use <code>WITH ... AS</code> aliasing).</li> <li>[x] Dashboard team filter normalization: Include legacy NULL/empty team IDs in Grafana ClickHouse queries via <code>ifNull(nullIf(team_id, ''), 'unassigned')</code>.</li> <li>[x] Investment metrics NULL team IDs: <code>investment_metrics_daily.team_id</code> stores NULL for unassigned; investment flow view casts via <code>toNullable(team_id)</code>.</li> <li>[x] Hotspot Explorer formatting: Use table format and order by day to keep Grafana sorting valid.</li> <li>[x] Hotspot ownership concentration: Derive ownership concentration from git blame line shares.</li> <li>[x] Synthetic blame coverage: Expand fixture file set to improve hotspot ownership coverage.</li> <li>[x] Blame-only sync: Add <code>cli.py sync blame --provider &lt;local|github|gitlab&gt;</code> for targeted blame backfill.</li> <li>[x] Backfill commit caps: GitHub/GitLab <code>--date/--backfill</code> runs default to unlimited commits unless explicitly capped.</li> <li>[x] Hotspot Explorer frame binding: Prefer facts frames via <code>churn_loc_30d</code> to keep drivers/trends rendering.</li> <li>[x] IC Drilldown churn vs throughput: Add identity-filtered churn vs throughput panel.</li> </ul>"},{"location":"roadmap/#remaining","title":"Remaining","text":""},{"location":"roadmap/#data-fixtures","title":"Data + Fixtures","text":"<ul> <li>[ ] Fixtures validation: Ensure synthetic work items + commits generate non-zero Phase 2 metrics.</li> </ul>"},{"location":"roadmap/#dashboards","title":"Dashboards","text":"<ul> <li>[x] Dashboards for CI/CD, deployments, incidents (panels for success rate, duration, deploy counts, MTTR).</li> <li>[x] Investment Areas team filter: use regex <code>match(...)</code> filters for the team variable in ClickHouse queries.</li> <li>[ ] Work tracking dashboards audit: Validate filters and table joins for synthetic + real providers.</li> <li>[ ] Fix dashboard templating filters: Ensure variable regex and <code>match(...)</code> filters do not return empty results.</li> </ul>"},{"location":"roadmap/#metrics-enhancements","title":"Metrics Enhancements","text":"<ul> <li>[x] Bus factor (true): Top contributors accounting for &gt;50% of churn (Bus Factor) and Gini coefficient (Knowledge Distribution).</li> <li>[x] Predictability index: Completion Rate (items completed / (completed + wip)).</li> <li>[ ] Capacity planning: Forecast completion using historical throughput.</li> <li>[ ] Identity Linking: Reliable mapping of Work Items (Jira/LinearB) to Git commits (e.g. via commit messages or smart matching).</li> <li>[ ] Work Item Repo Filtering by Tags/Settings: Allow <code>sync work-items</code> to filter repos using <code>repos.tags</code> or <code>repos.settings</code> (beyond name glob).</li> </ul>"},{"location":"roadmap/#testing-docs","title":"Testing + Docs","text":"<ul> <li>[x] Tests for new sinks columns (ClickHouse + SQLite write paths for Phase 2 + wellbeing).</li> <li>[x] Docs refresh: Usage examples for new CLI flags and fixture generation steps.</li> </ul>"},{"location":"task_trackers/","title":"Task Trackers &amp; Work Items","text":"<p>This repo normalizes Jira issues, GitHub issues/Projects items, and GitLab issues into a unified <code>WorkItem</code> model (<code>models/work_items.py</code>) and computes daily aggregates + cycle times.</p> <p>Jira is used to track associated project work (planning/throughput/WIP). Pull request metrics are computed from PR/MR data synced via the CLI (<code>python cli.py sync ...</code>) and are independent of Jira.</p>"},{"location":"task_trackers/#provider-credentials-env-vars","title":"Provider Credentials (env vars)","text":""},{"location":"task_trackers/#jira-cloud","title":"Jira (Cloud)","text":"<ul> <li><code>JIRA_BASE_URL</code> (e.g. <code>your-org.atlassian.net</code> or <code>https://your-org.atlassian.net</code>; normalized to <code>https://</code>)</li> <li><code>JIRA_EMAIL</code></li> <li><code>JIRA_API_TOKEN</code></li> <li><code>JIRA_PROJECT_KEYS</code> (optional, comma-separated, e.g. <code>ABC,XYZ</code>)</li> <li><code>JIRA_JQL</code> (optional override; if set, used as-is instead of the built-in windowed query)</li> <li><code>JIRA_FETCH_ALL</code> (optional; set to <code>1</code> to fetch all issues in the project(s) regardless of date window \u2014 can be very slow)</li> <li><code>JIRA_FETCH_COMMENTS</code> (optional; set to <code>0</code> to disable comment metadata ingestion; default: <code>1</code>)</li> </ul> <p>Optional Jira field mappings (instance-specific): - <code>JIRA_STORY_POINTS_FIELD</code> (e.g. <code>customfield_10016</code>) - <code>JIRA_SPRINT_FIELD</code> (default: <code>customfield_10020</code>) - <code>JIRA_EPIC_LINK_FIELD</code> (e.g. <code>customfield_10014</code>)</p>"},{"location":"task_trackers/#github","title":"GitHub","text":"<ul> <li><code>GITHUB_TOKEN</code></li> <li>Optional CLI override: <code>python cli.py sync work-items --provider github --auth \"$GITHUB_TOKEN\" ...</code></li> </ul> <p>Optional Projects v2 ingestion: - <code>GITHUB_PROJECTS_V2</code> as comma-separated <code>org_login:project_number</code> entries, e.g.:   - <code>GITHUB_PROJECTS_V2=\"myorg:3,anotherorg:12\"</code></p>"},{"location":"task_trackers/#gitlab","title":"GitLab","text":"<ul> <li><code>GITLAB_TOKEN</code></li> <li><code>GITLAB_URL</code> (optional, default: <code>https://gitlab.com</code>)</li> <li>Optional CLI override: <code>python cli.py sync work-items --provider gitlab --auth \"$GITLAB_TOKEN\" ...</code></li> </ul>"},{"location":"task_trackers/#status-type-normalization","title":"Status &amp; Type Normalization","text":"<p>Status normalization is config-driven via <code>config/status_mapping.yaml</code>.</p>"},{"location":"task_trackers/#status-categories","title":"Status categories","text":"<p>Normalized categories are: - <code>backlog</code> - <code>todo</code> - <code>in_progress</code> - <code>in_review</code> - <code>blocked</code> - <code>done</code> - <code>canceled</code></p>"},{"location":"task_trackers/#provider-specific-rules","title":"Provider-specific rules","text":"<p>The mapping file supports: - Jira: <code>providers.jira.statuses</code> (maps Jira status names) - GitHub/GitLab: <code>providers.&lt;provider&gt;.status_labels</code> (maps label names to categories)</p> <p>If no label/status match exists: - GitHub: <code>open \u2192 todo</code>, <code>closed \u2192 done</code> - GitLab: <code>opened \u2192 todo</code>, <code>closed \u2192 done</code></p>"},{"location":"task_trackers/#identity-mapping-optional","title":"Identity Mapping (optional)","text":"<p>To keep user metrics consistent across providers, populate <code>config/identity_mapping.yaml</code>.</p> <p>Schema: - <code>canonical</code>: stable identity (prefer email) - <code>aliases</code>: provider-qualified logins (e.g. <code>github:octocat</code>) or Jira account IDs (e.g. <code>jira:accountid:abcd123</code>)</p>"},{"location":"task_trackers/#team-mapping-optional","title":"Team Mapping (optional)","text":"<p>To enable team filtering in Grafana, you can sync teams from various sources.</p>"},{"location":"task_trackers/#config-based-mapping","title":"Config-based Mapping","text":"<p>Populate <code>config/team_mapping.yaml</code> (schema: <code>team_id</code>, <code>team_name</code>, <code>members</code>). Then run: <pre><code>python cli.py sync teams --path config/team_mapping.yaml\n</code></pre></p>"},{"location":"task_trackers/#jira-project-mapping","title":"Jira Project Mapping","text":"<p>Automatically import Jira projects as teams: <pre><code>python cli.py sync teams --provider jira\n</code></pre></p>"},{"location":"task_trackers/#running-jira-work-metrics","title":"Running Jira work metrics","text":"<p>Jira work items are fetched via the work item sync job:</p> <pre><code>python cli.py sync work-items --provider jira --date 2025-02-01 --backfill 30 --db \"clickhouse://localhost:8123/default\"\n</code></pre> <p>Use <code>-s</code>/<code>--search</code> to filter repos by name (glob pattern), e.g.:</p> <pre><code>python cli.py sync work-items --provider github -s \"org/*\" --date 2025-02-01 --backfill 30 --db \"clickhouse://localhost:8123/default\"\n</code></pre>"},{"location":"task_trackers/#quick-jira-api-smoke-test-curl","title":"Quick Jira API smoke test (curl)","text":"<p>Jira Cloud has removed <code>GET /rest/api/3/search</code>; use <code>GET /rest/api/3/search/jql</code>:</p> <pre><code>curl -sS -u \"$JIRA_EMAIL:$JIRA_API_TOKEN\" \\\n  --get \"https://$JIRA_BASE_URL/rest/api/3/search/jql\" \\\n  --data-urlencode \"jql=(updated &gt;= '2025-09-10' OR (statusCategory != Done AND created &lt;= '2025-12-18')) ORDER BY updated DESC\" \\\n  --data-urlencode \"maxResults=5\" \\\n  --data-urlencode \"fields=key,summary,updated,status\"\n</code></pre>"}]}