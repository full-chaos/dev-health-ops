{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"dev-health-ops","text":"<p>Developer health and operations analytics, built to be transparent, affordable, and useful.</p>"},{"location":"#why-this-exists","title":"Why this exists","text":"<p>Developer health tooling drifted into expensive, opaque scoring systems that are easy to misuse. This project is intentionally different.</p> <ul> <li>Accessibility over extraction</li> <li>Learning, not judgment</li> <li>Trends over absolutes</li> <li>Inspectable by default</li> </ul>"},{"location":"#what-you-can-do-here","title":"What you can do here","text":"<ul> <li>Sync data from GitHub, GitLab, Jira, and local Git repositories.</li> <li>Normalize and store work items, commits, and blame data.</li> <li>Compute metrics like throughput, cycle time, rework, and predictability.</li> <li>Explore optional Grafana dashboards for hotspots, investment flow, and team health (panel plugin moved to <code>dev-health-panels</code>).</li> </ul>"},{"location":"#quick-links","title":"Quick links","text":"<ul> <li>Getting started: <code>getting-started.md</code></li> <li>Architecture: <code>architecture.md</code></li> <li>CLI reference: <code>cli.md</code></li> <li>Metrics: <code>metrics.md</code></li> <li>Grafana: <code>grafana.md</code></li> </ul>"},{"location":"#product-documentation-prd-style","title":"Product documentation (PRD-style)","text":"<p>These pages are the canonical narrative docs that complement the reference pages.</p> <ul> <li>Product PRD: <code>product/prd.md</code></li> <li>Product brief: <code>product/product-brief.md</code></li> <li>Concepts and guardrails: <code>product/concepts.md</code></li> <li>Investment taxonomy: <code>product/investment-taxonomy.md</code></li> <li>Views and charts: <code>user-guide/views-index.md</code></li> </ul>"},{"location":"CONNECTOR_USAGE/","title":"Using <code>cli.py</code> with GitHub and GitLab Connectors","text":"<p>The <code>cli.py</code> front end exposes target-specific sync jobs with a provider switch:</p> <ol> <li>Sync targets: <code>git</code>, <code>prs</code>, <code>blame</code>, <code>cicd</code>, <code>deployments</code>, <code>incidents</code></li> <li>Providers: <code>local</code>, <code>github</code>, <code>gitlab</code>, <code>synthetic</code></li> </ol>"},{"location":"CONNECTOR_USAGE/#simplified-library-dispatch","title":"Simplified Library Dispatch","text":"<p>The dedicated CLI (<code>python cli.py sync ...</code>) now exposes:</p> <ul> <li>Unified authentication with <code>--auth</code> (works for both GitHub and GitLab)</li> <li>Auto-detection of database type from connection string URL scheme</li> <li>Consolidated batch processing arguments that work with both connectors (<code>-s/--search</code>, <code>--group</code>, <code>--batch-size</code>, <code>--max-concurrent</code>)</li> </ul>"},{"location":"CONNECTOR_USAGE/#local-repository-mode","title":"Local Repository Mode","text":"<p>This is the original mode that analyzes a local git repository.</p> <pre><code># Using environment variables\nexport DATABASE_URI=\"postgresql+asyncpg://localhost:5432/mergestat\"\nexport REPO_PATH=\"/path/to/repo\"\npython cli.py sync git --provider local\n\n# Using command-line arguments (auto-detects database type from URL)\npython cli.py sync git --provider local --db \"postgresql+asyncpg://localhost:5432/mergestat\" --repo-path \"/path/to/repo\"\n\n# Limit to commits and blames since a date (ISO date or datetime)\npython cli.py sync git --provider local --db \"sqlite+aiosqlite:///mergestat.db\" --repo-path \"/path/to/repo\" --since 2024-01-01\n\n# Explicit connector type (not needed with the CLI mode, but shown for parity)\npython cli.py sync git --provider local --db \"postgresql://...\" --repo-path \"/path/to/repo\"\n</code></pre> <p>How filtering works: <code>--since</code> / <code>--date</code> restricts commits and commit stats to changes at or after the timestamp.</p>"},{"location":"CONNECTOR_USAGE/#github-connector-mode","title":"GitHub Connector Mode","text":"<p>Fetch repository data directly from GitHub without cloning. Fully supports both public and private repositories.</p>"},{"location":"CONNECTOR_USAGE/#requirements","title":"Requirements","text":"<ul> <li>GitHub personal access token with appropriate permissions</li> <li>For public repositories: Any valid token (for higher rate limits)</li> <li>For private repositories: Token must have <code>repo</code> scope</li> <li>Repository owner and name</li> </ul>"},{"location":"CONNECTOR_USAGE/#usage","title":"Usage","text":"<pre><code># Public repository with unified auth\nexport GITHUB_TOKEN=\"ghp_xxxxxxxxxxxx\"\npython cli.py sync git --provider github \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner torvalds \\\n  --repo linux\n\n# Private repository (token must have 'repo' scope)\npython cli.py sync git --provider github \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner your-org \\\n  --repo your-private-repo\n\n# Token from environment variable (GITHUB_TOKEN) - no --auth needed\nexport GITHUB_TOKEN=\"ghp_xxxxxxxxxxxx\"\npython cli.py sync git --provider github \\\n  --db \"postgresql://...\" \\\n  --owner torvalds \\\n  --repo linux\n</code></pre>"},{"location":"CONNECTOR_USAGE/#batch-processing-multiple-repositories","title":"Batch Processing Multiple Repositories","text":"<pre><code># Process repositories matching a pattern\npython cli.py sync git --provider github \\\n  --db \"sqlite+aiosqlite:///mergestat.db\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  -s \"myorg/api-*\" \\\n  --group myorg \\\n  --batch-size 10 \\\n  --max-concurrent 4 \\\n  --max-repos 50 \\\n  --use-async\n</code></pre>"},{"location":"CONNECTOR_USAGE/#what-gets-stored","title":"What Gets Stored","text":"<ul> <li>Repository metadata (name, URL, default branch)</li> <li>Commits (up to 100 most recent)</li> <li>Commit statistics (additions, deletions per file for last 50 commits)</li> </ul>"},{"location":"CONNECTOR_USAGE/#gitlab-connector-mode","title":"GitLab Connector Mode","text":"<p>Fetch project data directly from GitLab (including self-hosted instances). Fully supports both public and private projects.</p>"},{"location":"CONNECTOR_USAGE/#requirements_1","title":"Requirements","text":"<ul> <li>GitLab private token with appropriate permissions</li> <li>For public projects: Token optional (but recommended for rate limits)</li> <li>For private projects: Token must have <code>read_api</code> and <code>read_repository</code> scopes</li> <li>Project ID (numeric ID, not path)</li> </ul>"},{"location":"CONNECTOR_USAGE/#usage_1","title":"Usage","text":"<pre><code># Public project on GitLab.com with unified auth\npython cli.py sync git --provider gitlab \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --project-id 278964\n\n# Private project (token must have required scopes)\npython cli.py sync git --provider gitlab \\\n  --db \"postgresql://...\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --project-id 12345\n\n# Self-hosted GitLab\npython cli.py sync git --provider gitlab \\\n  --db \"postgresql://...\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --gitlab-url \"https://gitlab.example.com\" \\\n  --project-id 123\n\n# Token from environment variable (GITLAB_TOKEN)\nexport GITLAB_TOKEN=\"glpat-xxxxxxxxxxxx\"\npython cli.py sync git --provider gitlab \\\n  --db \"mongodb://localhost:27017\" \\\n  --project-id 278964\n</code></pre>"},{"location":"CONNECTOR_USAGE/#batch-processing-multiple-projects","title":"Batch Processing Multiple Projects","text":"<pre><code># Process projects matching a pattern\npython cli.py sync git --provider gitlab \\\n  --db \"sqlite+aiosqlite:///mergestat.db\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --gitlab-url \"https://gitlab.com\" \\\n  --group mygroup \\\n  -s \"mygroup/api-*\" \\\n  --batch-size 10 \\\n  --max-concurrent 4 \\\n  --max-repos 50 \\\n  --use-async\n</code></pre>"},{"location":"CONNECTOR_USAGE/#what-gets-stored_1","title":"What Gets Stored","text":"<ul> <li>Project metadata (name, URL, default branch)</li> <li>Commits (up to 100 most recent)</li> <li>Commit statistics (aggregate additions/deletions for last 50 commits)</li> </ul>"},{"location":"CONNECTOR_USAGE/#finding-your-project-id","title":"Finding Your Project ID","text":"<p>The project ID is the numeric identifier for your GitLab project:</p> <ol> <li>Go to your project page on GitLab</li> <li>Look under the project name - you'll see \"Project ID: 12345\"</li> <li>Or use the GitLab API: <code>curl \"https://gitlab.com/api/v4/projects/owner%2Fproject\" --header \"PRIVATE-TOKEN: &lt;token&gt;\"</code></li> </ol>"},{"location":"CONNECTOR_USAGE/#environment-variables","title":"Environment Variables","text":"<p>All modes support these environment variables:</p> Variable Description Default <code>DATABASE_URI</code> Database connection string None (required) <code>SECONDARY_DATABASE_URI</code> Secondary database for <code>--sink both</code> None <code>DB_ECHO</code> Enable SQL logging <code>false</code> <code>BATCH_SIZE</code> Records per batch insert <code>100</code> <code>MAX_WORKERS</code> Parallel workers <code>4</code> <code>GITHUB_TOKEN</code> GitHub personal access token None <code>GITLAB_TOKEN</code> GitLab private token None <code>REPO_PATH</code> Path to local repository <code>.</code>"},{"location":"CONNECTOR_USAGE/#command-line-arguments","title":"Command-Line Arguments","text":"<p>The CLI exposes target-specific sync jobs (<code>sync git</code>, <code>sync prs</code>, <code>sync blame</code>, <code>sync cicd</code>, <code>sync deployments</code>, <code>sync incidents</code>) with a <code>--provider</code> flag.</p>"},{"location":"CONNECTOR_USAGE/#local-provider-python-clipy-sync-target-provider-local","title":"Local provider (<code>python cli.py sync &lt;target&gt; --provider local</code>)","text":"<ul> <li><code>--db DB</code> (required): Database connection string (auto-detects the backend from the URL).</li> <li><code>--repo-path PATH</code>: Path to the repository (<code>.</code> by default).</li> <li><code>--since / --date / --backfill</code>: Time filters for commits and stats.</li> <li><code>--db-type {postgres,mongo,sqlite,clickhouse}</code>: Optional override when the scheme is ambiguous.</li> </ul>"},{"location":"CONNECTOR_USAGE/#github-provider-python-clipy-sync-target-provider-github","title":"GitHub provider (<code>python cli.py sync &lt;target&gt; --provider github</code>)","text":"<p>Single-repo execution (required: <code>--owner</code> and <code>--repo</code>):</p> <ul> <li><code>--db DB</code> (required).</li> <li><code>--auth TOKEN</code>: Token overrides <code>GITHUB_TOKEN</code>.</li> <li><code>--owner OWNER</code> and <code>--repo REPO</code>: Target repository.</li> </ul> <p>Batch processing:</p> <ul> <li><code>-s, --search PATTERN</code> (required for batch).</li> <li><code>--group NAME</code>: Organization/user that owns the repositories.</li> <li><code>--batch-size N</code>, <code>--max-concurrent N</code>, <code>--rate-limit-delay SECONDS</code>: Tune pagination and throughput.</li> <li><code>--max-repos N</code>: Stop after processing N repositories.</li> <li><code>--use-async</code>: Enable async workers.</li> <li>Shared options: <code>--max-commits-per-repo</code>.</li> </ul>"},{"location":"CONNECTOR_USAGE/#gitlab-provider-python-clipy-sync-target-provider-gitlab","title":"GitLab provider (<code>python cli.py sync &lt;target&gt; --provider gitlab</code>)","text":"<ul> <li><code>--db DB</code> (required).</li> <li><code>--auth TOKEN</code>: Token overrides <code>GITLAB_TOKEN</code>.</li> <li><code>--project-id ID</code>: Required for single-project mode.</li> <li><code>--gitlab-url URL</code>: Defaults to <code>https://gitlab.com</code>.</li> <li>Batch options mirror the GitHub CLI flags (<code>-s/--search</code>, <code>--group</code>, etc.).</li> <li>Shared tuning: <code>--batch-size</code>, <code>--max-concurrent</code>, <code>--rate-limit-delay</code>, <code>--max-repos</code>, <code>--use-async</code>, <code>--max-commits-per-repo</code>.</li> </ul>"},{"location":"CONNECTOR_USAGE/#examples","title":"Examples","text":""},{"location":"CONNECTOR_USAGE/#analyze-linux-kernel-from-github","title":"Analyze Linux Kernel from GitHub","text":"<pre><code>python cli.py sync git --provider github \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner torvalds \\\n  --repo linux\n</code></pre>"},{"location":"CONNECTOR_USAGE/#analyze-gitlabs-gitlab-from-gitlabcom","title":"Analyze GitLab's GitLab from GitLab.com","text":"<pre><code>python cli.py sync git --provider gitlab \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --project-id 278964\n</code></pre>"},{"location":"CONNECTOR_USAGE/#analyze-local-repository","title":"Analyze Local Repository","text":"<pre><code>python cli.py sync git --provider local \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --repo-path \"/home/user/my-project\"\n</code></pre>"},{"location":"CONNECTOR_USAGE/#batch-process-multiple-repositories","title":"Batch Process Multiple Repositories","text":"<pre><code># GitHub batch processing\npython cli.py sync git --provider github \\\n  --db \"sqlite+aiosqlite:///mergestat.db\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --group myorg \\\n  -s \"myorg/api-*\" \\\n  --batch-size 5 \\\n  --max-repos 20 \\\n  --use-async\n\n# GitLab batch processing\npython cli.py sync git --provider gitlab \\\n  --db \"sqlite+aiosqlite:///mergestat.db\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --group mygroup \\\n  -s \"mygroup/service-*\" \\\n  --batch-size 5 \\\n  --max-repos 20 \\\n  --use-async\n</code></pre>"},{"location":"CONNECTOR_USAGE/#limitations","title":"Limitations","text":""},{"location":"CONNECTOR_USAGE/#github-connector-mode_1","title":"GitHub Connector Mode","text":"<ul> <li>Fetches up to 100 most recent commits</li> <li>Commit stats limited to last 50 commits (API rate limits)</li> <li>Does not fetch blame data (requires per-file API calls)</li> <li>Does not fetch file contents</li> </ul>"},{"location":"CONNECTOR_USAGE/#gitlab-connector-mode_1","title":"GitLab Connector Mode","text":"<ul> <li>Fetches up to 100 most recent commits</li> <li>Commit stats limited to last 50 commits (API rate limits)</li> <li>Stores aggregate stats per commit (not per-file breakdowns)</li> <li>Does not fetch blame data (requires per-file API calls)</li> <li>Does not fetch file contents</li> </ul>"},{"location":"CONNECTOR_USAGE/#rate-limits","title":"Rate Limits","text":"<ul> <li>GitHub: 5,000 requests/hour for authenticated users</li> <li>GitLab: 10 requests/second (self-hosted may vary)</li> </ul> <p>Both connectors include automatic retry with exponential backoff for rate limit handling.</p> <p>Batch modes also coordinate concurrent workers using a shared backoff gate to reduce rate-limit stampedes, and honor server-provided <code>Retry-After</code>/reset delays when available.</p>"},{"location":"CONNECTOR_USAGE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"CONNECTOR_USAGE/#connectors-are-not-available","title":"\"Connectors are not available\"","text":"<p>The connectors require additional dependencies. Install them:</p> <pre><code>pip install PyGithub python-gitlab\n</code></pre> <p>Or install all requirements:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"CONNECTOR_USAGE/#github-authentication-errors","title":"GitHub Authentication Errors","text":"<p>Ensure your token has the appropriate scopes:</p> <ul> <li><code>repo</code> - Full control of private repositories (REQUIRED for private repos)</li> <li><code>read:org</code> - Read org and team membership (recommended for organization repos)</li> </ul> <p>For private repositories: The <code>repo</code> scope is mandatory. Without it, you'll receive 404 errors when trying to access private repositories.</p> <p>To verify your token scopes:</p> <ol> <li>Go to https://github.com/settings/tokens</li> <li>Find your token and check which scopes are selected</li> <li>If <code>repo</code> is not checked, generate a new token with this scope</li> </ol>"},{"location":"CONNECTOR_USAGE/#gitlab-authentication-errors","title":"GitLab Authentication Errors","text":"<p>Ensure your token has the appropriate scopes:</p> <ul> <li><code>read_api</code> - Read access to API (REQUIRED for private projects)</li> <li><code>read_repository</code> - Read repository data (REQUIRED for private projects)</li> </ul> <p>For private projects: Both scopes are mandatory. Without them, you'll receive authentication or permission errors.</p> <p>To verify your token permissions:</p> <ol> <li>Go to your GitLab instance Settings \u2192 Access Tokens</li> <li>Review the token's scopes</li> <li>If needed, create a new token with <code>read_api</code> and <code>read_repository</code> scopes</li> </ol>"},{"location":"CONNECTOR_USAGE/#finding-repositoryproject-ids","title":"Finding Repository/Project IDs","text":"<p>GitHub: Use owner/repo format (e.g., <code>torvalds/linux</code>) GitLab: Use the numeric project ID (e.g., <code>278964</code>)</p>"},{"location":"CONNECTOR_USAGE/#integration-with-existing-workflows","title":"Integration with Existing Workflows","text":"<p>The connector modes integrate seamlessly with the existing storage system:</p> <ul> <li>All data is stored in the same database schema</li> <li>Repository metadata tagged with source (<code>github</code> or <code>gitlab</code>)</li> <li>Can mix local and remote repositories in the same database</li> <li>Query data the same way regardless of source</li> </ul>"},{"location":"CONNECTOR_USAGE/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Local mode: Best for comprehensive analysis (files, blame, full history)</li> <li>GitHub/GitLab modes: Faster for basic commits and stats</li> <li>API rate limits: GitHub and GitLab have rate limits; local mode has none</li> <li>Network dependency: Connector modes require internet access</li> </ul> <p>Choose the appropriate mode based on your needs:</p> <ul> <li>Use local mode for complete repository analysis</li> <li>Use connector modes for quick commit and stats analysis without cloning</li> </ul>"},{"location":"PRIVATE_REPO_TESTING/","title":"Testing Private Repository Access","text":"<p>This document explains how to verify that the GitHub and GitLab connectors work correctly with private repositories and projects.</p>"},{"location":"PRIVATE_REPO_TESTING/#overview","title":"Overview","text":"<p>The connectors fully support private repositories when provided with tokens that have the appropriate permissions:</p> <ul> <li>GitHub: Requires <code>repo</code> scope for private repository access</li> <li>GitLab: Requires <code>read_api</code> and <code>read_repository</code> scopes for private project access</li> </ul>"},{"location":"PRIVATE_REPO_TESTING/#quick-test","title":"Quick Test","text":"<p>Run the example script to verify private repository access:</p> <pre><code># Set up GitHub credentials\nexport GITHUB_TOKEN=ghp_your_token_with_repo_scope\nexport GITHUB_PRIVATE_REPO=your-username/your-private-repo\n\n# Set up GitLab credentials\nexport GITLAB_TOKEN=glpat_your_token\nexport GITLAB_PRIVATE_PROJECT=your-group/your-private-project\n\n# Run the test\npython examples/private_repo_example.py\n</code></pre>"},{"location":"PRIVATE_REPO_TESTING/#detailed-testing","title":"Detailed Testing","text":""},{"location":"PRIVATE_REPO_TESTING/#github-private-repository-testing","title":"GitHub Private Repository Testing","text":""},{"location":"PRIVATE_REPO_TESTING/#1-create-a-github-token","title":"1. Create a GitHub Token","text":"<ol> <li>Go to https://github.com/settings/tokens</li> <li>Click \"Generate new token\" \u2192 \"Generate new token (classic)\"</li> <li>Give it a descriptive name: \"MergeStat Private Repo Test\"</li> <li>Select scopes:</li> <li>\u2705 repo (Full control of private repositories)</li> <li>\u2705 read:org (optional, for organization repos)</li> <li>Click \"Generate token\" and copy it immediately</li> </ol>"},{"location":"PRIVATE_REPO_TESTING/#2-set-environment-variables","title":"2. Set Environment Variables","text":"<pre><code># Your GitHub token (with 'repo' scope)\nexport GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n# Your private repository (format: owner/repo)\nexport GITHUB_PRIVATE_REPO=myusername/my-private-repo\n</code></pre>"},{"location":"PRIVATE_REPO_TESTING/#3-run-tests","title":"3. Run Tests","text":"<p>Option A: Run the example script</p> <pre><code>python examples/private_repo_example.py\n</code></pre> <p>Option B: Run integration tests</p> <pre><code>pytest tests/test_private_repo_access.py::TestGitHubPrivateRepoAccess -v\n</code></pre> <p>Option C: Test with actual data sync</p> <pre><code>python cli.py sync git --provider github \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner myusername \\\n  --repo my-private-repo\n</code></pre>"},{"location":"PRIVATE_REPO_TESTING/#4-verify-success","title":"4. Verify Success","text":"<p>The test should:</p> <ul> <li>\u2705 Successfully list the private repository</li> <li>\u2705 Fetch repository statistics</li> <li>\u2705 Get contributors list</li> <li>\u2705 Access pull requests</li> <li>\u2705 Check rate limit status</li> </ul>"},{"location":"PRIVATE_REPO_TESTING/#gitlab-private-project-testing","title":"GitLab Private Project Testing","text":""},{"location":"PRIVATE_REPO_TESTING/#1-create-a-gitlab-token","title":"1. Create a GitLab Token","text":"<ol> <li>Go to your GitLab instance Settings \u2192 Access Tokens</li> <li>Create a new token:</li> <li>Name: \"MergeStat Private Project Test\"</li> <li>Scopes:<ul> <li>\u2705 read_api (Read API access)</li> <li>\u2705 read_repository (Read repository content)</li> </ul> </li> <li>Click \"Create personal access token\" and copy it</li> </ol>"},{"location":"PRIVATE_REPO_TESTING/#2-set-environment-variables_1","title":"2. Set Environment Variables","text":"<pre><code># Your GitLab token (with required scopes)\nexport GITLAB_TOKEN=glpat-xxxxxxxxxxxxxxxxxxxx\n\n# Your private project (format: group/project or numeric ID)\nexport GITLAB_PRIVATE_PROJECT=mygroup/my-private-project\n# OR\nexport GITLAB_PRIVATE_PROJECT=12345\n\n# Optional: Custom GitLab instance URL\nexport GITLAB_URL=https://gitlab.example.com\n</code></pre>"},{"location":"PRIVATE_REPO_TESTING/#3-run-tests_1","title":"3. Run Tests","text":"<p>Option A: Run the example script</p> <pre><code>python examples/private_repo_example.py\n</code></pre> <p>Option B: Run integration tests</p> <pre><code>pytest tests/test_private_repo_access.py::TestGitLabPrivateProjectAccess -v\n</code></pre> <p>Option C: Test with actual data sync</p> <pre><code>python cli.py sync git --provider gitlab \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --project-id 12345\n</code></pre>"},{"location":"PRIVATE_REPO_TESTING/#4-verify-success_1","title":"4. Verify Success","text":"<p>The test should:</p> <ul> <li>\u2705 Successfully access the private project</li> <li>\u2705 Fetch project statistics</li> <li>\u2705 Get contributors list</li> <li>\u2705 Access merge requests</li> <li>\u2705 List accessible projects (including private)</li> </ul>"},{"location":"PRIVATE_REPO_TESTING/#troubleshooting","title":"Troubleshooting","text":""},{"location":"PRIVATE_REPO_TESTING/#github-issues","title":"GitHub Issues","text":"<p>Problem: <code>404 Not Found</code> error</p> <ul> <li>Cause: Token doesn't have access to the repository or lacks <code>repo</code> scope</li> <li>Solution:</li> <li>Verify the repository exists and you have access</li> <li>Check token has <code>repo</code> scope at https://github.com/settings/tokens</li> <li>Generate a new token with correct scopes if needed</li> </ul> <p>Problem: <code>401 Unauthorized</code> error</p> <ul> <li>Cause: Invalid or expired token</li> <li>Solution: Generate a new token</li> </ul> <p>Problem: Rate limit exceeded</p> <ul> <li>Cause: Too many API requests</li> <li>Solution: Wait for rate limit to reset or use a different token</li> </ul>"},{"location":"PRIVATE_REPO_TESTING/#gitlab-issues","title":"GitLab Issues","text":"<p>Problem: <code>404 Not Found</code> or <code>401 Unauthorized</code> error</p> <ul> <li>Cause: Token doesn't have access to the project or lacks required scopes</li> <li>Solution:</li> <li>Verify the project exists and you have access</li> <li>Check token has <code>read_api</code> and <code>read_repository</code> scopes</li> <li>Generate a new token with correct scopes if needed</li> </ul> <p>Problem: Project ID vs Project Path</p> <ul> <li>Cause: Using project path when numeric ID is expected or vice versa</li> <li>Solution:</li> <li>For <code>cli.py sync git --provider gitlab</code>: Use numeric project ID</li> <li>For connector methods: Can use either project name or ID</li> </ul> <p>Problem: Self-hosted GitLab connection issues</p> <ul> <li>Cause: Incorrect URL or network issues</li> <li>Solution: Verify <code>GITLAB_URL</code> is correct and accessible</li> </ul>"},{"location":"PRIVATE_REPO_TESTING/#verifying-token-scopes","title":"Verifying Token Scopes","text":""},{"location":"PRIVATE_REPO_TESTING/#github-token-verification","title":"GitHub Token Verification","text":"<pre><code>from connectors import GitHubConnector\n\ntoken = \"ghp_your_token\"\nconnector = GitHubConnector(token=token)\n\n# If this works, token is valid\nrate_limit = connector.get_rate_limit()\nprint(f\"Token is valid. Rate limit: {rate_limit['remaining']}/{rate_limit['limit']}\")\n\n# If you can list private repos, token has 'repo' scope\nrepos = connector.list_repositories(max_repos=5)\nprint(f\"Can access {len(repos)} repositories\")\n\nconnector.close()\n</code></pre>"},{"location":"PRIVATE_REPO_TESTING/#gitlab-token-verification","title":"GitLab Token Verification","text":"<pre><code>from connectors import GitLabConnector\n\ntoken = \"glpat_your_token\"\nconnector = GitLabConnector(url=\"https://gitlab.com\", private_token=token)\n\n# If this works, token is valid\ntry:\n    projects = connector.list_projects(max_projects=5)\n    print(f\"Token is valid. Can access {len(projects)} projects\")\nexcept Exception as e:\n    print(f\"Token validation failed: {e}\")\n\nconnector.close()\n</code></pre>"},{"location":"PRIVATE_REPO_TESTING/#best-practices","title":"Best Practices","text":"<ol> <li>Never commit tokens to version control</li> <li>Use environment variables</li> <li>Add tokens to <code>.gitignore</code></li> <li> <p>Use secret management tools in production</p> </li> <li> <p>Use minimal required scopes</p> </li> <li>GitHub: <code>repo</code> for private repos, no scope needed for public</li> <li> <p>GitLab: <code>read_api</code> + <code>read_repository</code> for private projects</p> </li> <li> <p>Rotate tokens regularly</p> </li> <li>Generate new tokens periodically</li> <li> <p>Revoke old tokens when no longer needed</p> </li> <li> <p>Test with both public and private repositories</p> </li> <li>Ensure your setup works for both cases</li> <li> <p>Verify error handling for insufficient permissions</p> </li> <li> <p>Monitor rate limits</p> </li> <li>Check rate limit status regularly</li> <li>Implement backoff strategies for production use</li> </ol>"},{"location":"PRIVATE_REPO_TESTING/#integration-tests","title":"Integration Tests","text":"<p>Run the full test suite:</p> <pre><code># Run all private repo tests\npytest tests/test_private_repo_access.py -v\n\n# Run specific test classes\npytest tests/test_private_repo_access.py::TestGitHubPrivateRepoAccess -v\npytest tests/test_private_repo_access.py::TestGitLabPrivateProjectAccess -v\n\n# Skip integration tests in CI/CD\nexport SKIP_INTEGRATION_TESTS=1\npytest tests/test_private_repo_access.py -v\n</code></pre>"},{"location":"PRIVATE_REPO_TESTING/#cicd-considerations","title":"CI/CD Considerations","text":"<p>When running tests in CI/CD pipelines:</p> <ol> <li>Store tokens as secrets</li> <li>GitHub Actions: Use repository secrets</li> <li>GitLab CI: Use CI/CD variables (masked)</li> <li> <p>Other CI: Use secure secret storage</p> </li> <li> <p>Skip integration tests by default</p> </li> </ol> <pre><code># GitHub Actions example\n- name: Run tests\n  env:\n    SKIP_INTEGRATION_TESTS: 1\n  run: pytest\n</code></pre> <ol> <li>Optional: Run integration tests with secrets</li> </ol> <pre><code># Only run on main branch or with specific label\n- name: Run integration tests\n  if: github.ref == 'refs/heads/main'\n  env:\n    GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n    GITHUB_PRIVATE_REPO: ${{ secrets.TEST_PRIVATE_REPO }}\n  run: pytest tests/test_private_repo_access.py -v\n</code></pre>"},{"location":"PRIVATE_REPO_TESTING/#support","title":"Support","text":"<p>If you encounter issues:</p> <ol> <li>Check this documentation for troubleshooting steps</li> <li>Verify your token scopes and permissions</li> <li>Review the connector documentation in <code>connectors/README.md</code></li> <li>Check example scripts in <code>examples/</code> directory</li> <li>Review test files for usage patterns</li> </ol>"},{"location":"architecture/","title":"Architecture","text":"<p>The project follows a pipeline-style architecture that separates data collection, processing, storage, and analysis.</p>"},{"location":"architecture/#pipeline-stages","title":"Pipeline stages","text":"<ol> <li>Connectors (<code>connectors/</code>)</li> <li>Fetch raw data from providers (GitHub, GitLab, Jira).</li> <li>Processors (<code>processors/</code>)</li> <li>Normalize and enrich connector payloads.</li> <li>Storage (<code>storage.py</code>, <code>models/</code>)</li> <li>Persist processed data into PostgreSQL, ClickHouse, MongoDB, or SQLite.</li> <li>Metrics (<code>metrics/</code>)</li> <li>Compute high-level metrics like throughput, cycle time, rework, and predictability.</li> <li>Visualization (<code>grafana/</code>)</li> <li>Provision dashboards for exploration and reporting.</li> </ol>"},{"location":"architecture/#storage-backends","title":"Storage backends","text":"<ul> <li>PostgreSQL for relational storage with Alembic migrations.</li> <li>ClickHouse for analytics-heavy queries.</li> <li>MongoDB for document storage.</li> <li>SQLite for local development.</li> </ul>"},{"location":"architecture/#cli-entry-points","title":"CLI entry points","text":"<p>The CLI is implemented with argparse in <code>cli.py</code> and orchestrates sync and metrics workflows.</p>"},{"location":"architecture/#work-unit-investment-payload","title":"Work unit investment payload","text":"<p>The Work Unit Investment API payloads include optional <code>work_unit_type</code> and <code>work_unit_name</code> fields for UI labels. These fields are intended to be exposed through GraphQL later unchanged.</p>"},{"location":"architecture/#canonical-investment-view","title":"Canonical investment view","text":"<p>Investment categorization is computed at job time and persisted as distributions; UX-time systems may explain but must not recompute.</p> <ul> <li>Concepts: <code>product/concepts.md</code></li> <li>Categorization contract: <code>llm/categorization-contract.md</code></li> <li>Investment View: <code>user-guide/investment-view.md</code></li> </ul>"},{"location":"canonical-metrics/","title":"Canonical Metrics &amp; Data Mapping","text":"<p>This document provides the authoritative mapping between API views, metric keys, and the underlying database schema. It is intended to guide the implementation of the Explain endpoint and other analytical features.</p>"},{"location":"canonical-metrics/#metric-registry","title":"Metric Registry","text":"<p>The following table defines the canonical metrics exposed via the API, their database source, and interpretation.</p> Metric Key Label Unit Table Column Aggregation Scope Description <code>cycle_time</code> Cycle Time days <code>work_item_metrics_daily</code> <code>cycle_time_p50_hours</code> Avg (converted to days) Team Time from work start to completion. Lower is better. <code>review_latency</code> Review Latency hours <code>repo_metrics_daily</code> <code>pr_first_review_p50_hours</code> Avg Repo Time from PR creation to first review. Lower is better. <code>throughput</code> Throughput items <code>work_item_metrics_daily</code> <code>items_completed</code> Sum Team Count of completed work items. Higher is better. <code>deploy_freq</code> Deploy Frequency deploys <code>deploy_metrics_daily</code> <code>deployments_count</code> Sum Repo Number of deployments to production. Higher is better. <code>churn</code> Code Churn loc <code>repo_metrics_daily</code> <code>total_loc_touched</code> Sum Repo Total lines of code modified. High churn may indicate instability or rework. <code>wip_saturation</code> WIP Saturation % <code>work_item_metrics_daily</code> <code>wip_congestion_ratio</code> Avg Team Ratio of active items to developer capacity. Lower is better. <code>blocked_work</code> Blocked Work hours <code>work_item_state_durations_daily</code> <code>duration_hours</code> Sum Team Total time items spent in a blocked state. Lower is better. <code>change_failure_rate</code> Change Failure Rate % <code>repo_metrics_daily</code> <code>change_failure_rate</code> Avg Repo Percentage of deployments causing failure. Lower is better. <code>rework_ratio</code> Rework Ratio % <code>repo_metrics_daily</code> <code>rework_churn_ratio_30d</code> Avg Repo Ratio of churn in recently modified code. Lower is better. <code>ci_success</code> CI Success Rate % <code>cicd_metrics_daily</code> <code>success_rate</code> Avg Repo Percentage of successful CI pipeline runs. Higher is better."},{"location":"canonical-metrics/#person-level-metrics","title":"Person-Level Metrics","text":"<p>Person-level metrics use <code>identity_id</code> or <code>user_identity</code> to filter and group data.</p> Metric Key Label Unit Table Column Aggregation Description <code>cycle_time</code> Cycle Time days <code>work_item_user_metrics_daily</code> <code>cycle_time_p50_hours</code> Avg Individual cycle time. <code>review_latency</code> Review Latency hours <code>user_metrics_daily</code> <code>pr_first_review_p50_hours</code> Avg Time to review PRs assigned to this user. <code>throughput</code> Throughput items <code>work_item_user_metrics_daily</code> <code>items_completed</code> Sum Items completed by the user. <code>churn</code> Code Churn loc <code>user_metrics_daily</code> <code>loc_touched</code> Sum LOC touched by the user. <code>wip_overlap</code> WIP Overlap items <code>work_item_user_metrics_daily</code> <code>wip_count_end_of_day</code> Avg Average concurrent items in progress. <code>blocked_work</code> Blocked Work items <code>work_item_cycle_times</code> <code>status='blocked'</code> Sum Count of items blocked while assigned to user."},{"location":"canonical-metrics/#view-api-mapping","title":"View -&gt; API Mapping","text":"<p>Data is exposed through specific API endpoints that consume these metrics.</p>"},{"location":"canonical-metrics/#apiv1explain","title":"<code>/api/v1/explain</code>","text":"<ul> <li>Purpose: Provides detailed breakdown of a specific metric (drivers, contributors, drill-down).</li> <li>Parameters: <code>metric</code> (key from registry), <code>scope_type</code>, <code>scope_id</code>, <code>range_days</code>, <code>compare_days</code>.</li> <li>Response: Current value, delta %, top drivers (teams/repos contributing), and individual contributors (PRs/Issues).</li> </ul>"},{"location":"canonical-metrics/#apiv1home","title":"<code>/api/v1/home</code>","text":"<ul> <li>Purpose: Executive dashboard with high-level health signals.</li> <li>Metrics Used: All metrics in the registry are candidates for \"Insight Cards\" or \"Sparklines\".</li> <li>Logic: Computes deltas for all metrics and highlights the \"Constraint of the Week\" (metric with worst regression).</li> </ul>"},{"location":"canonical-metrics/#apiv1peopleperson_idmetric","title":"<code>/api/v1/people/{person_id}/metric</code>","text":"<ul> <li>Purpose: Detailed view for a single person's metric.</li> <li>Parameters: <code>person_id</code>, <code>metric</code>.</li> <li>Response: Timeseries, breakdown by dimension (repo, work_type, stage).</li> </ul>"},{"location":"canonical-metrics/#filters-scopes","title":"Filters &amp; Scopes","text":"<p>All metrics support standard filtering patterns.</p>"},{"location":"canonical-metrics/#time-window","title":"Time Window","text":"<ul> <li><code>start_day</code>, <code>end_day</code>: Explicit date range (inclusive).</li> <li><code>range_days</code>: Lookback period from today (e.g., last 14 days).</li> <li><code>compare_days</code>: Comparison period duration (usually matches <code>range_days</code>).</li> </ul>"},{"location":"canonical-metrics/#scope-scope_type","title":"Scope (<code>scope_type</code>)","text":"<ul> <li><code>org</code>: Aggregates data across the entire organization.</li> <li><code>team</code>: Filters by <code>team_id</code>.</li> <li>Maps to <code>team_id</code> column in <code>work_item_*</code> tables.</li> <li>Requires joining <code>repos</code> on <code>owner_team_id</code> for repo-scoped metrics (e.g., <code>churn</code>).</li> <li><code>repo</code>: Filters by <code>repo_id</code>.</li> <li>Maps to <code>repo_id</code> column in <code>repo_*</code> tables.</li> <li>For team-scoped metrics, this filter may not apply directly unless the team is mapped to the repo.</li> </ul>"},{"location":"canonical-metrics/#database-schema-reference","title":"Database Schema Reference","text":"<p>The metrics are derived from the following core ClickHouse tables:</p> <ul> <li><code>work_item_metrics_daily</code>: Aggregated daily stats for work items (Jira/GitLab Issues).</li> <li><code>repo_metrics_daily</code>: Aggregated daily stats for git repositories (Commits, PRs).</li> <li><code>deploy_metrics_daily</code>: Deployment events and frequencies.</li> <li><code>work_item_state_durations_daily</code>: Time spent in each workflow state (Active, Blocked, etc.).</li> <li><code>user_metrics_daily</code>: Developer activity stats (Commits, PR reviews).</li> <li><code>work_item_user_metrics_daily</code>: Developer delivery stats (Assigned items).</li> </ul>"},{"location":"cli/","title":"CLI","text":"<p>The CLI is the primary way to run sync jobs, compute metrics, and manage dashboards.</p>"},{"location":"cli/#common-commands","title":"Common commands","text":""},{"location":"cli/#sync-local-git-data","title":"Sync local Git data","text":"<pre><code>python cli.py sync git --provider local --db \"&lt;DB_CONN&gt;\" --repo-path /path/to/repo\n</code></pre>"},{"location":"cli/#sync-teams","title":"Sync teams","text":"<pre><code>python cli.py sync teams --provider config --db \"&lt;DB_CONN&gt;\" --path /path/to/teams.yml\n</code></pre>"},{"location":"cli/#sync-work-items","title":"Sync work items","text":"<pre><code>python cli.py sync work-items --provider github --auth \"$GITHUB_TOKEN\" -s \"org/*\" --db \"&lt;DB_CONN&gt;\"\n</code></pre>"},{"location":"cli/#metrics","title":"Metrics","text":"<pre><code>python cli.py metrics daily --db \"&lt;DB_CONN&gt;\"\npython cli.py metrics complexity --repo-path . -s \"*\" --db \"&lt;DB_CONN&gt;\"\n</code></pre> <p><code>metrics daily</code> defaults to <code>--provider auto</code>, which loads work items from the database only.</p>"},{"location":"cli/#fixtures","title":"Fixtures","text":"<pre><code>python cli.py fixtures generate --db \"&lt;DB_CONN&gt;\" --days 30\n</code></pre>"},{"location":"cli/#flags-and-overrides","title":"Flags and overrides","text":"<p>CLI flags override environment variables. Use <code>--db</code> or <code>DATABASE_URI</code> to target a specific database.</p>"},{"location":"getting-started/","title":"Getting started","text":""},{"location":"getting-started/#install","title":"Install","text":"<p>If you are using the project as a package:</p> <pre><code>pip install dev-health-ops\n</code></pre> <p>If you are working from source:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/#docs-site","title":"Docs site","text":"<pre><code>pip install -r requirements-docs.txt\nmkdocs serve\n</code></pre>"},{"location":"getting-started/#quick-start","title":"Quick start","text":""},{"location":"getting-started/#sync-a-local-repository","title":"Sync a local repository","text":"<pre><code>python cli.py sync git --provider local --db \"&lt;DB_CONN&gt;\" --repo-path /path/to/repo\n</code></pre>"},{"location":"getting-started/#sync-work-items-from-github","title":"Sync work items from GitHub","text":"<pre><code>python cli.py sync work-items --provider github --auth \"$GITHUB_TOKEN\" -s \"org/*\" --db \"&lt;DB_CONN&gt;\"\n</code></pre>"},{"location":"getting-started/#compute-daily-metrics","title":"Compute daily metrics","text":"<pre><code>python cli.py metrics daily --db \"&lt;DB_CONN&gt;\"\n</code></pre>"},{"location":"getting-started/#bring-up-grafana-dashboards","title":"Bring up Grafana dashboards","text":"<pre><code>docker compose -f compose.yml up -d\n</code></pre>"},{"location":"getting-started/#environment-notes","title":"Environment notes","text":"<p>CLI flags override environment variables. Common env vars:</p> <ul> <li><code>DATABASE_URI</code></li> <li><code>SECONDARY_DATABASE_URI</code></li> <li><code>GITHUB_TOKEN</code></li> <li><code>GITLAB_TOKEN</code></li> <li><code>REPO_PATH</code></li> </ul>"},{"location":"grafana/","title":"Grafana","text":"<p>This repo ships Grafana provisioning for ClickHouse dashboards under <code>grafana/</code>.</p>"},{"location":"grafana/#start-grafana-clickhouse-dev","title":"Start Grafana + ClickHouse (dev)","text":"<p>From the repo root, use Docker Compose directly (the CLI grafana command is deprecated):</p> <pre><code>docker compose -f compose.yml up -d\n</code></pre> <ul> <li>Grafana: <code>http://localhost:3000</code> (default <code>admin</code> / <code>admin</code>)</li> <li>ClickHouse HTTP: <code>http://localhost:8123</code></li> </ul>"},{"location":"grafana/#datasource-provisioning","title":"Datasource provisioning","text":"<p>Provisioning file: - <code>grafana/datasources/clickhouse.yaml</code></p> <p>It reads connection settings from environment variables inside the Grafana container: - <code>CLICKHOUSE_HOST</code> (defaults to <code>clickhouse</code> in the provided docker compose) - <code>CLICKHOUSE_PORT</code> (defaults to <code>8123</code>) - <code>CLICKHOUSE_DB</code> - <code>CLICKHOUSE_USER</code> - <code>CLICKHOUSE_PASSWORD</code></p>"},{"location":"grafana/#dashboards","title":"Dashboards","text":"<p>Dashboards are provisioned automatically into the \u201cDeveloper Health\u201d folder: - Repo Health: <code>grafana/dashboards/repo_health.json</code> - Code Hotspots: <code>grafana/dashboards/code_hotspots.json</code> - Work Tracking: <code>grafana/dashboards/work_tracking.json</code> - Advanced Work Tracking: <code>grafana/dashboards/advanced_work_tracking.json</code> - Collaboration: <code>grafana/dashboards/collaboration.json</code> - Quality &amp; Risk: <code>grafana/dashboards/quality_risk.json</code> - Well-being (team-level): <code>grafana/dashboards/wellbeing.json</code></p> <p>The Work Tracking dashboard supports filtering by <code>provider</code>, <code>team_id</code>, and <code>work_scope_id</code> (Jira project key / GitHub repo or project board / GitLab project path).</p>"},{"location":"grafana/#expected-clickhouse-tables","title":"Expected ClickHouse tables","text":"<p>Git facts (synced via the CLI, e.g., <code>python cli.py sync ...</code>): - <code>repos</code> - <code>git_commits</code> - <code>git_commit_stats</code> - <code>git_pull_requests</code></p> <p>Derived metrics (computed by <code>scripts/compute_metrics_daily.py</code>): - <code>repo_metrics_daily</code> - <code>user_metrics_daily</code> - <code>commit_metrics</code> - <code>team_metrics_daily</code> - <code>work_item_metrics_daily</code> - <code>work_item_user_metrics_daily</code> - <code>work_item_cycle_times</code></p>"},{"location":"grafana/#populate-data","title":"Populate data","text":"<p>1) Sync git data into ClickHouse:</p> <pre><code>python cli.py sync git --provider local --db \"clickhouse://localhost:8123/default\" --repo-path .\n</code></pre> <p>2) Compute derived metrics:</p> <pre><code>python cli.py sync work-items --provider all --date 2025-02-01 --backfill 30 --db \"clickhouse://localhost:8123/default\"\npython cli.py metrics daily --date 2025-02-01 --backfill 30 --db \"clickhouse://localhost:8123/default\"\n</code></pre> <p>Work tracking providers require credentials; see <code>docs/task_trackers.md</code>.</p>"},{"location":"grafana/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>The provisioned dashboards use repo names (e.g. <code>org/repo</code>) in the Repo picker and support multi-select + \u201cAll\u201d. Repo filtering is applied by resolving selected repo names to <code>repos.id</code> in ClickHouse.</li> </ul>"},{"location":"graphql/","title":"GraphQL Analytics API","text":"<p>The GraphQL analytics API provides a read-only query interface for dev-health-ops analytics data. All queries compile allowlisted primitives into parameterized SQL\u2014no arbitrary queries are permitted.</p>"},{"location":"graphql/#endpoint","title":"Endpoint","text":"<pre><code>POST /graphql\n</code></pre>"},{"location":"graphql/#authentication","title":"Authentication","text":"<p>All queries require <code>org_id</code> as a query parameter. This scopes all data access to the specified organization.</p>"},{"location":"graphql/#schema-overview","title":"Schema Overview","text":""},{"location":"graphql/#querycatalog","title":"Query.catalog","text":"<p>Fetch available dimensions, measures, and cost limits.</p> <pre><code>query Catalog($orgId: String!, $dimension: DimensionInput) {\n  catalog(orgId: $orgId, dimension: $dimension) {\n    dimensions { name description }\n    measures { name description }\n    limits {\n      maxDays\n      maxBuckets\n      maxTopN\n      maxSankeyNodes\n      maxSankeyEdges\n      maxSubRequests\n    }\n    values { value count }  # Only when dimension is specified\n  }\n}\n</code></pre>"},{"location":"graphql/#queryanalytics","title":"Query.analytics","text":"<p>Execute batch analytics queries including timeseries, breakdowns, and Sankey flows.</p> <pre><code>query Analytics($orgId: String!, $batch: AnalyticsRequestInput!) {\n  analytics(orgId: $orgId, batch: $batch) {\n    timeseries {\n      dimension\n      dimensionValue\n      measure\n      buckets { date value }\n    }\n    breakdowns {\n      dimension\n      measure\n      items { key value }\n    }\n    sankey {\n      nodes { id label dimension value }\n      edges { source target value }\n    }\n  }\n}\n</code></pre>"},{"location":"graphql/#example-queries","title":"Example Queries","text":""},{"location":"graphql/#1-catalog-values-for-team-and-repo","title":"1. Catalog Values for TEAM and REPO","text":"<pre><code># Fetch available teams\nquery TeamValues {\n  catalog(orgId: \"my-org\", dimension: TEAM) {\n    values { value count }\n  }\n}\n\n# Fetch available repos\nquery RepoValues {\n  catalog(orgId: \"my-org\", dimension: REPO) {\n    values { value count }\n  }\n}\n</code></pre>"},{"location":"graphql/#2-analytics-with-timeseries-breakdown-and-sankey","title":"2. Analytics with Timeseries, Breakdown, and Sankey","text":"<pre><code>query FullAnalytics {\n  analytics(\n    orgId: \"my-org\"\n    batch: {\n      timeseries: [{\n        dimension: THEME\n        measure: COUNT\n        interval: WEEK\n        dateRange: {\n          startDate: \"2025-01-01\"\n          endDate: \"2025-01-31\"\n        }\n      }]\n      breakdowns: [{\n        dimension: REPO\n        measure: CHURN_LOC\n        dateRange: {\n          startDate: \"2025-01-01\"\n          endDate: \"2025-01-31\"\n        }\n        topN: 10\n      }]\n      sankey: {\n        path: [WORK_TYPE, REPO, TEAM]\n        measure: COUNT\n        dateRange: {\n          startDate: \"2025-01-01\"\n          endDate: \"2025-01-31\"\n        }\n        maxNodes: 50\n        maxEdges: 200\n      }\n    }\n  ) {\n    timeseries {\n      dimension\n      dimensionValue\n      buckets { date value }\n    }\n    breakdowns {\n      dimension\n      items { key value }\n    }\n    sankey {\n      nodes { id label value }\n      edges { source target value }\n    }\n  }\n}\n</code></pre>"},{"location":"graphql/#curl-example-sankey-coverage","title":"Curl example (Sankey coverage)","text":"<pre><code>curl -s -X POST \"http://localhost:8000/graphql?org_id=default\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-Org-Id: default\" \\\n  -d '{\n    \"query\": \"query CoverageSankey($orgId: String!, $batch: AnalyticsRequestInput!) { analytics(orgId: $orgId, batch: $batch) { sankey { coverage { teamCoverage repoCoverage } nodes { id label dimension value } edges { source target value } } } }\",\n    \"variables\": {\n      \"orgId\": \"default\",\n      \"batch\": {\n        \"useInvestment\": true,\n        \"timeseries\": [],\n        \"breakdowns\": [],\n        \"sankey\": {\n          \"path\": [\"WORK_TYPE\", \"REPO\", \"TEAM\"],\n          \"measure\": \"COUNT\",\n          \"dateRange\": { \"startDate\": \"2026-01-01\", \"endDate\": \"2026-01-31\" }\n        }\n      }\n    }\n  }'\n</code></pre>"},{"location":"graphql/#curl-example-investment-breakdowns-via-dev-health-web","title":"Curl example (Investment breakdowns via dev-health-web)","text":"<pre><code>curl -s -X POST \"http://localhost:3000/graphql?org_id=default\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-Org-Id: default\" \\\n  -d '{\n    \"query\": \"query InvestmentBreakdown($orgId: String!, $batch: AnalyticsRequestInput!) { analytics(orgId: $orgId, batch: $batch) { breakdowns { dimension measure items { key value } } } }\",\n    \"variables\": {\n      \"orgId\": \"default\",\n      \"batch\": {\n        \"useInvestment\": true,\n        \"breakdowns\": [\n          { \"dimension\": \"THEME\", \"measure\": \"COUNT\", \"dateRange\": { \"startDate\": \"2026-01-06\", \"endDate\": \"2026-01-20\" }, \"topN\": 50 },\n          { \"dimension\": \"SUBCATEGORY\", \"measure\": \"COUNT\", \"dateRange\": { \"startDate\": \"2026-01-06\", \"endDate\": \"2026-01-20\" }, \"topN\": 100 }\n        ],\n        \"filters\": { \"scope\": { \"level\": \"ORG\", \"ids\": [] } }\n      }\n    }\n  }'\n</code></pre>"},{"location":"graphql/#available-dimensions","title":"Available Dimensions","text":"Dimension Description <code>TEAM</code> Team identifier <code>REPO</code> Repository identifier <code>AUTHOR</code> Author/contributor identifier <code>WORK_TYPE</code> Type of work item (issue, PR, etc.) <code>THEME</code> Investment theme category <code>SUBCATEGORY</code> Investment subcategory"},{"location":"graphql/#available-measures","title":"Available Measures","text":"Measure Description <code>COUNT</code> Count of work units <code>CHURN_LOC</code> Lines of code changed <code>CYCLE_TIME_HOURS</code> Average cycle time in hours <code>THROUGHPUT</code> Distinct work units completed"},{"location":"graphql/#bucket-intervals","title":"Bucket Intervals","text":"<p>For timeseries queries: <code>DAY</code>, <code>WEEK</code>, <code>MONTH</code></p>"},{"location":"graphql/#cost-limits","title":"Cost Limits","text":"<p>All requests are subject to cost limits to protect database performance:</p> Limit Default Description <code>maxDays</code> 365 Maximum date range in days <code>maxBuckets</code> 100 Maximum timeseries buckets <code>maxTopN</code> 50 Maximum breakdown items <code>maxSankeyNodes</code> 100 Maximum Sankey nodes <code>maxSankeyEdges</code> 500 Maximum Sankey edges <code>maxSubRequests</code> 10 Maximum queries in a batch <p>Requests exceeding limits return a <code>COST_LIMIT_EXCEEDED</code> error:</p> <pre><code>{\n  \"errors\": [{\n    \"message\": \"Date range of 400 days exceeds limit of 365\",\n    \"extensions\": {\n      \"code\": \"COST_LIMIT_EXCEEDED\",\n      \"limit_name\": \"max_days\",\n      \"limit_value\": 365,\n      \"requested_value\": 400\n    }\n  }]\n}\n</code></pre>"},{"location":"graphql/#error-codes","title":"Error Codes","text":"Code Description <code>COST_LIMIT_EXCEEDED</code> Request exceeds a cost limit <code>VALIDATION_ERROR</code> Invalid dimension, measure, or input <code>AUTHORIZATION_ERROR</code> Missing or invalid org_id <code>PERSISTED_QUERY_ERROR</code> Unknown query ID or version mismatch <code>QUERY_TIMEOUT</code> Query exceeded timeout limit"},{"location":"graphql/#persisted-queries","title":"Persisted Queries","text":"<p>For performance, you can use persisted query IDs instead of sending the full query text:</p> <pre><code>curl -X POST /graphql \\\n  -H \"X-Persisted-Query-Id: catalog-dimensions\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"variables\": {\"orgId\": \"my-org\"}}'\n</code></pre> <p>Available persisted queries:</p> <ul> <li><code>catalog-dimensions</code> \u2013 Fetch catalog with dimensions and measures</li> <li><code>team-values</code> \u2013 Fetch distinct team values</li> <li><code>repo-values</code> \u2013 Fetch distinct repo values</li> </ul>"},{"location":"graphql/#security-notes","title":"Security Notes","text":"<ol> <li>No arbitrary SQL: All queries are compiled from allowlisted primitives</li> <li>org_id enforcement: Required on all queries, enforced in all SQL WHERE clauses</li> <li>Parameterized SQL: All values are parameterized, preventing SQL injection</li> <li>Query timeouts: Default 30-second timeout on all database queries</li> </ol>"},{"location":"metrics-inventory/","title":"Metrics Inventory","text":"<p>This inventory tracks the implementation status of all metrics defined in the <code>dev-health-ops</code> project. Work item metrics assume provider data has been synced via <code>python cli.py sync work-items ...</code> (use <code>-s</code> to filter repos; <code>--auth</code> to override GitHub/GitLab tokens when needed; tags/settings filtering planned).</p>"},{"location":"metrics-inventory/#1-delivery-velocity-flow-dora","title":"1. Delivery &amp; Velocity (Flow &amp; DORA)","text":"Metric Status Source/Implementation Cycle Time (PR) [x] metrics/compute.py Coding Time [x] metrics/compute.py Review Time [x] metrics/compute.py Pickup Time [x] metrics/compute.py Deploy Time [x] GitHub/GitLab: Synced via <code>sync deployments --provider github|gitlab</code>. Computed in compute_deployments.py Deployment Frequency [x] Derived from deployments table + fallback to <code>prs_merged</code> Lead Time for Changes [x] DORA metric in compute.py MTTR [x] Calculated from Bug work items + Incident records Change Failure Rate [x] Calculated from \"revert\" PRs in compute.py Work Item Cycle Time [x] metrics/compute_work_items.py Work Item Lead Time [x] metrics/compute_work_items.py WIP Count/Age [x] metrics/compute_work_items.py Flow Efficiency [x] metrics/compute_work_items.py CI/CD Pipeline Metrics [x] GitHub/GitLab: Synced via <code>sync cicd --provider github|gitlab</code>. Computed in compute_cicd.py Incident MTTR [x] GitHub/GitLab: Issues labeled 'incident'. Computed in compute_incidents.py"},{"location":"metrics-inventory/#2-code-quality-risk","title":"2. Code Quality &amp; Risk","text":"Metric Status Source/Implementation Churn [x] Total LOC touched in compute.py Rework Rate (30-day) [x] Computed in quality.py and wired in job_daily.py File Hotspot Score [x] metrics/hotspots.py combining churn and contributor count PR Size [x] <code>avg_commit_size_loc</code> and <code>large_pr_ratio</code> in compute.py PR Rework Ratio [x] <code>pr_rework_ratio</code> in compute.py (based on changes requested) Defect Introduction Rate [x] metrics/compute_work_items.py (bugs created vs items closed) Single Owner File Ratio [x] Computed in quality.py and wired in job_daily.py Cyclomatic Complexity [x] Computed via <code>metrics complexity</code> using <code>radon</code>. Stored in <code>file_complexity_snapshots</code> (per-file) and <code>repo_complexity_daily</code>; loaded in <code>metrics daily</code> via <code>store.get_complexity_snapshots()</code>."},{"location":"metrics-inventory/#3-investment-portfolio","title":"3. Investment &amp; Portfolio","text":"Metric Status Source/Implementation Investment Allocation [x] Classified via <code>InvestmentClassifier</code> in <code>analytics/investment.py</code>. Rules in <code>config/investment_areas.yaml</code>. Project Stream Churn [x] LOC churn aggregated by project stream and investment area. Strategic vs KTLO % [x] Derived from Investment Allocation daily rollups."},{"location":"metrics-inventory/#4-ic-metrics-landscape-new","title":"4. IC Metrics &amp; Landscape (New)","text":"Metric Status Source/Implementation IC Throughput [x] <code>delivery_units</code> (PRs merged + Items completed) in metrics/compute_ic.py. Relies on <code>identity_mapping.yaml</code>. IC Churn [x] <code>loc_touched</code> (Additions + Deletions) in metrics/compute_ic.py IC Cycle Time [x] <code>cycle_p50_hours</code> (PR median cycle time) in metrics/compute_ic.py IC Active WIP [x] <code>work_items_active</code> (Items in progress/review/blocked) in metrics/compute_ic.py Landscape: Churn vs Throughput [x] 2D map with team percentile normalization in metrics/compute_ic.py Landscape: Cycle vs Throughput [x] 2D map with team percentile normalization in metrics/compute_ic.py Landscape: WIP vs Throughput [x] 2D map with team percentile normalization in metrics/compute_ic.py"},{"location":"metrics-inventory/#4-collaboration-team-dynamics","title":"4. Collaboration &amp; Team Dynamics","text":"Metric Status Source/Implementation Review Responsiveness [x] <code>pr_pickup_time_p50_hours</code> in compute.py Review Load [x] <code>reviews_given</code> in compute.py Review Reciprocity [x] Ratio of reviews given vs received in compute.py Bus Factor [x] metrics/knowledge.py (Truck Factor: min authors for 50% churn) Knowledge Distribution [x] metrics/knowledge.py (Gini Coefficient of churn ownership)"},{"location":"metrics-inventory/#5-developer-well-being-cognitive-load","title":"5. Developer Well-being &amp; Cognitive Load","text":"Metric Status Source/Implementation Late-night Activity [x] metrics/compute_wellbeing.py Weekend Activity [x] metrics/compute_wellbeing.py Burnout Risk Score [x] Combined after-hours/weekend activity per developer Flow Score [ ] Requires IDE telemetry Cognitive Load [ ] Requires IDE telemetry"},{"location":"metrics-inventory/#6-systemic-process-health","title":"6. Systemic &amp; Process Health","text":"Metric Status Source/Implementation Bottleneck Index [x] metrics/compute_work_item_state_durations.py WIP Congestion [x] metrics/compute_work_items.py (WIP / Weekly Throughput) Predictability Index [x] metrics/compute_work_items.py (Completion Rate: Completed / (Completed + WIP)) Pipeline Success Rate [x] metrics/compute_cicd.py (GitHub only, GitLab pending) Deploy Success Rate [x] metrics/compute_deployments.py"},{"location":"metrics/","title":"Developer Health Metrics (v2)","text":"<p>This repository computes daily \u201cdeveloper health\u201d metrics from:</p> <ul> <li>synced Git facts (<code>git_commits</code>, <code>git_commit_stats</code>, <code>git_pull_requests</code>)</li> <li>optional work tracking data (Jira issues, GitHub issues/Projects v2, GitLab issues)</li> </ul> <p>Derived time series are written to ClickHouse (preferred) and/or MongoDB (optional) for Grafana.</p> <p>All timestamps are treated as UTC (ClickHouse stores DateTime in UTC; Mongo stores naive datetimes as UTC by convention).</p> <p>Important: Jira (and other issue trackers) is not a replacement for pull request data. Work items are used for planning/throughput/WIP metrics, while PR metrics (cycle time, merge frequency, review metrics when available) come from the Git provider sync.</p>"},{"location":"metrics/#source-data","title":"Source Data","text":"<p>Git facts must already exist in the backend you point the job at:</p> <ul> <li><code>git_commits</code></li> <li><code>git_commit_stats</code></li> <li><code>git_pull_requests</code></li> </ul> <p>Work tracking facts should be synced from provider APIs via <code>python cli.py sync work-items ...</code>. See <code>docs/task_trackers.md</code> for configuration. (<code>metrics daily --provider ...</code> still exists as a convenience/backward-compatible path.) <code>--provider auto</code> (default) loads work items from the database only and skips provider API calls.</p> <p>CI/CD pipeline facts are synced from GitHub/GitLab via the <code>sync cicd</code> job:</p> <pre><code>python cli.py sync cicd --provider github --db \"&lt;DB_CONN&gt;\" --auth \"$GITHUB_TOKEN\" --owner \"&lt;org&gt;\" --repo \"&lt;repo&gt;\"\npython cli.py sync cicd --provider gitlab --db \"&lt;DB_CONN&gt;\" --auth \"$GITLAB_TOKEN\" --gitlab-url \"&lt;URL&gt;\" --project-id &lt;ID&gt;\n</code></pre> <p>Deployments and incident facts are synced via their own jobs:</p> <pre><code>python cli.py sync deployments --provider github --db \"&lt;DB_CONN&gt;\" --auth \"$GITHUB_TOKEN\" --owner \"&lt;org&gt;\" --repo \"&lt;repo&gt;\"\npython cli.py sync incidents --provider github --db \"&lt;DB_CONN&gt;\" --auth \"$GITHUB_TOKEN\" --owner \"&lt;org&gt;\" --repo \"&lt;repo&gt;\"\npython cli.py sync deployments --provider gitlab --db \"&lt;DB_CONN&gt;\" --auth \"$GITLAB_TOKEN\" --gitlab-url \"&lt;URL&gt;\" --project-id &lt;ID&gt;\npython cli.py sync incidents --provider gitlab --db \"&lt;DB_CONN&gt;\" --auth \"$GITLAB_TOKEN\" --gitlab-url \"&lt;URL&gt;\" --project-id &lt;ID&gt;\n</code></pre> <p>Note: Jira Ops/Service Desk incidents are planned once project-to-repo or deployment mapping is defined.</p>"},{"location":"metrics/#derived-tables-collections","title":"Derived Tables / Collections","text":""},{"location":"metrics/#git-repo-user","title":"Git / Repo / User","text":"<ul> <li><code>repo_metrics_daily</code></li> <li><code>user_metrics_daily</code></li> <li><code>commit_metrics</code></li> </ul>"},{"location":"metrics/#work-tracking","title":"Work Tracking","text":"<ul> <li><code>work_item_metrics_daily</code> (daily aggregates, by provider/team/repo)</li> <li><code>work_item_user_metrics_daily</code> (daily aggregates, by provider/user/team)</li> <li><code>work_item_cycle_times</code> (per-work-item fact rows for completed items)</li> </ul>"},{"location":"metrics/#team-well-being-team-level-only","title":"Team Well-being (team-level only)","text":"<ul> <li><code>team_metrics_daily</code></li> </ul>"},{"location":"metrics/#metric-definitions","title":"Metric Definitions","text":""},{"location":"metrics/#commit-size-bucketing","title":"Commit size bucketing","text":"<ul> <li><code>total_loc = additions + deletions</code> (summed from <code>git_commit_stats</code>)</li> <li>bucket:</li> <li><code>small</code>: <code>total_loc &lt;= 50</code></li> <li><code>medium</code>: <code>51..300</code></li> <li><code>large</code>: <code>&gt; 300</code></li> </ul>"},{"location":"metrics/#pr-cycle-time","title":"PR cycle time","text":"<p>For PRs with <code>merged_at</code> on day D:</p> <ul> <li><code>cycle_time_hours = (merged_at - created_at) / 3600</code></li> <li>Distribution fields (per repo/day and user/day):</li> <li><code>median_pr_cycle_hours</code> (p50)</li> <li><code>pr_cycle_p75_hours</code></li> <li><code>pr_cycle_p90_hours</code></li> </ul>"},{"location":"metrics/#large-change-thresholds","title":"Large change thresholds","text":"<ul> <li>Large commits: <code>total_loc &gt; 300</code></li> <li>Large PRs: <code>additions + deletions &gt;= LARGE_PR_LOC_THRESHOLD</code> (default: <code>1000</code>)</li> <li>Note: PR size facts are best-effort and may be missing depending on the connector.</li> </ul>"},{"location":"metrics/#daily-user-metrics-user_metrics_daily","title":"Daily user metrics (<code>user_metrics_daily</code>)","text":"<p>Keyed by <code>(repo_id, author_email, day)</code> where <code>author_email</code> falls back to <code>author_name</code> when email is missing.</p> <ul> <li>Commits: counts, LOC added/deleted, distinct files changed (union across the day), large commits, avg commit size</li> <li>PRs: authored (created that day), merged (merged that day), avg/p50/p75/p90 PR cycle hours (for PRs merged that day)</li> <li>Collaboration (nullable when not available):</li> <li><code>pr_pickup_time_p50_hours</code>: PR created \u2192 first interaction (comment/review)</li> <li><code>pr_first_review_p50_hours</code> / <code>p90</code>: PR created \u2192 first review</li> <li><code>pr_review_time_p50_hours</code>: first review \u2192 merge</li> <li><code>reviews_given</code>, <code>changes_requested_given</code>: counts of review submissions for the day</li> </ul> <p>Null behavior:</p> <ul> <li>if review/comment facts are unavailable, pickup/first-review/review-time fields remain <code>NULL</code></li> </ul>"},{"location":"metrics/#daily-repo-metrics-repo_metrics_daily","title":"Daily repo metrics (<code>repo_metrics_daily</code>)","text":"<p>Keyed by <code>(repo_id, day)</code>.</p> <ul> <li>Commits: count, LOC touched, avg size, large commit ratio</li> <li>PRs: merged count, p50/p75/p90 PR cycle hours</li> <li>Quality (best-effort): <code>large_pr_ratio</code>, <code>pr_rework_ratio</code> (requires PR size and review facts)</li> <li>Knowledge:</li> <li><code>bus_factor</code>: smallest number of developers accounting for \u2265 50% of code churn</li> <li><code>code_ownership_gini</code>: Gini coefficient of code contribution inequality (0..1)</li> </ul>"},{"location":"metrics/#optional-per-commit-metrics-commit_metrics","title":"Optional per-commit metrics (<code>commit_metrics</code>)","text":"<p>Keyed by <code>(repo_id, day, author_email, commit_hash)</code>.</p>"},{"location":"metrics/#work-item-normalization-cycle-times","title":"Work item normalization + cycle times","text":"<p>Work items are normalized into a unified <code>WorkItem</code> abstraction (<code>models/work_items.py</code>).</p> <p>Best-effort fields:</p> <ul> <li><code>started_at</code>: first transition into normalized <code>in_progress</code></li> <li><code>completed_at</code>: first transition into normalized <code>done</code> or <code>canceled</code></li> </ul> <p>Time metrics:</p> <ul> <li><code>lead_time_hours = completed_at - created_at</code> (when completed)</li> <li><code>cycle_time_hours = completed_at - started_at</code> (when started + completed)</li> </ul> <p>Null behavior:</p> <ul> <li>items missing <code>started_at</code> are excluded from cycle-time distributions</li> </ul>"},{"location":"metrics/#work-tracking-daily-metrics-work_item_metrics_daily","title":"Work tracking daily metrics (<code>work_item_metrics_daily</code>)","text":"<p>Keyed by <code>(day, provider, team_id, work_scope_id)</code>.</p> <p><code>work_scope_id</code> is provider-native and is used to avoid assuming a repo UUID exists for work items:</p> <ul> <li>Jira: Jira project key (e.g. <code>ABC</code>)</li> <li>GitHub: repository full name (e.g. <code>owner/repo</code>) when sourced from issues; Projects v2 uses <code>ghprojv2:&lt;org&gt;#&lt;number&gt;</code></li> <li>GitLab: project full path (e.g. <code>group/project</code>)</li> <li>Throughput: <code>items_started</code>, <code>items_completed</code></li> <li>Ownership gap: <code>items_completed_unassigned</code> (subset of completed items that had no assignee at completion time)</li> <li>WIP: <code>wip_count_end_of_day</code></li> <li>Cycle/lead time distributions (nullable if no samples): p50/p90</li> <li>WIP age distributions (nullable if no WIP samples): p50/p90</li> <li><code>bug_completed_ratio</code>: completed bugs / total completed</li> <li><code>story_points_completed</code>: sum of story points completed (Jira only, when configured)</li> <li><code>predictability_score</code>: Completion Rate = <code>items_completed / (items_completed + wip_count_end_of_day)</code></li> </ul>"},{"location":"metrics/#work-item-facts-work_item_cycle_times","title":"Work item facts (<code>work_item_cycle_times</code>)","text":"<p>Keyed by <code>(provider, work_item_id)</code>; stored as ClickHouse <code>ReplacingMergeTree</code> by <code>computed_at</code> and as Mongo upserts.</p>"},{"location":"metrics/#work-item-time-in-state-work_item_state_durations_daily","title":"Work item time-in-state (<code>work_item_state_durations_daily</code>)","text":"<p>Keyed by <code>(day, provider, work_scope_id, team_id, status)</code> and computed from provider status transitions (Jira changelog when available).</p> <p>Null behavior:</p> <ul> <li>items without status transition history contribute no rows</li> </ul>"},{"location":"metrics/#unassigned-work","title":"Unassigned work","text":"<p>If a work item has no assignee, daily rollups:</p> <ul> <li>count it under <code>team_id=''</code> (shown as \u201cunassigned\u201d in dashboards)</li> <li>emit a <code>work_item_user_metrics_daily</code> row with <code>user_identity='unassigned'</code> so you can chart \u201cclosed unassigned\u201d work over time</li> </ul>"},{"location":"metrics/#team-well-being-team-level-only_1","title":"Team well-being (team-level only)","text":"<p>Computed from commits (deduplicated by commit hash) and a team mapping.</p> <ul> <li><code>after_hours_commit_ratio</code>: commits outside business hours on weekdays</li> <li><code>weekend_commit_ratio</code>: commits on weekends</li> </ul> <p>Configuration:</p> <ul> <li><code>BUSINESS_TIMEZONE</code> (default: <code>UTC</code>)</li> <li><code>BUSINESS_HOURS_START</code> (default: <code>9</code>)</li> <li><code>BUSINESS_HOURS_END</code> (default: <code>17</code>)</li> </ul>"},{"location":"metrics/#storage-targets","title":"Storage Targets","text":""},{"location":"metrics/#clickhouse-tables","title":"ClickHouse (tables)","text":"<p>Tables are created automatically if missing:</p> <ul> <li><code>repo_metrics_daily</code></li> <li><code>user_metrics_daily</code></li> <li><code>commit_metrics</code></li> <li><code>team_metrics_daily</code></li> <li><code>work_item_metrics_daily</code></li> <li><code>work_item_user_metrics_daily</code></li> <li><code>work_item_cycle_times</code></li> </ul> <p>They are <code>MergeTree</code> tables partitioned by <code>toYYYYMM(day)</code> and ordered by the natural keys for Grafana queries.</p>"},{"location":"metrics/#clickhouse-query-notes","title":"ClickHouse query notes","text":"<p>These derived tables are append-only with a <code>computed_at</code> version. To query the latest value per key, use <code>argMax(&lt;metric&gt;, computed_at)</code> grouped by the key columns. If you need a daily total, do it in two steps (no nested aggregates):</p> <pre><code>SELECT\n  day,\n  sum(items_completed_unassigned_latest) AS items_completed_unassigned\nFROM\n(\n  SELECT\n    day,\n    provider,\n    work_scope_id,\n    team_id,\n    argMax(items_completed_unassigned, computed_at) AS items_completed_unassigned_latest\n  FROM work_item_metrics_daily\n  WHERE provider = 'jira'\n  GROUP BY day, provider, work_scope_id, team_id\n)\nGROUP BY day\nORDER BY day;\n</code></pre> <p>Re-computations are append-only and distinguished by <code>computed_at</code>. To query the latest metrics for a key/day, use <code>argMax(..., computed_at)</code> in ClickHouse.</p>"},{"location":"metrics/#mongodb-collections","title":"MongoDB (collections)","text":"<p>Collections are created automatically:</p> <ul> <li><code>repo_metrics_daily</code></li> <li><code>user_metrics_daily</code></li> <li><code>commit_metrics</code></li> <li><code>team_metrics_daily</code></li> <li><code>work_item_metrics_daily</code></li> <li><code>work_item_user_metrics_daily</code></li> <li><code>work_item_cycle_times</code></li> </ul> <p>Documents use stable compound <code>_id</code> keys and are written via upserts, so recomputation is safe.</p>"},{"location":"metrics/#sqlite-tables","title":"SQLite (tables)","text":"<p>Tables are created automatically in the same <code>.db</code> file:</p> <ul> <li><code>repo_metrics_daily</code></li> <li><code>user_metrics_daily</code></li> <li><code>commit_metrics</code></li> <li><code>team_metrics_daily</code></li> <li><code>work_item_metrics_daily</code></li> <li><code>work_item_user_metrics_daily</code></li> <li><code>work_item_cycle_times</code></li> </ul>"},{"location":"metrics/#running-the-daily-job","title":"Running The Daily Job","text":"<p>The job reads source data from the same backend you point it at (ClickHouse or MongoDB), using the synced tables/collections:</p> <ul> <li><code>git_commits</code></li> <li><code>git_commit_stats</code></li> <li><code>git_pull_requests</code>   It also supports SQLite, reading the same tables and writing metrics tables into the same <code>.db</code> file.</li> </ul>"},{"location":"metrics/#environment-variables","title":"Environment variables","text":"<ul> <li><code>DATABASE_URI</code> (or <code>DATABASE_URL</code>): ClickHouse, MongoDB, SQLite, or PostgreSQL URI for both reading source data and writing derived metrics.</li> <li><code>SECONDARY_DATABASE_URI</code>: Required when running with <code>--sink both</code> to write to a secondary backend.</li> </ul>"},{"location":"metrics/#examples","title":"Examples","text":"<ul> <li>Compute one day (backend inferred from <code>--db</code> or <code>DATABASE_URI</code>):</li> <li><code>python cli.py metrics daily --date 2025-02-01 --db clickhouse://localhost:8123/default</code></li> <li><code>python cli.py metrics daily --date 2025-02-01 --db mongodb://localhost:27017/mergestat</code></li> <li><code>python cli.py metrics daily --date 2025-02-01 --db sqlite:///./mergestat.db</code></li> <li>Compute 7-day backfill ending at a date:</li> <li><code>python cli.py metrics daily --date 2025-02-01 --backfill 7 --db clickhouse://localhost:8123/default</code></li> <li>Filter to one repository:</li> <li><code>python cli.py metrics daily --date 2025-02-01 --repo-id &lt;uuid&gt; --db clickhouse://localhost:8123/default</code></li> <li>Compute git + work item metrics (requires provider credentials; see <code>docs/task_trackers.md</code>):</li> <li><code>python cli.py sync work-items --provider all --date 2025-02-01 --backfill 30 --db clickhouse://localhost:8123/default</code></li> <li><code>python cli.py metrics daily --date 2025-02-01 --backfill 30 --db clickhouse://localhost:8123/default</code></li> </ul>"},{"location":"metrics/#dependencies","title":"Dependencies","text":"<ul> <li>ClickHouse uses <code>clickhouse-connect</code> (already in <code>requirements.txt</code>).</li> <li>MongoDB uses <code>pymongo</code> (available via the <code>motor</code> dependency in <code>requirements.txt</code>).</li> <li>SQLite uses <code>sqlalchemy</code> (already in <code>requirements.txt</code>).</li> </ul>"},{"location":"metrics/#ic-metrics-landscape-v3","title":"IC Metrics &amp; Landscape (v3)","text":"<p>We compute canonical Individual Contributor (IC) metrics and \"Developer Landscape\" maps to visualize patterns in churn, throughput, and work-in-progress.</p> <p>Note: These metrics are designed for identifying signals and patterns, not for ranking individuals.</p>"},{"location":"metrics/#identity-resolution","title":"Identity Resolution","text":"<ul> <li>Users are mapped to a canonical <code>identity_id</code> (preferring email) across providers.</li> <li>Configuration: <code>config/teams.yaml</code> or <code>config/team_mapping.yaml</code>.</li> </ul>"},{"location":"metrics/#ic-metrics-user_metrics_daily","title":"IC Metrics (<code>user_metrics_daily</code>)","text":"<p>Extends the daily user metrics with work tracking and unified throughput signals:</p> <ul> <li><code>identity_id</code>: Canonical user identity.</li> <li><code>loc_touched</code>: Sum of additions + deletions.</li> <li><code>delivery_units</code>: <code>prs_merged</code> + <code>work_items_completed</code>.</li> <li><code>work_items_active</code>: Number of items in progress/review/blocked at end of day.</li> <li><code>cycle_p50_hours</code>: Median cycle time (PRs).</li> </ul>"},{"location":"metrics/#landscape-maps-ic_landscape_rolling_30d","title":"Landscape Maps (<code>ic_landscape_rolling_30d</code>)","text":"<p>Rolling 30-day metrics normalized by team percentiles (0..1). Stored in ClickHouse table <code>ic_landscape_rolling_30d</code>.</p>"},{"location":"metrics/#map-1-churn-vs-throughput","title":"Map 1: Churn vs Throughput","text":"<ul> <li>X: <code>log(churn_loc_30d)</code> (rolling 30d sum of LOC touched)</li> <li>Y: <code>delivery_units_30d</code> (rolling 30d sum of delivery units)</li> </ul>"},{"location":"metrics/#map-2-cycle-time-vs-throughput","title":"Map 2: Cycle Time vs Throughput","text":"<ul> <li>X: <code>log(cycle_p50_30d_hours)</code> (median of daily median PR cycle times over 30d)</li> <li>Y: <code>delivery_units_30d</code></li> </ul>"},{"location":"metrics/#map-3-wip-vs-throughput","title":"Map 3: WIP vs Throughput","text":"<ul> <li>X: <code>wip_max_30d</code> (max active work items over 30d)</li> <li>Y: <code>delivery_units_30d</code></li> </ul> <p>Each coordinate (<code>x_raw</code>, <code>y_raw</code>) is normalized per team into (<code>x_norm</code>, <code>y_norm</code>) representing the percentile rank within the team.</p>"},{"location":"metrics/#code-complexity-repo_complexity_daily","title":"Code Complexity (<code>repo_complexity_daily</code>)","text":"<p>Complexity metrics are computed by scanning local git clones at specific historical references using <code>radon</code>.</p> <ul> <li>Cyclomatic Complexity (CC): Measures the number of linearly independent paths through a program's source code.</li> <li>Metric fields:</li> <li><code>cyclomatic_total</code>: Sum of CC for all functions/classes in the repo.</li> <li><code>cyclomatic_avg</code>: Mean CC per function.</li> <li><code>high_complexity_functions</code>: Count of functions with CC &gt; 15 (configurable).</li> <li><code>very_high_complexity_functions</code>: Count of functions with CC &gt; 25.</li> <li>Backfilling: Uses <code>git checkout</code> (via <code>GitPython</code>) to analyze historical state.</li> </ul>"},{"location":"metrics/#investment-metrics-investment_metrics_daily","title":"Investment Metrics (<code>investment_metrics_daily</code>)","text":"<p>Categorizes engineering effort into investment areas (e.g., \"New Value\", \"Security\", \"Infrastructure\") using a rule-based classifier.</p> <ul> <li>Artifacts Classified:</li> <li>Work Items: Based on labels, components, and title keywords.</li> <li>Commits (Churn): Based on file path patterns (e.g., <code>infra/</code> or <code>tests/</code>).</li> <li>Metric fields:</li> <li><code>investment_area</code>: The assigned category.</li> <li><code>project_stream</code>: A secondary grouping (e.g., \"Project Phoenix\").</li> <li><code>delivery_units</code>: Story points or count of work items completed.</li> <li><code>churn_loc</code>: Sum of additions + deletions associated with the area.</li> <li>Configuration: <code>config/investment_areas.yaml</code> defines the matching rules and priorities.</li> </ul>"},{"location":"metrics/#views-and-interpretations","title":"Views and interpretations","text":"<p>Metrics are intended to be consumed through views that constrain interpretation and provide drill-down.</p> <ul> <li>Views index: <code>user-guide/views-index.md</code></li> <li>Investment View: <code>user-guide/investment-view.md</code></li> <li>Work Graph: <code>user-guide/work-graph.md</code></li> <li>Computations index: <code>computations/index.md</code></li> </ul>"},{"location":"project/","title":"Part 1 \u2014 Existing Platforms: What They Measure","text":""},{"location":"project/#system-flow-connect-sync-calculate","title":"System Flow (Connect \u2192 Sync \u2192 Calculate)","text":"<ul> <li>connect: Github, GitLab, IDEs, CI/CD, SCM, Databases</li> <li>sync: code events, PRs, reviews, issues, deployments</li> <li>calculate: metrics, scores, trends, risk indicators, health signals</li> </ul>"},{"location":"project/#linearb-flow-team-efficiency","title":"LinearB \u2014 Flow &amp; Team Efficiency","text":"<p>Focus: flow, predictability, PR health, resource allocation.</p> <p>Metrics</p> <ul> <li> <p>Cycle Time</p> </li> <li> <p>Coding Time</p> </li> <li>Pickup Time</li> <li>Review Time</li> <li> <p>Deploy Time</p> </li> <li> <p>Deployment Frequency</p> </li> <li>Lead Time for Changes</li> <li> <p>Work Breakdown</p> </li> <li> <p>New work vs rework vs unplanned vs refactor</p> </li> <li> <p>Investment Profile</p> </li> <li> <p>Strategic work %</p> </li> <li>Maintenance %</li> <li> <p>Unplanned work %</p> </li> <li> <p>PR Process Health</p> </li> <li> <p>PR size</p> </li> <li>Review depth</li> <li>Review response time</li> <li> <p>Time-to-approve</p> </li> <li> <p>Bottlenecks</p> </li> <li> <p>Review congestion</p> </li> <li> <p>Idle WIP</p> </li> <li> <p>Review Silos / Single-reviewer dependencies</p> </li> </ul>"},{"location":"project/#gitprime-pluralsight-flow-engineering-behavior","title":"GitPrime / Pluralsight Flow \u2014 Engineering Behavior","text":"<p>Focus: individual contribution patterns and risk.</p> <p>Metrics</p> <ul> <li>Impact Score (weighted contribution proxy)</li> <li>Efficiency (merged vs reworked code)</li> <li> <p>Code Fundamentals</p> </li> <li> <p>Active days</p> </li> <li>Commits/day</li> <li>PRs opened/merged</li> <li>Lines changed</li> <li>Rework rate</li> <li> <p>Churn %</p> </li> <li> <p>Collaboration</p> </li> <li> <p>Review load</p> </li> <li> <p>Review turnaround</p> </li> <li> <p>Risk Indicators</p> </li> <li> <p>High-churn files</p> </li> <li> <p>Ownership hotspots</p> </li> <li> <p>Health Signals</p> </li> <li> <p>Sustained intensity</p> </li> <li>Weekend / off-hours work</li> <li>Irregular contribution patterns</li> </ul>"},{"location":"project/#gitlab-analytics-value-stream-devops","title":"GitLab Analytics \u2014 Value Stream &amp; DevOps","text":"<p>Focus: delivery pipeline and DORA.</p> <p>Metrics</p> <ul> <li>Lead Time for Changes</li> <li> <p>Value Stream Stages</p> </li> <li> <p>Issue \u2192 Code \u2192 Review \u2192 Merge \u2192 Deploy</p> </li> <li> <p>Merge Request Analytics</p> </li> <li> <p>MR size</p> </li> <li>Approval count</li> <li>Discussion count</li> <li> <p>Time to merge</p> </li> <li> <p>CI/CD</p> </li> <li> <p>Pipeline duration</p> </li> <li>Success rate</li> <li>MTTR</li> <li> <p>Change failure rate</p> </li> <li> <p>Contribution Analytics</p> </li> <li>Security / Vulnerability introduction rate</li> </ul>"},{"location":"project/#typeappapp-cognitive-load-well-being","title":"typeapp.app \u2014 Cognitive Load &amp; Well-being","text":"<p>Focus: IDE-level behavior and burnout risk.</p> <p>Metrics</p> <ul> <li>Flow State Duration</li> <li> <p>Context Switching</p> </li> <li> <p>File switches</p> </li> <li>Tab switches</li> <li> <p>Project switches</p> </li> <li> <p>Typing Behavior</p> </li> <li> <p>Error rate</p> </li> <li> <p>Undo/redo density</p> </li> <li> <p>Distraction Index</p> </li> <li> <p>Wellness Signals</p> </li> <li> <p>Late-night streaks</p> </li> <li> <p>Burst\u2013burnout cycles</p> </li> <li> <p>Cognitive Load Index</p> </li> </ul>"},{"location":"project/#part-2-unified-metrics-framework","title":"Part 2 \u2014 Unified Metrics Framework","text":""},{"location":"project/#1-delivery-velocity","title":"1. Delivery &amp; Velocity","text":"<ul> <li>Coding Time</li> <li>Review Time</li> <li>Rework Time</li> <li>Deploy Time</li> <li>Deployment Frequency</li> <li>Throughput (PRs, issues, story points)</li> <li>Work Composition</li> <li>Investment Profile (Strategic vs Maintenance)</li> </ul>"},{"location":"project/#2-code-quality-risk","title":"2. Code Quality &amp; Risk","text":"<ul> <li>Code Risk Index</li> <li>Churn</li> <li>Ownership concentration</li> <li>Hotspots</li> <li>Cyclomatic Complexity (radon; snapshots persisted and loaded via <code>store.get_complexity_snapshots()</code>)</li> <li>Rework Rate</li> <li> <p>PR Quality</p> </li> <li> <p>Size distribution</p> </li> <li>Comment density</li> <li> <p>Rejection rate</p> </li> <li> <p>Stability</p> </li> <li> <p>Bugs introduced</p> </li> <li>Defect escape rate</li> </ul>"},{"location":"project/#3-collaboration-team-dynamics","title":"3. Collaboration &amp; Team Dynamics","text":"<ul> <li>Review Responsiveness</li> <li>Reviewer Load</li> <li>Review Network Graph</li> <li> <p>Knowledge Distribution</p> </li> <li> <p>Bus factor</p> </li> <li> <p>Contribution entropy</p> </li> <li> <p>Communication Density</p> </li> </ul>"},{"location":"project/#4-developer-health-cognitive-load","title":"4. Developer Health &amp; Cognitive Load","text":"<ul> <li>Context Switching Score</li> <li>Focus Time Index</li> <li> <p>Work Pattern Health</p> </li> <li> <p>Late-night activity</p> </li> <li> <p>Weekend work</p> </li> <li> <p>Editing &amp; Typing Behavior</p> </li> </ul>"},{"location":"project/#5-systemic-process-health","title":"5. Systemic &amp; Process Health","text":"<ul> <li>Bottleneck Index</li> <li>WIP Congestion</li> <li>Stale Work Detection</li> <li>Predictability Index</li> </ul>"},{"location":"project/#part-3-developer-health-model","title":"Part 3 \u2014 Developer Health Model","text":"<p>Four Dimensions</p> <ol> <li>Delivery \u2014 speed, throughput, predictability</li> <li>Durability \u2014 quality, stability, long-term risk</li> <li>Developer Well-being \u2014 flow, burnout risk, load</li> <li>Dynamics \u2014 collaboration, knowledge spread</li> </ol> <p>This framework fully covers and extends LinearB, GitPrime, GitLab, and typeapp.app with a single coherent analytics model.</p> <p>Absolutely \u2014 I\u2019ll continue without pause and expand the entire system:</p> <ul> <li>Data model</li> <li>Events and ingestion</li> <li>Scoring &amp; formulas</li> <li>Detailed metric definitions</li> <li>Architecture</li> <li>Dashboards &amp; views</li> <li>Roadmap</li> </ul> <p>This will give you a complete draft developer health platform capable of rivaling LinearB, GitPrime, TypeApp, and GitLab Analytics in one unified product.</p>"},{"location":"project/#part-4-detailed-data-model","title":"\ud83d\udea7 PART 4 \u2014 Detailed Data Model","text":"<ol> <li>Git Events (commits, branches, PRs, reviews)</li> <li>Issue/Task Events (create, update, close, transitions)</li> <li>CI/CD Events (pipelines, deploys, failures, restores)</li> <li>IDE Telemetry Events (typing, context switching, focus time)</li> </ol>"},{"location":"project/#part-5-metric-definitions-formulas","title":"\ud83e\uddee PART 5 \u2014 Metric Definitions &amp; Formulas","text":"<p>Everything the earlier platforms measure + extended metrics.</p>"},{"location":"project/#1-delivery-metrics-flow-velocity","title":"1\ufe0f\u20e3 DELIVERY METRICS (Flow &amp; Velocity)","text":""},{"location":"project/#cycle-time","title":"Cycle Time","text":"<pre><code>cycle_time = merged_at - first_commit_timestamp_in_branch\n</code></pre>"},{"location":"project/#coding-time","title":"Coding Time","text":"<pre><code>coding_time = pr_created_at - first_commit_timestamp\n</code></pre>"},{"location":"project/#review-time","title":"Review Time","text":"<pre><code>review_time = first_review_timestamp - pr_created_at\n</code></pre>"},{"location":"project/#rework-time","title":"Rework Time","text":"<pre><code>rework_time = merged_at - first_approval_timestamp\n</code></pre>"},{"location":"project/#deploy-time","title":"Deploy Time","text":"<pre><code>deploy_time = first_prod_deploy_timestamp - merged_at\n</code></pre>"},{"location":"project/#throughput","title":"Throughput","text":"<pre><code>features_completed = count(tasks.type==\"feature\")\nbugs_fixed = count(tasks.type==\"bug\")\nprs_merged = count(prs where state=\"merged\")\n</code></pre>"},{"location":"project/#work-mix","title":"Work Mix","text":"<pre><code>rework_percent = churn_loc_last_30_days / total_loc_last_30_days\nnew_work_percent = new_features_loc / total_loc\nbugfix_percent = bugfix_loc / total_loc\nrefactor_percent = refactor_loc / total_loc\n</code></pre>"},{"location":"project/#dora-metrics","title":"DORA Metrics","text":"<ul> <li>Deployment frequency = deploy_count / time_period</li> <li>Lead time for changes = average(coding_time + review_time + deploy_time)</li> <li>MTTR = mean(time_from_incident_to_restore)</li> <li>Change failure rate = failed_deploys / total_deploys</li> </ul>"},{"location":"project/#2-code-quality-risk-metrics","title":"2\ufe0f\u20e3 CODE QUALITY &amp; RISK METRICS","text":""},{"location":"project/#churn","title":"Churn","text":"<pre><code>commit_churn = (loc_added + loc_deleted)\nrework_rate = churn_in_30_days / total_loc_touched\n</code></pre>"},{"location":"project/#hotspot-risk","title":"Hotspot Risk","text":"<p>A weighted model:</p> <pre><code>hotspot_score = file_churn * number_of_contributors * commit_frequency\n</code></pre>"},{"location":"project/#pr-size","title":"PR Size","text":"<pre><code>pr_size = lines_added + lines_deleted\n</code></pre>"},{"location":"project/#review-quality","title":"Review Quality","text":"<pre><code>comments_per_review = total_review_comments / number_of_reviews\nreview_coverage = files_reviewed / files_changed\n</code></pre>"},{"location":"project/#3-collaboration-team-dynamics_1","title":"3\ufe0f\u20e3 COLLABORATION &amp; TEAM DYNAMICS","text":""},{"location":"project/#review-responsiveness","title":"Review Responsiveness","text":"<pre><code>time_to_first_review = min(review.submitted_at) - pr_created_at\n</code></pre>"},{"location":"project/#collaboration-network","title":"Collaboration Network","text":"<p>Track edges: <code>author \u2192 reviewer</code></p> <p>Metrics:</p> <ul> <li>reciprocity</li> <li>centrality</li> <li>isolated contributors</li> <li>bottleneck reviewers</li> </ul>"},{"location":"project/#knowledge-distribution-bus-factor","title":"Knowledge Distribution (Bus Factor)","text":"<pre><code>ownership_score(file) = commits_by_user / total_commits_on_file\nbus_factor = number_of_files_with_ownership&gt;0.75\n</code></pre>"},{"location":"project/#4-developer-health-cognitive-load_1","title":"4\ufe0f\u20e3 DEVELOPER HEALTH &amp; COGNITIVE LOAD","text":""},{"location":"project/#flow-score-0100","title":"Flow Score (0\u2013100)","text":"<p>Uses editor telemetry:</p> <pre><code>focus_blocks = sequences of 10+ min uninterrupted editing\ncontext_switch_penalty = file_switches + tab_switches\nflow_score = focus_blocks * 10 - context_switch_penalty * 2\n</code></pre>"},{"location":"project/#cognitive-load","title":"Cognitive Load","text":"<pre><code>load_index = (avg_time_between_edits + tab_switch_rate + undo_density)\n</code></pre>"},{"location":"project/#burnout-indicators","title":"Burnout Indicators","text":"<pre><code>late_night_activity = activity between 12am\u20135am\nweekend_activity = sat/sun commits\nburst_cycles = # of commit storms within &lt;2h\nrisk_score = weighted_sum(late_night, weekend, bursts)\n</code></pre>"},{"location":"project/#5-system-health-metrics","title":"5\ufe0f\u20e3 SYSTEM HEALTH METRICS","text":""},{"location":"project/#wip-congestion","title":"WIP Congestion","text":"<pre><code>stale_prs = prs open &gt; X days\nqueued_reviews = prs waiting for review &gt; Y hours\nbuild_queue_length = count(pipelines waiting)\n</code></pre>"},{"location":"project/#predictability","title":"Predictability","text":"<pre><code>estimate_accuracy = |estimate - actual| / estimate\ncycle_time_variance = variance(cycle_time)\n</code></pre>"},{"location":"project/#part-7-dashboard-drafts","title":"\ud83d\udcca PART 7 \u2014 Dashboard Drafts","text":""},{"location":"project/#1-org-dashboard","title":"1. Org Dashboard","text":"<ul> <li>Cycle Time</li> <li>Deployment Frequency</li> <li>Hotspot files</li> <li>Bottleneck teams</li> <li>Investment Profile</li> <li>Predictability Index</li> </ul>"},{"location":"project/#2-team-dashboard","title":"2. Team Dashboard","text":"<ul> <li>PR review health</li> <li>Collaboration graph</li> <li>Throughput</li> <li>Roadmap completion forecast</li> <li>Team burnout risk</li> </ul>"},{"location":"project/#3-developer-dashboard","title":"3. Developer Dashboard","text":"<ul> <li>Flow score</li> <li>Deep work patterns</li> <li>Impact score</li> <li>Review contribution</li> <li>Churn &amp; rework (healthy or high)</li> </ul>"},{"location":"project/#4-repo-dashboard","title":"4. Repo Dashboard","text":"<ul> <li>Hotspots</li> <li>Bus factor</li> <li>Risky files</li> <li>Churn trends</li> <li>Contribution activity</li> </ul>"},{"location":"project/#developer-health-platform-full-draft-specification","title":"Developer Health Platform \u2013 Full Draft Specification","text":""},{"location":"project/#1-metric-formulations-weights","title":"1. Metric formulations &amp; weights","text":""},{"location":"project/#11-normalization-scoring-pattern","title":"1.1 Normalization &amp; scoring pattern","text":"<p>General pattern (so everything fits into 0\u2013100):</p> <ul> <li>Raw metric \u2192 normalize into [0,1] using either</li> <li>min\u2013max based on historical window, or</li> <li>percentile within org/team.</li> <li>Direction: some metrics \u201clower is better\u201d.</li> <li>If lower is better: <code>score = 1 - normalized_value</code></li> <li>If higher is better: <code>score = normalized_value</code></li> <li>Metric Score (0\u2013100): <code>metric_score = score * 100</code></li> </ul> <p>All metrics below follow: raw formula \u2192 normalized score and then roll into dimension scores.</p>"},{"location":"project/#12-delivery-metrics-flow-dora","title":"1.2 Delivery metrics (Flow &amp; DORA)","text":""},{"location":"project/#121-cycle-time","title":"1.2.1 Cycle time","text":"<p>Per PR:</p> <pre><code>cycle_time(pr) = merged_at - first_commit_time_on_branch(pr)\n</code></pre> <p>Normalized (lower = better):</p> <pre><code>ct_norm = clip( (cycle_time - CT_min) / (CT_max - CT_min), 0, 1 )\nct_score = (1 - ct_norm) * 100\n</code></pre>"},{"location":"project/#122-coding-review-deploy-times","title":"1.2.2 Coding / Review / Deploy times","text":"<p>For each PR:</p> <pre><code>coding_time  = pr_created_at - first_commit_time\nreview_time  = time_of_first_review - pr_created_at\nrework_time  = merged_at - time_of_first_approval\ndeploy_time  = first_prod_deploy_at - merged_at\n</code></pre> <p>Flow Balance score (detect one-stage dominance):</p> <pre><code>flow_balance_ratio = max(coding_time, review_time, rework_time, deploy_time) / cycle_time\n# If one stage &gt; 60% of total, that\u2019s a bottleneck\nbalance_norm = clip( (flow_balance_ratio - 0.25) / (0.75 - 0.25), 0, 1 )\nflow_balance_score = (1 - balance_norm) * 100\n</code></pre>"},{"location":"project/#123-deployment-frequency","title":"1.2.3 Deployment frequency","text":"<p>For a team in a period T (e.g., 14 days):</p> <pre><code>deploy_freq = number_of_prod_deploys / T_days\n</code></pre> <p>Normalize vs org history:</p> <pre><code>df_norm = clip( (deploy_freq - DF_min) / (DF_max - DF_min), 0, 1 )\ndeploy_freq_score = df_norm * 100\n</code></pre>"},{"location":"project/#124-dora-metrics","title":"1.2.4 DORA metrics","text":"<ul> <li> <p>Lead time for changes = average cycle_time for prod-bound PRs \u2192 same pattern as cycle time.</p> </li> <li> <p>MTTR (Mean Time To Restore):</p> </li> </ul> <pre><code>MTTR = avg(incident_resolved_at - incident_detected_at)\nmttr_norm, mttr_score = lower-is-better normalization\n</code></pre> <ul> <li>Change failure rate:</li> </ul> <pre><code>change_failure_rate = failed_deploys / total_deploys\ncfr_norm = clip( (change_failure_rate - CFR_min) / (CFR_max - CFR_min), 0, 1 )\ncfr_score = (1 - cfr_norm) * 100\n</code></pre>"},{"location":"project/#125-delivery-dimension-score","title":"1.2.5 Delivery Dimension Score","text":"<p>For a team or org:</p> <ul> <li><code>Cycle Time score</code> (CT)</li> <li><code>Deploy Frequency score</code> (DF)</li> <li><code>Lead Time score</code> (LT)</li> <li><code>MTTR score</code> (MT)</li> <li><code>Change Failure Rate score</code> (CF)</li> </ul> <p>Weights (example):</p> <pre><code>DeliveryScore = 0.30*CT + 0.20*DF + 0.20*LT + 0.15*MT + 0.15*CF\n</code></pre>"},{"location":"project/#13-code-quality-risk-metrics","title":"1.3 Code quality &amp; risk metrics","text":""},{"location":"project/#131-churn-rework","title":"1.3.1 Churn &amp; rework","text":"<p>Per file over window W (e.g., 30 days):</p> <pre><code>loc_touched = \u03a3(|loc_added| + |loc_deleted|)\nloc_reworked_soon = \u03a3(loc_modified_within_30d_of_being_added)\n\nrework_rate = loc_reworked_soon / loc_touched\n</code></pre> <p>Normalize (lower is better):</p> <pre><code>rw_norm = clip( (rework_rate - RW_min) / (RW_max - RW_min), 0, 1 )\nrework_score = (1 - rw_norm) * 100\n</code></pre>"},{"location":"project/#132-file-hotspot-score","title":"1.3.2 File hotspot score","text":"<p>For each file f:</p> <pre><code>churn_f        = loc_touched_in_W\ncontributors_f = number_of_distinct_authors_in_W\ncommit_freq_f  = commits_touching_file_in_W / days_in_W\n\nhotspot_raw = \u03b1*log(1 + churn_f) + \u03b2*contributors_f + \u03b3*commit_freq_f\n# \u03b1,\u03b2,\u03b3 ~ 0.4, 0.3, 0.3 initially\n</code></pre> <p>Normalize across files:</p> <pre><code>hs_norm = (hotspot_raw - HS_min) / (HS_max - HS_min)\nhotspot_score_file = hs_norm * 100\n</code></pre> <p>Team-level Code Risk score can be e.g. 80th percentile of hotspot scores in that team\u2019s modules, inverted:</p> <pre><code>risk_raw = P80(hotspot_score_file_for_team)\nrisk_score = 100 - risk_raw\n</code></pre> <p>Ownership concentration (for hotspot drivers) is derived from git blame data:</p> <pre><code>ownership_concentration = max(lines_by_author) / total_lines\n</code></pre> <p>Synthetic fixtures include an expanded file set to improve blame-driven ownership coverage. Blame-only sync is available via <code>cli.py sync blame --provider &lt;local|github|gitlab&gt;</code>.</p>"},{"location":"project/#133-pr-size","title":"1.3.3 PR Size","text":"<pre><code>pr_size = loc_added + loc_deleted\n</code></pre> <p>Normalize with a sweet spot (e.g. 20\u2013400 LOC):</p> <pre><code>if pr_size &lt; target_min:\n    size_score = 40 + 60*(pr_size / target_min)   # Very tiny PRs not ideal\nelif pr_size &lt;= target_max:\n    size_score = 100\nelse:\n    overflow = min(pr_size - target_max, cap)\n    size_score = max(40, 100 - overflow / scale) # penalize very large ones\n</code></pre>"},{"location":"project/#134-review-depth-coverage","title":"1.3.4 Review depth &amp; coverage","text":"<pre><code>review_comments_per_pr = total_comments_on_pr / 1\nreview_coverage = reviewed_files_count / files_changed\n</code></pre> <p>Combined PR Review Health score:</p> <pre><code>depth_score = sigmoid( a * (review_comments_per_pr - target_comments) )\ncoverage_score = review_coverage * 100\n\nreview_health_score = 0.4*size_score + 0.3*depth_score + 0.3*coverage_score\n</code></pre> <p>Where <code>sigmoid(x) = 100 / (1 + exp(-x))</code> scaled to [0,100].</p>"},{"location":"project/#135-quality-durability-dimension-score","title":"1.3.5 Quality &amp; Durability Dimension Score","text":"<p>Metrics:</p> <ul> <li>Rework score</li> <li>Code risk score</li> <li>Review health score</li> <li>Defect introduction rate score</li> </ul> <p>Weights:</p> <pre><code>DurabilityScore = 0.35*ReworkScore + 0.30*CodeRiskScore\n                  + 0.20*ReviewHealthScore + 0.15*DefectRateScore\n</code></pre>"},{"location":"project/#14-collaboration-team-dynamics-metrics","title":"1.4 Collaboration &amp; team dynamics metrics","text":""},{"location":"project/#141-review-responsiveness","title":"1.4.1 Review responsiveness","text":"<pre><code>time_to_first_review = avg( first_review_timestamp - pr_created_at )\n\n# lower better\nresp_norm = (time_to_first_review - RESP_min) / (RESP_max - RESP_min)\nReviewResponsivenessScore = (1 - resp_norm) * 100\n</code></pre>"},{"location":"project/#142-review-load-reciprocity","title":"1.4.2 Review load &amp; reciprocity","text":"<p>For each user u in window W:</p> <pre><code>reviews_given_u  = count(reviews where reviewer_id = u)\nreviews_received_u = count(prs authored_by_u that had reviews)\nreview_balance_u = reviews_given_u / (reviews_received_u + 1)\n</code></pre> <p>Team reciprocity via dispersion:</p> <pre><code>team_balance = variance(review_balance_u across team)\nReciprocityScore = 100 - normalized_variance(team_balance)\n</code></pre>"},{"location":"project/#143-knowledge-distribution-bus-factor","title":"1.4.3 Knowledge distribution / Bus factor","text":"<ul> <li>Bus Factor (Truck Factor): The smallest number of developers that account for &gt;= 50% of the total code churn in the window.</li> <li>Code Ownership Gini: Gini coefficient of code contribution (churn) distribution. 0.0 = perfect equality, 1.0 = perfect inequality.</li> </ul> <pre><code>bus_factor = number_of_devs_contributing_50_percent_churn\ngini = (2 * sum(i * y_i) / (n * sum(y_i))) - (n + 1) / n\n</code></pre>"},{"location":"project/#144-dynamics-dimension-score","title":"1.4.4 Dynamics Dimension Score","text":"<p>Use:</p> <ul> <li>ReviewResponsivenessScore</li> <li>ReciprocityScore</li> <li>BusFactorScore</li> <li>Cross-team-review score (optional)</li> </ul> <pre><code>DynamicsScore = 0.30*ReviewResponsivenessScore\n              + 0.25*ReciprocityScore\n              + 0.30*BusFactorScore\n              + 0.15*CrossTeamReviewScore\n</code></pre>"},{"location":"project/#15-developer-well-being-cognitive-load-metrics","title":"1.5 Developer well-being &amp; cognitive load metrics","text":""},{"location":"project/#151-flow-score","title":"1.5.1 Flow Score","text":"<p>From editor events:</p> <pre><code>focus_block = a continuous period \u2265 10 min\n              with no context_switch events (tab/file/project) and\n              active edits every \u2264 2 min\n\nnum_blocks   = count(focus_block) in day\navg_block_len = avg(duration of focus_block)\ncontext_switches = count(context_switch events per hour)\ninterruptions = count(non-editor-window-focus events per hour)\n\nflow_raw = w1*num_blocks + w2*avg_block_len - w3*context_switches - w4*interruptions\n</code></pre> <p>Normalize:</p> <pre><code>flow_norm = (flow_raw - FLOW_min) / (FLOW_max - FLOW_min)\nFlowScore = clip(flow_norm, 0, 1) * 100\n</code></pre> <p>Start with <code>w1=2, w2=0.1, w3=1, w4=1</code> as tunables.</p>"},{"location":"project/#152-cognitive-load-index","title":"1.5.2 Cognitive load index","text":"<p>Signals:</p> <ul> <li>avg time between edits in same file</li> <li>undo/redo density</li> <li>error bursts (rapid changes + reverts)</li> </ul> <p>Example:</p> <pre><code>avg_edit_gap = avg(time_between_edits_same_file)\nundo_density = total_undo_ops / total_edits\nswitch_density = tab_switches / hour\n\nload_raw = a1*avg_edit_gap + a2*undo_density + a3*switch_density\nLoadScore = (1 - normalized(load_raw)) * 100\n</code></pre> <p>Higher score = better (manageable load).</p>"},{"location":"project/#153-burnout-risk-score","title":"1.5.3 Burnout risk score","text":"<p>Signals in last 14\u201330 days:</p> <pre><code>late_night_ratio = commits_or_edits_between_00_05 / total_commits_or_edits\nweekend_ratio    = weekend_commits / total_commits\nstreak_length    = longest_consecutive_days_with_activity\nburst_index      = fraction_of_commits_in_top_20% busiest_hours\n</code></pre> <p>Combine:</p> <pre><code>burnout_raw = b1*late_night_ratio + b2*weekend_ratio + b3*streak_length_norm + b4*burst_index\nBurnoutRiskScore = (1 - normalized(burnout_raw)) * 100\n</code></pre> <p>(High score = low risk.)</p>"},{"location":"project/#154-well-being-dimension-score","title":"1.5.4 Well-being Dimension Score","text":"<p>Combine:</p> <pre><code>WellBeingScore = 0.40*FlowScore + 0.25*LoadScore + 0.35*BurnoutRiskScore\n</code></pre>"},{"location":"project/#16-system-process-health-metrics","title":"1.6 System &amp; process health metrics","text":""},{"location":"project/#161-bottleneck-index","title":"1.6.1 Bottleneck index","text":"<p>For each stage s in {coding, review, qa, deploy}:</p> <pre><code>stage_time_s = avg(time_spent_in_stage_s)\nstage_fraction_s = stage_time_s / total_cycle_time\nbottleneck_raw = max(stage_fraction_s)\nBottleneckScore = (1 - normalized(bottleneck_raw)) * 100\n</code></pre>"},{"location":"project/#162-wip-congestion","title":"1.6.2 WIP congestion","text":"<pre><code>stale_prs_ratio = stale_prs / total_open_prs\nqueue_length    = queued_reviews / team_size\nbuild_queue     = queued_builds / historical_median\n\ncongestion_raw = c1*stale_prs_ratio + c2*queue_length + c3*build_queue\nCongestionScore = (1 - normalized(congestion_raw)) * 100\n</code></pre>"},{"location":"project/#163-predictability","title":"1.6.3 Predictability","text":"<p>Defined as Completion Rate (how well the team clears its plate).</p> <pre><code>predictability_score = items_completed / (items_completed + wip_count_end_of_day)\n</code></pre>"},{"location":"project/#164-system-health-dimension-score","title":"1.6.4 System Health Dimension Score","text":"<pre><code>SystemHealthScore = 0.40*BottleneckScore\n                  + 0.30*CongestionScore\n                  + 0.30*PredictabilityScore\n</code></pre>"},{"location":"project/#2-healthy-vs-unhealthy-thresholds-initial-draft","title":"2. Healthy vs unhealthy thresholds (initial draft)","text":"<p>These are initial org-agnostic defaults; tune them to your context.</p>"},{"location":"project/#delivery","title":"Delivery","text":"<ul> <li> <p>Cycle time (PR \u2192 deploy)</p> </li> <li> <p>Excellent: &lt; 24h</p> </li> <li>Healthy: 24\u201372h</li> <li>At risk: 3\u20137 days</li> <li> <p>Unhealthy: &gt; 7 days</p> </li> <li> <p>Time to first review</p> </li> <li> <p>Excellent: &lt; 2h</p> </li> <li>Healthy: 2\u20138h</li> <li>At risk: 8\u201324h</li> <li> <p>Unhealthy: &gt; 24h</p> </li> <li> <p>Deployment frequency</p> </li> <li> <p>Excellent: multiple times per day</p> </li> <li>Healthy: daily\u2013few times/week</li> <li>At risk: &lt; 1/week</li> <li> <p>Unhealthy: &lt; 1/month</p> </li> <li> <p>Change failure rate</p> </li> <li>Excellent: &lt; 10%</li> <li>Healthy: 10\u201320%</li> <li>At risk: 20\u201330%</li> <li>Unhealthy: &gt; 30%</li> </ul>"},{"location":"project/#code-quality-risk","title":"Code Quality &amp; Risk","text":"<ul> <li> <p>Rework rate (loc re-touched within 30 days)</p> </li> <li> <p>Excellent: &lt; 10%</p> </li> <li>Healthy: 10\u201320%</li> <li>At risk: 20\u201335%</li> <li> <p>Unhealthy: &gt; 35%</p> </li> <li> <p>Average PR size (median)</p> </li> <li> <p>Healthy: 50\u2013300 LOC</p> </li> <li>At risk: 300\u2013600 LOC</li> <li> <p>Unhealthy: &gt; 600 LOC regularly</p> </li> <li> <p>Hotspot concentration (files with high risk)</p> </li> <li>Healthy: &lt; 5% of files</li> <li>At risk: 5\u201315%</li> <li>Unhealthy: &gt; 15%</li> </ul>"},{"location":"project/#dynamics-collaboration","title":"Dynamics &amp; Collaboration","text":"<ul> <li> <p>Bus factor (single-owner files &gt;75%)</p> </li> <li> <p>Healthy: &lt; 25% of team files</p> </li> <li>At risk: 25\u201350%</li> <li> <p>Unhealthy: &gt; 50%</p> </li> <li> <p>Review reciprocity</p> </li> <li>Healthy: most devs in [.5, 2] given/received ratio</li> <li>Unhealthy: many devs only receiving or only giving</li> </ul>"},{"location":"project/#well-being","title":"Well-being","text":"<ul> <li> <p>Late-night ratio</p> </li> <li> <p>Healthy: &lt; 5%</p> </li> <li>At risk: 5\u201315%</li> <li> <p>Unhealthy: &gt; 15%</p> </li> <li> <p>Weekend ratio</p> </li> <li> <p>Healthy: &lt; 5\u201310%</p> </li> <li>At risk: 10\u201325%</li> <li> <p>Unhealthy: &gt; 25%</p> </li> <li> <p>Flow time (uninterrupted focus per day)</p> </li> <li>Excellent: \u2265 2\u20133 hours</li> <li>Healthy: 1\u20132 hours</li> <li>At risk: &lt; 1 hour</li> <li>Unhealthy: mostly fragmented 10\u201315 min blocks</li> </ul>"},{"location":"project/#3-org-wide-developer-health-score","title":"3. Org-wide \u201cDeveloper Health Score\u201d","text":"<p>Use the 4D model:</p> <ol> <li>DeliveryScore</li> <li>DurabilityScore</li> <li>WellBeingScore</li> <li>DynamicsScore</li> </ol> <p>Optionally add SystemHealthScore as a 5th dimension.</p>"},{"location":"project/#31-dimension-aggregation","title":"3.1 Dimension aggregation","text":"<p>Example weights:</p> <pre><code>DH_Delivery   = DeliveryScore\nDH_Durability = DurabilityScore\nDH_WellBeing  = WellBeingScore\nDH_Dynamics   = DynamicsScore\nDH_System     = SystemHealthScore\n\nDeveloperHealthScore = 0.25*DH_Delivery\n                      + 0.25*DH_Durability\n                      + 0.20*DH_WellBeing\n                      + 0.20*DH_Dynamics\n                      + 0.10*DH_System\n</code></pre> <p>Compute per org, per team, per repo, per individual (with some metric substitutions).</p>"},{"location":"project/#7-predictive-ai-models","title":"7. Predictive AI models","text":""},{"location":"project/#71-delivery-risk-prediction-pr-level","title":"7.1 Delivery risk prediction (PR-level)","text":"<p>Goal: Predict whether a PR will be \u201cslow\u201d or \u201cproblematic\u201d at creation time.</p> <ul> <li>Label: <code>slow_pr = cycle_time &gt; org_p75_cycle_time</code> (binary)</li> <li>Features: PR size, file count, hotspot involvement, author historical cycle time, team WIP, time-of-day/day-of-week, reviewer count</li> <li>Model: Gradient boosting (XGBoost/LightGBM) or logistic regression</li> <li>Output: <code>P(slow_pr)</code> + suggested mitigations (split PR, add reviewer, etc.)</li> </ul>"},{"location":"project/#72-burnout-risk-prediction-user-level","title":"7.2 Burnout risk prediction (user-level)","text":"<p>Goal: Predict burnout risk for each dev next 2\u20134 weeks.</p> <ul> <li>Label: Proxy: future spike in late-night/weekend + drop in FlowScore (or HR flag if integrated)</li> <li>Features: late-night ratio, weekend ratio, FlowScore mean/variance, streak length, review load, context switching, team stress/incident load</li> <li>Model: time-series classification or GBM on rolling window features</li> <li>Output: <code>BurnoutRiskProbability</code> 0\u20131 \u2192 0\u2013100 display</li> </ul>"},{"location":"project/#73-expected-cycle-time-delivery-date","title":"7.3 Expected cycle time / delivery date","text":"<p>Goal: Predict cycle time for a new PR or task.</p> <ul> <li>Label: numeric cycle time</li> <li>Features: 7.1 + repo/component + team throughput context</li> <li>Model: regression (GBM, random forest, or linear w/ interactions)</li> <li>Output: predicted cycle time + confidence interval</li> </ul>"},{"location":"project/#74-hotspot-evolution","title":"7.4 Hotspot evolution","text":"<p>Goal: Predict which files will become high-risk hotspots soon.</p> <ul> <li>Label: <code>future_hotspot = hotspot_score_file &gt; threshold in next W</code></li> <li>Features: churn, entropy, commit frequency, bug density, ownership volatility</li> <li>Model: GBM / logistic regression</li> <li>Output: \u201cemerging risky modules\u201d highlights</li> </ul>"},{"location":"project/#8-dashboard-mockups-textual-figma","title":"8. Dashboard mockups (textual Figma)","text":""},{"location":"project/#81-org-dashboard-executive-view","title":"8.1 Org Dashboard (\u201cExecutive View\u201d)","text":"<p>Top bar</p> <ul> <li>Time range selector (Last 7 / 30 / 90 days)</li> <li>Org dropdown</li> <li>Overall Developer Health Score pill (e.g. 82/100) + sparkline</li> </ul> <p>Row 1: 4 dimension cards</p> <ul> <li>Delivery: score + median cycle time, deploy frequency, change failure rate</li> <li>Durability: score + rework %, hotspot count, defect rate</li> <li>Well-being: score + flow hrs/dev/day, burnout risk index</li> <li>Dynamics: score + review responsiveness, bus factor</li> </ul> <p>Row 2: Charts</p> <ul> <li>Cycle Time Breakdown (stacked: coding, review, deploy) over time</li> <li>Deploy Frequency vs Change Failure Rate (dual-axis)</li> </ul> <p>Row 3: Tables</p> <ul> <li>Teams ranked by Developer Health Score</li> <li>Top 10 hotspot repos/services</li> </ul>"},{"location":"project/#82-team-dashboard-eng-lead-view","title":"8.2 Team Dashboard (\u201cEng Lead View\u201d)","text":"<p>Header</p> <ul> <li>Team name + date range + Team Health Score</li> </ul> <p>Row 1: KPIs</p> <ul> <li>Delivery: cycle time, time to first review, deploys/week</li> <li>Quality: rework %, defect rate, top hotspots</li> <li>Well-being: flow hrs/dev/day, late-night %, burnout risk</li> <li>Dynamics: review delay, bus factor, review balance</li> </ul> <p>Row 2: Charts</p> <ul> <li>PR Flow Timeline (scatter: PRs by age vs cycle time, colored by size)</li> <li>Team Review Network (graph: nodes=devs, edges=reviews)</li> </ul> <p>Row 3: People table</p> <ul> <li>Dev, Impact proxy, FlowScore, BurnoutRisk, ReviewLoad, Churn%</li> <li>Flags: high review load, high late-night work, isolation</li> </ul>"},{"location":"project/#83-developer-dashboard-personal-view","title":"8.3 Developer Dashboard (\u201cPersonal View\u201d)","text":"<p>Header</p> <ul> <li>Dev name + \u201cYour Health this month: 78/100\u201d</li> </ul> <p>Row 1: Summary</p> <ul> <li>Delivery vs team, Quality vs team, Flow, Burnout risk</li> </ul> <p>Row 2: Charts</p> <ul> <li>Flow over time</li> <li>Work mix (new vs bugfix vs refactor)</li> </ul> <p>Row 3: Suggestions</p> <ul> <li>Auto-generated coaching insights (split PRs, protect focus blocks, reduce late-night pattern, etc.)</li> </ul>"},{"location":"project/#84-repo-service-dashboard","title":"8.4 Repo / Service Dashboard","text":"<p>Header</p> <ul> <li>Repo name + risk indicator</li> </ul> <p>Row 1: Risk &amp; Quality</p> <ul> <li>Risk gauge</li> <li>Hotspot file count</li> <li>Defects tied to repo</li> </ul> <p>Row 2: Hotspots table</p> <ul> <li>file path, risk, churn, contributors, linked bugs</li> </ul> <p>Row 3: Ownership &amp; Bus factor</p> <ul> <li>contributor distribution</li> <li>single-owner file visualization</li> </ul>"},{"location":"roadmap/","title":"Roadmap &amp; Remaining Tasks","text":"<p>This checklist tracks what is complete and what remains to finalize <code>dev-health-ops</code>.</p>"},{"location":"roadmap/#completed","title":"Completed","text":"<ul> <li>[x] Core Git + PR Metrics: Commit size, churn, PR cycle/pickup/review times, review load, PR size stats.</li> <li>[x] Work Item Flow Metrics: Cycle/lead time p50/p90, WIP count/age, flow efficiency, state duration + avg WIP.</li> <li>[x] Quality + Risk Metrics: Defect introduction rate, WIP congestion, rework churn ratio, single-owner file ratio proxy.</li> <li>[x] DORA Metrics: MTTR, change failure rate, deployment/incident daily rollups.</li> <li>[x] Wellbeing Signals: After-hours and weekend commit ratios; weekend active users (derived from user activity).</li> <li>[x] Connectors + Pipelines: GitHub/GitLab CI/CD + deployments + incidents ingestion; async batch helpers.</li> <li>[x] Storage + Schema: ClickHouse migrations and sink support for new metrics tables/columns.</li> <li>[x] SQLite test cleanup: Dispose SQLAlchemyStore engines to avoid aiosqlite event-loop teardown warnings.</li> <li>[x] CLI Controls: Dedicated <code>sync cicd</code>, <code>sync deployments</code>, and <code>sync incidents</code> targets.</li> <li>[x] Complexity Metrics (Batch): Radon-based cyclomatic complexity scanning with DB-driven batch mode (<code>-s \"*\"</code>).</li> <li>[x] Unified Complexity Snapshot Loading: <code>metrics daily</code> loads complexity snapshots via storage abstraction (<code>get_complexity_snapshots</code>).</li> <li>[x] Investment Area Classification: Automated work item and churn classification based on configurable rules.</li> <li>[x] Work Item Sync Command: Dedicated <code>sync work-items</code> flow to fetch provider work items separately from <code>metrics daily</code>.</li> <li>[x] Work Item Auth Override: <code>sync work-items --auth</code> can override GitHub/GitLab tokens for provider sync.</li> <li>[x] Synthetic fixtures: CI/CD + deployments + incidents with metrics rollups for ClickHouse.</li> <li>[x] IC Metrics + Landscape: Identity resolution, unified UserMetricsDailyRecord, and landscape rolling stats (Churn/Cycle/WIP vs Throughput).</li> <li>[x] Grafana Dev Health Panels: Panel plugin with Developer Landscape, Hotspot Explorer, and Investment Flow views.</li> <li>[x] Dev Health panel theming: Plugin-local Grafana theme with a custom visualization palette.</li> <li>[x] Dev Health panel selector: Panel settings dropdown to switch between Developer Landscape, Hotspot Explorer, and Investment Flow.</li> <li>[x] Grafana Panel ClickHouse Contracts: Stats schema views for landscape, hotspot, and investment flow panels (use <code>WITH ... AS</code> aliasing).</li> <li>[x] Dashboard team filter normalization: Include legacy NULL/empty team IDs in Grafana ClickHouse queries via <code>ifNull(nullIf(team_id, ''), 'unassigned')</code>.</li> <li>[x] Investment metrics NULL team IDs: <code>investment_metrics_daily.team_id</code> stores NULL for unassigned; investment flow view casts via <code>toNullable(team_id)</code>.</li> <li>[x] Hotspot Explorer formatting: Use table format and order by day to keep Grafana sorting valid.</li> <li>[x] Hotspot ownership concentration: Derive ownership concentration from git blame line shares.</li> <li>[x] Synthetic blame coverage: Expand fixture file set to improve hotspot ownership coverage.</li> <li>[x] Blame-only sync: Add <code>cli.py sync blame --provider &lt;local|github|gitlab&gt;</code> for targeted blame backfill.</li> <li>[x] Backfill commit caps: GitHub/GitLab <code>--date/--backfill</code> runs default to unlimited commits unless explicitly capped.</li> <li>[x] Hotspot Explorer frame binding: Prefer facts frames via <code>churn_loc_30d</code> to keep drivers/trends rendering.</li> <li>[x] IC Drilldown churn vs throughput: Add identity-filtered churn vs throughput panel.</li> </ul>"},{"location":"roadmap/#remaining","title":"Remaining","text":""},{"location":"roadmap/#data-fixtures","title":"Data + Fixtures","text":"<ul> <li>[ ] Fixtures validation: Ensure synthetic work items + commits generate non-zero Phase 2 metrics.</li> </ul>"},{"location":"roadmap/#dashboards","title":"Dashboards","text":"<ul> <li>[x] Dashboards for CI/CD, deployments, incidents (panels for success rate, duration, deploy counts, MTTR).</li> <li>[x] Investment Areas team filter: use regex <code>match(...)</code> filters for the team variable in ClickHouse queries.</li> <li>[ ] Work tracking dashboards audit: Validate filters and table joins for synthetic + real providers.</li> <li>[ ] Fix dashboard templating filters: Ensure variable regex and <code>match(...)</code> filters do not return empty results.</li> </ul>"},{"location":"roadmap/#metrics-enhancements","title":"Metrics Enhancements","text":"<ul> <li>[x] Bus factor (true): Top contributors accounting for &gt;50% of churn (Bus Factor) and Gini coefficient (Knowledge Distribution).</li> <li>[x] Predictability index: Completion Rate (items completed / (completed + wip)).</li> <li>[ ] Capacity planning: Forecast completion using historical throughput.</li> <li>[ ] Identity Linking: Reliable mapping of Work Items (Jira/LinearB) to Git commits (e.g. via commit messages or smart matching).</li> <li>[ ] Work Item Repo Filtering by Tags/Settings: Allow <code>sync work-items</code> to filter repos using <code>repos.tags</code> or <code>repos.settings</code> (beyond name glob).</li> </ul>"},{"location":"roadmap/#testing-docs","title":"Testing + Docs","text":"<ul> <li>[x] Tests for new sinks columns (ClickHouse + SQLite write paths for Phase 2 + wellbeing).</li> <li>[x] Docs refresh: Usage examples for new CLI flags and fixture generation steps.</li> </ul>"},{"location":"task_trackers/","title":"Task Trackers &amp; Work Items","text":"<p>This repo normalizes Jira issues, GitHub issues/Projects items, and GitLab issues into a unified <code>WorkItem</code> model (<code>models/work_items.py</code>) and computes daily aggregates + cycle times.</p> <p>Jira is used to track associated project work (planning/throughput/WIP). Pull request metrics are computed from PR/MR data synced via the CLI (<code>python cli.py sync ...</code>) and are independent of Jira.</p>"},{"location":"task_trackers/#provider-credentials-env-vars","title":"Provider Credentials (env vars)","text":""},{"location":"task_trackers/#jira-cloud","title":"Jira (Cloud)","text":"<ul> <li><code>JIRA_BASE_URL</code> (e.g. <code>your-org.atlassian.net</code> or <code>https://your-org.atlassian.net</code>; normalized to <code>https://</code>)</li> <li><code>JIRA_EMAIL</code></li> <li><code>JIRA_API_TOKEN</code></li> <li><code>JIRA_PROJECT_KEYS</code> (optional, comma-separated, e.g. <code>ABC,XYZ</code>)</li> <li><code>JIRA_JQL</code> (optional override; if set, used as-is instead of the built-in windowed query)</li> <li><code>JIRA_FETCH_ALL</code> (optional; set to <code>1</code> to fetch all issues in the project(s) regardless of date window \u2014 can be very slow)</li> <li><code>JIRA_FETCH_COMMENTS</code> (optional; set to <code>0</code> to disable comment metadata ingestion; default: <code>1</code>)</li> </ul> <p>Optional Jira field mappings (instance-specific): - <code>JIRA_STORY_POINTS_FIELD</code> (e.g. <code>customfield_10016</code>) - <code>JIRA_SPRINT_FIELD</code> (default: <code>customfield_10020</code>) - <code>JIRA_EPIC_LINK_FIELD</code> (e.g. <code>customfield_10014</code>)</p>"},{"location":"task_trackers/#github","title":"GitHub","text":"<ul> <li><code>GITHUB_TOKEN</code></li> <li>Optional CLI override: <code>python cli.py sync work-items --provider github --auth \"$GITHUB_TOKEN\" ...</code></li> </ul> <p>Optional Projects v2 ingestion: - <code>GITHUB_PROJECTS_V2</code> as comma-separated <code>org_login:project_number</code> entries, e.g.:   - <code>GITHUB_PROJECTS_V2=\"myorg:3,anotherorg:12\"</code></p>"},{"location":"task_trackers/#gitlab","title":"GitLab","text":"<ul> <li><code>GITLAB_TOKEN</code></li> <li><code>GITLAB_URL</code> (optional, default: <code>https://gitlab.com</code>)</li> <li>Optional CLI override: <code>python cli.py sync work-items --provider gitlab --auth \"$GITLAB_TOKEN\" ...</code></li> </ul>"},{"location":"task_trackers/#status-type-normalization","title":"Status &amp; Type Normalization","text":"<p>Status normalization is config-driven via <code>config/status_mapping.yaml</code>.</p>"},{"location":"task_trackers/#status-categories","title":"Status categories","text":"<p>Normalized categories are: - <code>backlog</code> - <code>todo</code> - <code>in_progress</code> - <code>in_review</code> - <code>blocked</code> - <code>done</code> - <code>canceled</code></p>"},{"location":"task_trackers/#provider-specific-rules","title":"Provider-specific rules","text":"<p>The mapping file supports: - Jira: <code>providers.jira.statuses</code> (maps Jira status names) - GitHub/GitLab: <code>providers.&lt;provider&gt;.status_labels</code> (maps label names to categories)</p> <p>If no label/status match exists: - GitHub: <code>open \u2192 todo</code>, <code>closed \u2192 done</code> - GitLab: <code>opened \u2192 todo</code>, <code>closed \u2192 done</code></p>"},{"location":"task_trackers/#identity-mapping-optional","title":"Identity Mapping (optional)","text":"<p>To keep user metrics consistent across providers, populate <code>config/identity_mapping.yaml</code>.</p> <p>Schema: - <code>canonical</code>: stable identity (prefer email) - <code>aliases</code>: provider-qualified logins (e.g. <code>github:octocat</code>) or Jira account IDs (e.g. <code>jira:accountid:abcd123</code>)</p>"},{"location":"task_trackers/#team-mapping-optional","title":"Team Mapping (optional)","text":"<p>To enable team filtering in Grafana, you can sync teams from various sources.</p>"},{"location":"task_trackers/#config-based-mapping","title":"Config-based Mapping","text":"<p>Populate <code>config/team_mapping.yaml</code> (schema: <code>team_id</code>, <code>team_name</code>, <code>members</code>). Then run: <pre><code>python cli.py sync teams --path config/team_mapping.yaml\n</code></pre></p>"},{"location":"task_trackers/#jira-project-mapping","title":"Jira Project Mapping","text":"<p>Automatically import Jira projects as teams: <pre><code>python cli.py sync teams --provider jira\n</code></pre></p>"},{"location":"task_trackers/#running-jira-work-metrics","title":"Running Jira work metrics","text":"<p>Jira work items are fetched via the work item sync job:</p> <pre><code>python cli.py sync work-items --provider jira --date 2025-02-01 --backfill 30 --db \"clickhouse://localhost:8123/default\"\n</code></pre> <p>Use <code>-s</code>/<code>--search</code> to filter repos by name (glob pattern), e.g.:</p> <pre><code>python cli.py sync work-items --provider github -s \"org/*\" --date 2025-02-01 --backfill 30 --db \"clickhouse://localhost:8123/default\"\n</code></pre>"},{"location":"task_trackers/#quick-jira-api-smoke-test-curl","title":"Quick Jira API smoke test (curl)","text":"<p>Jira Cloud has removed <code>GET /rest/api/3/search</code>; use <code>GET /rest/api/3/search/jql</code>:</p> <pre><code>curl -sS -u \"$JIRA_EMAIL:$JIRA_API_TOKEN\" \\\n  --get \"https://$JIRA_BASE_URL/rest/api/3/search/jql\" \\\n  --data-urlencode \"jql=(updated &gt;= '2025-09-10' OR (statusCategory != Done AND created &lt;= '2025-12-18')) ORDER BY updated DESC\" \\\n  --data-urlencode \"maxResults=5\" \\\n  --data-urlencode \"fields=key,summary,updated,status\"\n</code></pre>"},{"location":"webhooks/","title":"Webhook Setup Documentation","text":"<p>This document describes how to configure webhooks for real-time data synchronization in Dev Health Ops.</p>"},{"location":"webhooks/#github-configuration","title":"GitHub Configuration","text":"<ol> <li>Go to your repository or organization Settings.</li> <li>Select Webhooks from the sidebar.</li> <li>Click Add webhook.</li> <li>Set Payload URL to <code>https://your-dev-health-instance.com/api/v1/webhooks/github</code>.</li> <li>Set Content type to <code>application/json</code>.</li> <li>Enter a Secret (must match <code>GITHUB_WEBHOOK_SECRET</code> environment variable).</li> <li>Select Let me select individual events:</li> <li>Pushes</li> <li>Pull requests</li> <li>Issues</li> <li>Deployments</li> <li>Workflow runs</li> <li>Click Add webhook.</li> </ol>"},{"location":"webhooks/#gitlab-configuration","title":"GitLab Configuration","text":"<ol> <li>Go to your project or group Settings &gt; Webhooks.</li> <li>Set URL to <code>https://your-dev-health-instance.com/api/v1/webhooks/gitlab</code>.</li> <li>Set Secret token (must match <code>GITLAB_WEBHOOK_TOKEN</code> environment variable).</li> <li>Under Trigger, select:</li> <li>Push events</li> <li>Tag push events</li> <li>Merge request events</li> <li>Issue events</li> <li>Pipeline events</li> <li>Job events</li> <li>Click Add webhook.</li> </ol>"},{"location":"webhooks/#jira-configuration","title":"Jira Configuration","text":"<ol> <li>Log in as a Jira Administrator.</li> <li>Go to System &gt; Webhooks.</li> <li>Click Create a Webhook.</li> <li>Set URL to <code>https://your-dev-health-instance.com/api/v1/webhooks/jira</code>.</li> <li>(Optional) Add <code>?secret=your_secret</code> to the URL if <code>JIRA_WEBHOOK_SECRET</code> is configured.</li> <li>Under Events, select:</li> <li>Issue: created, updated, deleted.</li> <li>Click Create.</li> </ol>"},{"location":"webhooks/#environment-variables","title":"Environment Variables","text":"<p>Ensure the following variables are set in your deployment environment:</p> Variable Description <code>GITHUB_WEBHOOK_SECRET</code> HMAC secret for GitHub signature validation <code>GITLAB_WEBHOOK_TOKEN</code> Token for GitLab X-Gitlab-Token validation <code>JIRA_WEBHOOK_SECRET</code> Optional secret for Jira validation <code>REDIS_URL</code> Required for webhook delivery idempotency"},{"location":"api/graphql-overview/","title":"GraphQL Analytics API","text":"<p>Primary server code: - <code>api/graphql/app.py</code> - <code>api/graphql/schema.py</code> - <code>api/graphql/resolvers/analytics.py</code> - <code>api/graphql/sql/compiler.py</code></p>"},{"location":"api/graphql-overview/#what-it-provides","title":"What it provides","text":"<ul> <li>Breakdowns: grouped aggregations (for treemaps, tables)</li> <li>Timeseries: bucketed metrics (for area/line charts)</li> <li>Sankey: node/edge flows (for investment flows)</li> </ul>"},{"location":"api/graphql-overview/#key-design-points","title":"Key design points","text":"<ul> <li>Queries compile to SQL via <code>api/graphql/sql/*</code> and execute against the analytics store.</li> <li>Cost limits and validation are enforced in <code>api/graphql/cost.py</code>.</li> <li>Caching and persisted queries are supported via <code>api/graphql/persisted.py</code> and <code>api/graphql/persisted_queries.json</code>.</li> </ul>"},{"location":"api/graphql-overview/#persisted-queries","title":"Persisted queries","text":"<p>See: <code>docs/50-api/04-persisted-queries.md</code>.</p>"},{"location":"api/graphql-overview/#web-client","title":"Web client","text":"<p>See: <code>docs/50-api/06-web-graphql-client.md</code>.</p>"},{"location":"api/persisted-queries/","title":"Persisted queries","text":"<p>File: <code>api/graphql/persisted_queries.json</code></p>"},{"location":"api/persisted-queries/#purpose","title":"Purpose","text":"<p>Persisted queries allow the UI to reference stable query identifiers instead of sending ad-hoc query text.</p>"},{"location":"api/persisted-queries/#operational-benefits","title":"Operational benefits","text":"<ul> <li>Enables server-side caching keyed by query id + variables</li> <li>Simplifies authorization gates and cost enforcement</li> <li>Reduces attack surface (fewer dynamic query shapes)</li> </ul>"},{"location":"api/persisted-queries/#invalidation","title":"Invalidation","text":"<p>Invalidation hooks live in: - <code>api/graphql/cache_invalidation.py</code> - <code>api/graphql/pubsub.py</code></p>"},{"location":"api/view-mapping/","title":"View-to-API mapping","text":"<p>This is the contract between UI charts and backend query surfaces.</p>"},{"location":"api/view-mapping/#investment-mix-treemapsunburst","title":"Investment Mix (treemap/sunburst)","text":"<ul> <li>GraphQL: analytics \u2192 breakdowns</li> <li>Backend: <code>api/graphql/resolvers/analytics.py</code></li> <li>Compiler: <code>api/graphql/sql/compiler.py::compile_breakdown</code></li> <li>Dimensions: THEME, SUBCATEGORY</li> <li>Measure: effort/count (depends on chart config)</li> </ul>"},{"location":"api/view-mapping/#investment-expense-stacked-area","title":"Investment Expense (stacked area)","text":"<ul> <li>GraphQL: analytics \u2192 timeseries</li> <li>Compiler: <code>compile_timeseries</code></li> </ul>"},{"location":"api/view-mapping/#investment-flows-sankey","title":"Investment Flows (sankey)","text":"<ul> <li>GraphQL: analytics \u2192 sankey</li> <li>Compiler: <code>compile_sankey</code></li> <li>Limits: <code>validate_sankey_limits</code></li> </ul>"},{"location":"api/view-mapping/#hotspots","title":"Hotspots","text":"<ul> <li>GraphQL: breakdowns or timeseries depending on visualization</li> <li>Backend: analytics resolver + SQL compiler</li> </ul>"},{"location":"api/view-mapping/#explore-flame-diagrams","title":"Explore: Flame diagrams","text":"<ul> <li>Input: \u201crepresentative point\u201d selection</li> <li>Output: hierarchical breakdown suitable for flame render</li> <li>Server: (add explicit endpoint if not already present; otherwise a persisted breakdown query)</li> </ul>"},{"location":"api/web-graphql-client/","title":"Web GraphQL client","text":"<p>Source: <code>dev-health-web/docs/graphql-client.md</code></p>"},{"location":"api/web-graphql-client/#library-stack","title":"Library stack","text":"<ul> <li>urql for operations + normalized caching</li> <li>React hooks for fetching</li> <li>Subscriptions via WebSocket</li> <li>Zod validation for runtime checking</li> </ul>"},{"location":"api/web-graphql-client/#environment-switch","title":"Environment switch","text":"<p><code>NEXT_PUBLIC_USE_GRAPHQL_ANALYTICS=false</code> disables GraphQL analytics.</p>"},{"location":"api/web-graphql-client/#key-implementation-files","title":"Key implementation files","text":"<ul> <li><code>src/lib/graphql/provider.tsx</code></li> <li><code>src/lib/graphql/hooks.ts</code></li> <li><code>src/lib/graphql/client.ts</code></li> </ul>"},{"location":"api/web-graphql-investment/","title":"Web: GraphQL investment usage","text":"<p>Source: <code>dev-health-web/docs/graphql-investment.md</code></p> <p>This is the frontend usage and shape expectations for investment views, including: - Dimension keys (THEME/SUBCATEGORY) - Sankey request shape - Response parsing conventions</p> <p>Keep this aligned with: - backend taxonomy (<code>investment_taxonomy.py</code>) - GraphQL inputs/outputs (<code>api/graphql/models/*</code>)</p>"},{"location":"appendix/ops-AGENTS/","title":"AGENTS \u2014 Single Source of Truth (dev-health-ops)","text":"<p>This document is the canonical agent briefing for this repo: how it works, what not to break, and the minimum rules to move fast without causing architectural regressions. It consolidates prior agent guidance into one set of enforceable constraints.</p>"},{"location":"appendix/ops-AGENTS/#0-read-first-order-matters","title":"0) Read-first (order matters)","text":"<ol> <li><code>cli.py</code></li> <li><code>processors/local.py</code></li> <li><code>connectors/__init__.py</code></li> </ol> <p>Goal: understand boundaries (ingest \u2192 normalize \u2192 persist \u2192 metricize \u2192 visualize) before touching anything.</p>"},{"location":"appendix/ops-AGENTS/#1-mission-and-product-intent","title":"1) Mission and product intent","text":"<p><code>dev-health-ops</code> is an OSS analytics platform for team operating modes and developer health, backed by provider data (GitHub, GitLab, Jira, local Git) and computed metrics stored in DB sinks.</p>"},{"location":"appendix/ops-AGENTS/#investment-view-is-canonical","title":"Investment View is canonical","text":"<ul> <li>Signals is retired (POC only). Do not extend it.</li> <li>The platform answers: \u201cWhere is human effort actually being invested, and what is the cost to people when certain work dominates?\u201d</li> </ul>"},{"location":"appendix/ops-AGENTS/#2-system-architecture-pipeline","title":"2) System architecture (pipeline)","text":""},{"location":"appendix/ops-AGENTS/#21-data-flow","title":"2.1 Data flow","text":"<ol> <li> <p>Connectors (<code>connectors/</code>)</p> </li> <li> <p>Fetch raw provider data.</p> </li> <li> <p>Network I/O should be async and batch-friendly.</p> </li> <li> <p>Processors (<code>processors/</code>)</p> </li> <li> <p>Normalize/transform connector outputs into internal models.</p> </li> <li> <p>Storage / Sinks (<code>metrics/sinks/</code>)</p> </li> <li> <p>Persist computed outputs.</p> </li> <li> <p>No file exports. No debug dumps. No JSON/YAML output paths.</p> </li> <li> <p>Metrics (<code>metrics/</code>)</p> </li> <li> <p>Compute higher-level rollups (daily/backfills/etc.) from persisted data.</p> </li> <li> <p>Visualization (Grafana + dev-health-web)</p> </li> <li> <p>UI renders persisted data.</p> </li> <li><code>dev-health-web</code> is visualization-only: it must not become the source of truth.</li> </ol>"},{"location":"appendix/ops-AGENTS/#22-storage-backends-supported","title":"2.2 Storage backends (supported)","text":"<ul> <li>PostgreSQL (SQLAlchemy + Alembic)</li> <li>ClickHouse (analytics store)</li> <li>MongoDB</li> <li>SQLite</li> </ul> <p>Backend selection:</p> <ul> <li>CLI <code>--db</code> flag or <code>DATABASE_URI</code></li> <li>Secondary sink via <code>SECONDARY_DATABASE_URI</code> when using <code>sink='both'</code>.</li> </ul>"},{"location":"appendix/ops-AGENTS/#3-the-non-negotiable-work-graph-investment-contract","title":"3) The non-negotiable Work Graph + Investment contract","text":""},{"location":"appendix/ops-AGENTS/#31-core-contract","title":"3.1 Core contract","text":"<ul> <li>WorkUnits are evidence containers, not categories.</li> <li>LLM decides subcategory distributions at compute time only.</li> <li>Theme roll-up is deterministic from subcategories.</li> <li>UX renders only persisted distributions and edges.</li> <li>LLM explanations may run on-demand but must not alter persisted decisions.</li> <li>Sinks only for persistence (no output files).</li> </ul>"},{"location":"appendix/ops-AGENTS/#32-canonical-taxonomy","title":"3.2 Canonical taxonomy","text":""},{"location":"appendix/ops-AGENTS/#themes-fixed","title":"Themes (fixed)","text":"<ul> <li>Feature Delivery</li> <li>Operational / Support</li> <li>Maintenance / Tech Debt</li> <li>Quality / Reliability</li> <li>Risk / Security</li> </ul> <p>Rules:</p> <ul> <li>No synonyms.</li> <li>No overrides.</li> <li>No per-team configuration.</li> <li>Provider-native labels/types are inputs only and must be normalized away.</li> </ul>"},{"location":"appendix/ops-AGENTS/#subcategories-fixed-per-theme","title":"Subcategories (fixed per theme)","text":"<ul> <li>There is a small curated set per theme.</li> <li>Subcategories provide resolution without fragmenting language.</li> <li>Subcategory probabilities roll up to theme probabilities.</li> </ul>"},{"location":"appendix/ops-AGENTS/#33-data-model-guarantees","title":"3.3 Data model guarantees","text":"<p>For every WorkUnit (evidence container):</p> <ul> <li>Theme probabilities sum to ~1.0</li> <li>Subcategory probabilities sum to theme probabilities</li> <li>Evidence arrays exist (may be empty)</li> <li>Evidence quality always emitted</li> <li>Categorization never returns \u201cunknown\u201d</li> </ul>"},{"location":"appendix/ops-AGENTS/#4-llm-usage-rules","title":"4) LLM usage rules","text":""},{"location":"appendix/ops-AGENTS/#41-compute-time-categorization-required","title":"4.1 Compute-time categorization (required)","text":"<ul> <li>Output must be strict JSON matching <code>work_graph/investment/llm_schema.py</code>.</li> <li>Keys must be from canonical subcategory registry.</li> <li>Probabilities must be normalized and valid.</li> <li>Evidence quotes must be extractive substrings from provided inputs.</li> <li>Retry policy: one repair attempt only.</li> <li>On failure: mark invalid + apply deterministic fallback.</li> <li>Persist audit fields for every run.</li> </ul>"},{"location":"appendix/ops-AGENTS/#42-ux-time-explanations-allowed-constrained","title":"4.2 UX-time explanations (allowed, constrained)","text":"<ul> <li>Explanations can only use persisted distributions and stored evidence.</li> <li>Explanations must not recompute categories/edges/weights.</li> <li>All explanation output must be labeled AI-generated.</li> </ul> <p>Language constraints:</p> <ul> <li>Allowed: appears, leans, suggests</li> <li>Forbidden: is, was, detected, determined</li> </ul> <p>Explanation format:</p> <ul> <li>SUMMARY (max 3 sentences)</li> <li>REASONS (specific evidence)</li> <li>UNCERTAINTY (limits + evidence quality)</li> </ul>"},{"location":"appendix/ops-AGENTS/#5-visualization-rules-grafana-web","title":"5) Visualization rules (Grafana + web)","text":""},{"location":"appendix/ops-AGENTS/#51-investment-views","title":"5.1 Investment views","text":"<ul> <li>Treemap: Theme-level by default</li> <li>Sunburst: Theme \u2192 (optional) scope/team \u2192 (optional) clusters</li> <li>Sankey: Theme \u2192 scope/team pressure flows</li> </ul>"},{"location":"appendix/ops-AGENTS/#52-drill-down-contract","title":"5.2 Drill-down contract","text":"<ul> <li>Default: Theme-only (leadership readable)</li> <li>Drill: Theme \u2192 Subcategory \u2192 Evidence (WorkUnits)</li> <li>Never show WorkUnits as peers to themes/subcategories.</li> </ul>"},{"location":"appendix/ops-AGENTS/#53-grafana-query-conventions-when-touching-dashboards","title":"5.3 Grafana query conventions (when touching dashboards)","text":"<ul> <li>Prefer table format and stable time ordering where required.</li> <li>Handle legacy <code>team_id</code> null/empty normalization.</li> <li>Avoid ClickHouse <code>WITH name = expr</code> syntax; use <code>WITH ... AS</code> aliasing.</li> </ul> <p>(Only modify Grafana provisioning when needed; do not replatform dashboards incidentally.)</p>"},{"location":"appendix/ops-AGENTS/#6-developer-workflow-cli","title":"6) Developer workflow (CLI)","text":""},{"location":"appendix/ops-AGENTS/#61-sync-data","title":"6.1 Sync data","text":"<ul> <li> <p>Git data (local):</p> </li> <li> <p><code>python cli.py sync git --provider local --db \"$DATABASE_URI\" --repo-path /path/to/repo</code></p> </li> <li> <p>Work items:</p> </li> <li> <p><code>python cli.py sync work-items --provider &lt;jira|github|gitlab|synthetic|all&gt; -s \"org/*\" --db \"$DATABASE_URI\"</code></p> </li> <li> <p>Teams:</p> </li> <li> <p><code>python cli.py sync teams --provider &lt;config|jira|synthetic&gt; --db \"$DATABASE_URI\"</code></p> </li> </ul>"},{"location":"appendix/ops-AGENTS/#62-generate-synthetic-data","title":"6.2 Generate synthetic data","text":"<ul> <li><code>python cli.py fixtures generate --db \"$DATABASE_URI\" --days 30</code></li> </ul>"},{"location":"appendix/ops-AGENTS/#63-compute-metrics","title":"6.3 Compute metrics","text":"<ul> <li> <p>Daily rollups:</p> </li> <li> <p><code>python cli.py metrics daily --db \"$DATABASE_URI\"</code></p> </li> </ul>"},{"location":"appendix/ops-AGENTS/#7-engineering-rules-for-agents","title":"7) Engineering rules for agents","text":""},{"location":"appendix/ops-AGENTS/#71-change-discipline","title":"7.1 Change discipline","text":"<ul> <li>Prefer minimal, surgical changes.</li> <li>Keep surrounding style; use targeted edits.</li> <li>Add/adjust tests under <code>tests/</code> for behavior changes.</li> <li>If DB models change: include Alembic migrations (Postgres).</li> </ul>"},{"location":"appendix/ops-AGENTS/#72-correct-boundaries","title":"7.2 Correct boundaries","text":"<ul> <li>Connectors fetch. Processors normalize. Metrics compute. Sinks persist.</li> <li>Do not collapse responsibilities into one layer.</li> <li>Do not add \u201chelpful\u201d outputs like file dumps. Persistence goes through sinks only.</li> </ul>"},{"location":"appendix/ops-AGENTS/#73-performance-and-reliability","title":"7.3 Performance and reliability","text":"<ul> <li>Async/batching for network I/O.</li> <li>Respect any existing rate limit/backoff mechanisms.</li> <li>Close SQLAlchemy engines in tests to avoid event-loop teardown warnings.</li> </ul>"},{"location":"appendix/ops-AGENTS/#8-what-to-do-when-you-start-a-task","title":"8) What to do when you start a task","text":"<ol> <li>Identify which layer you\u2019re changing (connector, processor, metric, sink, viz).</li> <li>Re-state the relevant non-negotiables (WorkUnits are evidence; themes/subcats canonical; sinks only).</li> <li>Make the smallest possible change that achieves the outcome.</li> <li>Add a test (or update an existing one) for the new behavior.</li> <li>Ensure no new outputs bypass sinks.</li> </ol>"},{"location":"appendix/ops-AGENTS/#9-quick-reference","title":"9) Quick reference","text":""},{"location":"appendix/ops-AGENTS/#hard-bans","title":"Hard bans","text":"<ul> <li>Treating WorkUnits as categories</li> <li>User-configurable categories/subcategories</li> <li>\u201cUnknown\u201d categorization output</li> <li>LLM recomputation at UX-time</li> <li>Any persistence path outside <code>metrics/sinks/*</code></li> </ul>"},{"location":"appendix/ops-AGENTS/#allowed-references-to-dev-health-web","title":"Allowed references to dev-health-web","text":"<ul> <li>Visualization implementation details only (charts, drill-down UX, rendering).</li> <li>Not allowed: redefining taxonomy, recomputing categories, or becoming data source.</li> </ul>"},{"location":"appendix/ops-AGENTS/#landing-the-plane-session-completion","title":"Landing the Plane (Session Completion)","text":"<p>When ending a work session, you MUST complete ALL steps below. Work is NOT complete until <code>git push</code> succeeds.</p> <p>MANDATORY WORKFLOW:</p> <ol> <li>File issues for remaining work - Create issues for anything that needs follow-up</li> <li>Run quality gates (if code changed) - Tests, linters, builds</li> <li>Update issue status - Close finished work, update in-progress items</li> <li>PUSH TO REMOTE - This is MANDATORY:    <pre><code>git pull --rebase\nbd sync\ngit push\ngit status  # MUST show \"up to date with origin\"\n</code></pre></li> <li>Clean up - Clear stashes, prune remote branches</li> <li>Verify - All changes committed AND pushed</li> <li>Hand off - Provide context for next session</li> </ol> <p>CRITICAL RULES: - Work is NOT complete until <code>git push</code> succeeds - NEVER stop before pushing - that leaves work stranded locally - NEVER say \"ready to push when you are\" - YOU must push - If push fails, resolve and retry until it succeeds</p>"},{"location":"appendix/web-graphql-client/","title":"GraphQL Client Guide","text":"<p>This guide covers using the GraphQL client in dev-health-web.</p>"},{"location":"appendix/web-graphql-client/#overview","title":"Overview","text":"<p>The frontend uses urql for GraphQL operations with:</p> <ul> <li>Normalized caching - Automatic cache updates and deduplication</li> <li>React hooks - Easy data fetching in components</li> <li>Subscriptions - Real-time updates via WebSocket</li> <li>Zod validation - Runtime type checking of responses</li> </ul>"},{"location":"appendix/web-graphql-client/#setup","title":"Setup","text":""},{"location":"appendix/web-graphql-client/#provider","title":"Provider","text":"<p>Wrap your app or page with the GraphQL provider:</p> <pre><code>import { GraphQLProvider } from '@/lib/graphql/provider';\n\nexport default function Layout({ children }) {\n  return (\n    &lt;GraphQLProvider orgId=\"my-org\"&gt;\n      {children}\n    &lt;/GraphQLProvider&gt;\n  );\n}\n</code></pre>"},{"location":"appendix/web-graphql-client/#environment","title":"Environment","text":"<p>GraphQL is enabled by default. To disable:</p> <pre><code>NEXT_PUBLIC_USE_GRAPHQL_ANALYTICS=false\n</code></pre>"},{"location":"appendix/web-graphql-client/#hooks","title":"Hooks","text":""},{"location":"appendix/web-graphql-client/#useanalytics","title":"useAnalytics","text":"<p>Fetch analytics data (breakdowns, timeseries, sankey):</p> <pre><code>import { useAnalytics } from '@/lib/graphql/hooks';\n\nfunction InvestmentChart() {\n  const { data, loading, error } = useAnalytics({\n    orgId: 'my-org',\n    batch: {\n      breakdowns: [\n        {\n          dimension: 'THEME',\n          measure: 'COUNT',\n          dateRange: { startDate: '2024-01-01', endDate: '2024-01-31' },\n          topN: 10,\n        },\n      ],\n      timeseries: [],\n    },\n  });\n\n  if (loading) return &lt;Spinner /&gt;;\n  if (error) return &lt;Error message={error.message} /&gt;;\n\n  return (\n    &lt;Chart data={data.breakdowns[0].items} /&gt;\n  );\n}\n</code></pre>"},{"location":"appendix/web-graphql-client/#usebreakdown","title":"useBreakdown","text":"<p>Simplified hook for single breakdown queries:</p> <pre><code>import { useBreakdown } from '@/lib/graphql/hooks';\n\nfunction TeamBreakdown() {\n  const { data, loading } = useBreakdown({\n    orgId: 'my-org',\n    dimension: 'TEAM',\n    measure: 'THROUGHPUT',\n    startDate: '2024-01-01',\n    endDate: '2024-01-31',\n    topN: 10,\n  });\n\n  // ...\n}\n</code></pre>"},{"location":"appendix/web-graphql-client/#usesankey","title":"useSankey","text":"<p>Fetch Sankey flow data:</p> <pre><code>import { useSankey } from '@/lib/graphql/hooks';\n\nfunction FlowChart() {\n  const { data, loading } = useSankey({\n    orgId: 'my-org',\n    path: ['THEME', 'TEAM'],\n    measure: 'COUNT',\n    startDate: '2024-01-01',\n    endDate: '2024-01-31',\n    maxNodes: 50,\n    maxEdges: 200,\n  });\n\n  // data.sankey.nodes, data.sankey.edges\n}\n</code></pre>"},{"location":"appendix/web-graphql-client/#usecatalog","title":"useCatalog","text":"<p>Fetch catalog metadata:</p> <pre><code>import { useCatalog } from '@/lib/graphql/hooks';\n\nfunction FilterBar() {\n  const { data, loading } = useCatalog({\n    orgId: 'my-org',\n  });\n\n  // data.dimensions, data.measures, data.limits\n}\n</code></pre>"},{"location":"appendix/web-graphql-client/#usedimensionvalues","title":"useDimensionValues","text":"<p>Fetch distinct values for a dimension:</p> <pre><code>import { useDimensionValues } from '@/lib/graphql/hooks';\n\nfunction TeamSelect() {\n  const { values, loading } = useDimensionValues({\n    orgId: 'my-org',\n    dimension: 'TEAM',\n  });\n\n  return (\n    &lt;Select&gt;\n      {values.map((v) =&gt; (\n        &lt;Option key={v.value} value={v.value}&gt;\n          {v.value} ({v.count})\n        &lt;/Option&gt;\n      ))}\n    &lt;/Select&gt;\n  );\n}\n</code></pre>"},{"location":"appendix/web-graphql-client/#subscriptions","title":"Subscriptions","text":""},{"location":"appendix/web-graphql-client/#usemetricsupdated","title":"useMetricsUpdated","text":"<p>Subscribe to metrics updates:</p> <pre><code>import { useMetricsUpdated } from '@/lib/graphql/hooks';\n\nfunction Dashboard() {\n  useMetricsUpdated({\n    orgId: 'my-org',\n    onUpdate: (update) =&gt; {\n      console.log(`Metrics updated for ${update.day}`);\n      refetchData();\n    },\n  });\n\n  // ...\n}\n</code></pre>"},{"location":"appendix/web-graphql-client/#usetaskstatus","title":"useTaskStatus","text":"<p>Monitor background task progress:</p> <pre><code>import { useTaskStatus } from '@/lib/graphql/hooks';\n\nfunction TaskProgress({ taskId }) {\n  const { data } = useTaskStatus({\n    taskId,\n    onUpdate: (status) =&gt; {\n      if (status.status === 'completed') {\n        showSuccess('Task complete!');\n      }\n    },\n  });\n\n  return (\n    &lt;ProgressBar value={data?.progress ?? 0} /&gt;\n  );\n}\n</code></pre>"},{"location":"appendix/web-graphql-client/#validation","title":"Validation","text":"<p>Use Zod schemas to validate responses:</p> <pre><code>import { validateAnalyticsResponse } from '@/lib/graphql/validate';\nimport { AnalyticsResultSchema } from '@/lib/graphql/schemas';\n\n// Validate raw data\nconst result = validateAnalyticsResponse(rawData);\nif (!result.success) {\n  console.error('Validation failed:', result.error);\n}\n\n// Or validate with schema directly\nimport { safeParse } from '@/lib/graphql/validate';\nconst data = safeParse(AnalyticsResultSchema, rawData);\n</code></pre>"},{"location":"appendix/web-graphql-client/#direct-client-usage","title":"Direct Client Usage","text":"<p>For non-React contexts or custom logic:</p> <pre><code>import { getUrqlClient } from '@/lib/graphql/urqlClient';\nimport { INVESTMENT_BREAKDOWN_QUERY } from '@/lib/graphql/queries';\n\nasync function fetchData() {\n  const client = getUrqlClient('my-org');\n\n  const result = await client.query(INVESTMENT_BREAKDOWN_QUERY, {\n    orgId: 'my-org',\n    batch: { ... },\n  });\n\n  return result.data;\n}\n</code></pre>"},{"location":"appendix/web-graphql-client/#error-handling","title":"Error Handling","text":"<pre><code>function MyComponent() {\n  const { data, loading, error } = useAnalytics({ ... });\n\n  if (error) {\n    // Check for specific error types\n    if (error.message.includes('COST_LIMIT_EXCEEDED')) {\n      return &lt;Error&gt;Query too expensive. Try reducing the date range.&lt;/Error&gt;;\n    }\n    return &lt;Error&gt;Something went wrong: {error.message}&lt;/Error&gt;;\n  }\n\n  // ...\n}\n</code></pre>"},{"location":"appendix/web-graphql-client/#best-practices","title":"Best Practices","text":"<ol> <li>Use hooks at component level - Let urql handle caching and deduplication</li> <li>Validate responses - Use Zod schemas in development to catch API changes</li> <li>Handle loading states - Always show feedback during data fetching</li> <li>Unsubscribe on unmount - Hooks handle this automatically</li> <li>Prefer persisted queries - For production, use server-registered queries</li> </ol>"},{"location":"appendix/web-graphql-investment/","title":"GraphQL Analytics for Investment View","text":"<p>This document describes how to use the GraphQL analytics endpoint for the Investment view, which is now available behind a feature flag.</p>"},{"location":"appendix/web-graphql-investment/#feature-flag","title":"Feature Flag","text":"<p>Set the environment variable to enable GraphQL transport for Investment view data:</p> <pre><code># In .env.local or environment\nNEXT_PUBLIC_USE_GRAPHQL_ANALYTICS=true\n</code></pre> <p>For compiled builds (<code>next start</code>), the runtime flag is written to <code>public/runtime-config.js</code> at startup (see <code>scripts/write-runtime-config.mjs</code>). That file mirrors all <code>NEXT_PUBLIC_*</code> values under <code>publicEnv</code> (for example <code>NEXT_PUBLIC_DOCS_URL</code> and <code>NEXT_PUBLIC_DEV_HEALTH_TEST_MODE</code>). You can also set <code>USE_GRAPHQL_ANALYTICS=true</code> for a runtime-only flag.</p> <p>When enabled, the following Investment view API calls switch from REST to GraphQL:</p> <ul> <li><code>getInvestment()</code> - theme/subcategory distributions</li> <li><code>getInvestmentFlow()</code> - Sankey flow (team \u2192 category \u2192 repo)</li> <li><code>getInvestmentRepoTeamFlow()</code> - Sankey flow (repo \u2192 team)</li> </ul> <p>When disabled (default), all calls continue to use the existing REST endpoints.</p>"},{"location":"appendix/web-graphql-investment/#running-graphql-locally","title":"Running GraphQL Locally","text":""},{"location":"appendix/web-graphql-investment/#backend-dev-health-ops","title":"Backend (dev-health-ops)","text":"<p>The GraphQL endpoint is already mounted at <code>/graphql</code>:</p> <pre><code>cd dev-health-ops\n# Start the API server\npython cli.py api --port 8000 --db \"$DATABASE_URI\"\n\n# GraphiQL is available at http://localhost:8000/graphql\n</code></pre>"},{"location":"appendix/web-graphql-investment/#frontend-dev-health-web","title":"Frontend (dev-health-web)","text":"<pre><code>cd dev-health-web\n# Enable GraphQL analytics\nexport NEXT_PUBLIC_USE_GRAPHQL_ANALYTICS=true\nnpm run dev\n</code></pre> <p>For a compiled build:</p> <pre><code>npm run build\nNEXT_PUBLIC_USE_GRAPHQL_ANALYTICS=true npm start\n</code></pre> <p><code>npm start</code> runs <code>scripts/write-runtime-config.mjs</code> to refresh <code>public/runtime-config.js</code>.</p>"},{"location":"appendix/web-graphql-investment/#investment-query-examples","title":"Investment Query Examples","text":""},{"location":"appendix/web-graphql-investment/#1-investment-breakdown-themesubcategory-distribution","title":"1. Investment Breakdown (Theme/Subcategory Distribution)","text":"<pre><code>query InvestmentBreakdown($orgId: String!, $batch: AnalyticsRequestInput!) {\n  analytics(orgId: $orgId, batch: $batch) {\n    breakdowns {\n      dimension\n      measure\n      items {\n        key\n        value\n      }\n    }\n  }\n}\n</code></pre> <p>Variables:</p> <pre><code>{\n  \"orgId\": \"my-org\",\n  \"batch\": {\n    \"breakdowns\": [\n      {\n        \"dimension\": \"THEME\",\n        \"measure\": \"CHURN_LOC\",\n        \"dateRange\": { \"startDate\": \"2025-01-01\", \"endDate\": \"2025-01-31\" },\n        \"topN\": 50\n      },\n      {\n        \"dimension\": \"SUBCATEGORY\",\n        \"measure\": \"CHURN_LOC\",\n        \"dateRange\": { \"startDate\": \"2025-01-01\", \"endDate\": \"2025-01-31\" },\n        \"topN\": 100\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"appendix/web-graphql-investment/#2-investment-sankey-flow","title":"2. Investment Sankey Flow","text":"<pre><code>query InvestmentSankey($orgId: String!, $batch: AnalyticsRequestInput!) {\n  analytics(orgId: $orgId, batch: $batch) {\n    sankey {\n      nodes { id label dimension value }\n      edges { source target value }\n    }\n  }\n}\n</code></pre> <p>Variables:</p> <pre><code>{\n  \"orgId\": \"my-org\",\n  \"batch\": {\n    \"sankey\": {\n      \"path\": [\"TEAM\", \"THEME\", \"REPO\"],\n      \"measure\": \"CHURN_LOC\",\n      \"dateRange\": { \"startDate\": \"2025-01-01\", \"endDate\": \"2025-01-31\" },\n      \"maxNodes\": 50,\n      \"maxEdges\": 200\n    }\n  }\n}\n</code></pre>"},{"location":"appendix/web-graphql-investment/#persisted-query-ids","title":"Persisted Query IDs","text":"<p>For reduced query string churn, use these persisted query IDs:</p> ID Description <code>investment-breakdown</code> Theme and subcategory breakdowns <code>investment-sankey</code> Sankey flow data <p>Example:</p> <pre><code>curl -X POST /graphql \\\n  -H \"X-Persisted-Query-Id: investment-breakdown\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"variables\": {\"orgId\": \"my-org\", \"batch\": {...}}}'\n</code></pre>"},{"location":"appendix/web-graphql-investment/#response-shape-mapping","title":"Response Shape Mapping","text":"<p>The GraphQL fetchers adapt responses to match the existing REST shapes:</p>"},{"location":"appendix/web-graphql-investment/#breakdown-investmentresponse","title":"Breakdown \u2192 InvestmentResponse","text":"<pre><code>// GraphQL response\n{ analytics: { breakdowns: [{ dimension: \"theme\", items: [...] }] } }\n\n// Adapted to REST shape\n{ theme_distribution: {...}, subcategory_distribution: {...} }\n</code></pre>"},{"location":"appendix/web-graphql-investment/#sankey-sankeyresponse","title":"Sankey \u2192 SankeyResponse","text":"<pre><code>// GraphQL response\n{ analytics: { sankey: { nodes: [...], edges: [...] } } }\n\n// Adapted to REST shape\n{ mode: \"investment\", nodes: [...], links: [...] }\n</code></pre>"},{"location":"appendix/web-graphql-investment/#testing","title":"Testing","text":"<p>With the flag enabled, the Investment page should render identically to REST mode:</p> <ul> <li>Same charts (treemap, sunburst, Sankey)</li> <li>Same tooltip wording</li> <li>Same filter behavior</li> <li>Same Sankey structure and values (within expected rounding)</li> </ul> <p>With the flag disabled, everything works exactly as before using REST endpoints.</p>"},{"location":"appendix/web-graphql-investment/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Frontend (dev-health-web)               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  src/lib/api.ts                                              \u2502\n\u2502    \u2193 checks runtime GraphQL flag                             \u2502\n\u2502    \u251c\u2500 false \u2192 REST endpoints (/api/v1/investment/*)          \u2502\n\u2502    \u2514\u2500 true  \u2192 GraphQL fetchers                               \u2502\n\u2502                                                              \u2502\n\u2502  scripts/write-runtime-config.mjs                            \u2502\n\u2502    \u2193 writes public/runtime-config.js at start                \u2502\n\u2502                                                              \u2502\n\u2502  src/lib/graphql/                                            \u2502\n\u2502    \u251c\u2500 client.ts          # GraphQL request wrapper           \u2502\n\u2502    \u251c\u2500 types.ts           # TypeScript types                  \u2502\n\u2502    \u251c\u2500 queries.ts         # Query strings                     \u2502\n\u2502    \u2514\u2500 investmentFetchers.ts  # Adapters to REST shapes       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Backend (dev-health-ops)                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  /graphql (POST)                                             \u2502\n\u2502    \u251c\u2500 catalog(orgId, dimension?) \u2192 dimension values          \u2502\n\u2502    \u2514\u2500 analytics(orgId, batch) \u2192 timeseries, breakdowns, sankey\u2502\n\u2502                                                              \u2502\n\u2502  api/graphql/                                                \u2502\n\u2502    \u251c\u2500 resolvers/analytics.py   # Query execution             \u2502\n\u2502    \u251c\u2500 sql/compiler.py          # SQL generation              \u2502\n\u2502    \u2514\u2500 persisted_queries.json   # Cached queries              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"appendix/web-visualizations/","title":"Visualizations Guide","text":""},{"location":"appendix/web-visualizations/#heatmaps-vs-line-charts","title":"Heatmaps vs line charts","text":"<p>Use heatmaps when you need to detect patterns that averages hide:</p> <ul> <li>Cyclical congestion (hour-of-day x weekday) where work accumulates or stalls.</li> <li>Fragmentation across repos, modules, or systems where attention is split.</li> <li>Risk build-up across files or services where hotspots intensify over time.</li> </ul> <p>Use line charts when you need a single trend line or KPI trajectory:</p> <ul> <li>Clear directional movement over time (e.g., median cycle time).</li> <li>Simple comparisons between time windows without multi-dimensional breakdowns.</li> <li>Executive-level summaries where detailed context is unnecessary.</li> </ul> <p>Heatmaps should always link to evidence for the selected bucket. If you cannot trace a cell to real artifacts, use a line chart instead.</p>"},{"location":"appendix/web-visualizations/#quadrants-state-and-trajectory","title":"Quadrants: state and trajectory","text":"<p>Quadrants classify system modes without ranking or judging performance. They help answer:</p> <ul> <li>What mode is this team or repo operating in right now?</li> <li>Which direction are we moving across recent windows?</li> <li>Where do effort and outcomes drift out of alignment?</li> </ul> <p>Use quadrants when you need to reason about system behavior rather than a single trend line. Quadrants are hypothesis starters; they should always link to evidence and follow up with heatmaps or flame diagrams for diagnosis.</p>"},{"location":"appendix/web-visualizations/#required-quadrants","title":"Required quadrants","text":"<p>1) Churn \u00d7 Throughput    - Detect thrashing vs stable progress.    - High churn + high throughput can indicate deliberate refactors.    - Low churn + low throughput can signal constraints or hidden blocks.</p> <p>2) Cycle Time \u00d7 Throughput    - Rising cycle time with stable throughput indicates coordination debt.    - Slow-and-steady systems can mask accumulating queues.</p> <p>3) WIP \u00d7 Throughput    - Flat throughput with rising WIP is an early saturation warning.    - Use to spot local optimization before teams feel pain.</p> <p>4) Review Load \u00d7 Review Latency    - Identify structural review bottlenecks and reviewer concentration.    - Pair with flame diagrams to see handoff loops.</p>"},{"location":"appendix/web-visualizations/#zone-maps-experimental","title":"Zone maps (experimental)","text":"<p>Zone maps are optional overlays that highlight fuzzy, overlapping regions with common constraint patterns derived from observed metrics. Zone maps are heuristics. If they feel prescriptive, turn them off.</p>"},{"location":"appendix/web-visualizations/#scope-guardrails","title":"Scope guardrails","text":"<ul> <li>Org/team/repo views show teams, repos, or services only.</li> <li>Individual views show one person across time windows, optionally as a path.</li> <li>Never label quadrants as good/bad, high/low performer, or rankings.</li> </ul>"},{"location":"appendix/web-visualizations/#flame-diagrams-vs-cycle-time","title":"Flame diagrams vs cycle time","text":"<p>Use flame diagrams when you need to understand where time was spent in a single work item:</p> <ul> <li>Visualize waiting, rework, or handoffs inside an issue, PR, or deployment.</li> <li>Diagnose whether delays were sequential or overlapped.</li> <li>Identify rework loops and review churn hidden in aggregate cycle time.</li> </ul> <p>Use cycle time metrics when you want a fleet-wide signal:</p> <ul> <li>Track aggregate performance across a team or system.</li> <li>Monitor improvement or regression over time.</li> <li>Compare before/after windows at a high level.</li> </ul> <p>Flame diagrams are diagnostic and item-scoped; cycle time is strategic and portfolio-scoped. Use flames to explain a single outlier, and cycle time to prove the trend.</p>"},{"location":"appendix/web-visualizations/#guardrails","title":"Guardrails","text":"<ul> <li>No person-to-person rankings or comparisons in heatmaps or flame diagrams.</li> <li>Individual views are single-person only, for reflection and coaching.</li> <li>Heatmap cells should remain explorable down to evidence artifacts.</li> <li>Quadrants must show raw values only, no percentiles or scoring.</li> </ul>"},{"location":"appendix/web-visualizations/#relationship-map","title":"Relationship map","text":"<p>Quadrants answer: \u201cWhat state are we in?\u201d</p> <p>Heatmaps answer: \u201cWhere and when is pressure accumulating?\u201d</p> <p>Flame diagrams answer: \u201cWhy did this specific item take so long?\u201d</p> <p>Use them together: Quadrant \u2192 heatmap slice \u2192 flame diagram \u2192 evidence.</p>"},{"location":"appendix/agent-instructions/","title":"Agent Instructions \u2014 Deep Dive Documentation","text":"<p>This directory contains detailed documentation for AI agents and developers working on the Dev Health platform.</p> <p>Start here: <code>/AGENTS.md</code> \u2014 Canonical summarized agent lookup</p>"},{"location":"appendix/agent-instructions/#directory-structure","title":"Directory Structure","text":"<pre><code>agent-instructions/\n\u251c\u2500\u2500 architecture/           # System architecture documentation\n\u2502   \u251c\u2500\u2500 data-pipeline.md    # Backend pipeline (connectors \u2192 sinks)\n\u2502   \u2514\u2500\u2500 frontend-architecture.md  # Next.js frontend patterns\n\u2502\n\u251c\u2500\u2500 product/                # Product specifications\n\u2502   \u251c\u2500\u2500 investment-view.md  # Investment View product spec\n\u2502   \u2514\u2500\u2500 work-graph.md       # Work Graph contract\n\u2502\n\u251c\u2500\u2500 metrics/                # Metrics system documentation\n\u2502   \u251c\u2500\u2500 canonical-metrics.md    # Metric registry and API mapping\n\u2502   \u2514\u2500\u2500 metric-calculations.md  # Calculation formulas and edge cases\n\u2502\n\u251c\u2500\u2500 visualizations/         # Visualization guidelines\n\u2502   \u2514\u2500\u2500 visualization-patterns.md  # Chart selection, quadrants, guardrails\n\u2502\n\u251c\u2500\u2500 connectors/             # Data connector documentation\n\u2502   \u251c\u2500\u2500 github-gitlab.md    # GitHub/GitLab connectors\n\u2502   \u251c\u2500\u2500 jira.md             # Jira connector\n\u2502   \u2514\u2500\u2500 atlassian-graphql.md    # Atlassian GraphQL Gateway client\n\u2502\n\u251c\u2500\u2500 llm/                    # LLM usage specifications\n\u2502   \u2514\u2500\u2500 categorization-contract.md  # Compute-time and UX-time LLM rules\n\u2502\n\u2514\u2500\u2500 workflows/              # Developer workflows\n    \u2514\u2500\u2500 cli-reference.md    # CLI command reference\n</code></pre>"},{"location":"appendix/agent-instructions/#document-index","title":"Document Index","text":""},{"location":"appendix/agent-instructions/#architecture","title":"Architecture","text":"Document Description data-pipeline.md Backend pipeline stages, storage backends, sink interface frontend-architecture.md Next.js structure, component patterns, testing"},{"location":"appendix/agent-instructions/#product","title":"Product","text":"Document Description investment-view.md Investment View PRD, canonical themes, data model work-graph.md Work Graph contract, WorkUnit definition, materialization rules"},{"location":"appendix/agent-instructions/#metrics","title":"Metrics","text":"Document Description canonical-metrics.md Metric registry, API mapping, database schema metric-calculations.md Formulas, bucketing, edge cases, null handling"},{"location":"appendix/agent-instructions/#visualizations","title":"Visualizations","text":"Document Description visualization-patterns.md Chart selection, quadrants, guardrails, drill-down patterns"},{"location":"appendix/agent-instructions/#connectors","title":"Connectors","text":"Document Description github-gitlab.md Git provider setup, auth, batch processing jira.md Jira Cloud setup, status normalization, team mapping atlassian-graphql.md AGG client architecture, rate limiting, schema evolution"},{"location":"appendix/agent-instructions/#llm","title":"LLM","text":"Document Description categorization-contract.md Compute-time/UX-time rules, prompts, language constraints"},{"location":"appendix/agent-instructions/#workflows","title":"Workflows","text":"Document Description cli-reference.md Complete CLI command reference"},{"location":"appendix/agent-instructions/#how-to-use-this-documentation","title":"How to Use This Documentation","text":""},{"location":"appendix/agent-instructions/#for-ai-agents","title":"For AI Agents","text":"<ol> <li>Start with <code>/AGENTS.md</code> for the summarized contract</li> <li>Deep dive into specific topics in this directory as needed</li> <li>Reference during implementation for constraints and patterns</li> </ol>"},{"location":"appendix/agent-instructions/#for-developers","title":"For Developers","text":"<ol> <li>Architecture overview \u2192 <code>architecture/</code> directory</li> <li>Metrics implementation \u2192 <code>metrics/</code> directory</li> <li>Connector setup \u2192 <code>connectors/</code> directory</li> </ol>"},{"location":"appendix/agent-instructions/#navigation-pattern","title":"Navigation Pattern","text":"<pre><code>/AGENTS.md (summary)\n    \u2193\ndocs/agent-instructions/&lt;topic&gt;/&lt;document&gt;.md (deep dive)\n    \u2193\nSub-project code and tests\n</code></pre>"},{"location":"appendix/agent-instructions/#relationship-to-sub-project-agentsmd","title":"Relationship to Sub-Project AGENTS.md","text":"<p>Each sub-project has its own <code>AGENTS.md</code> with project-specific details:</p> <ul> <li><code>dev-health-ops/AGENTS.md</code> \u2014 Backend specifics</li> <li><code>dev-health-web/AGENTS.md</code> \u2014 Frontend specifics</li> <li><code>atlassian/AGENTS.md</code> \u2014 GraphQL client specifics</li> </ul> <p>The root <code>/AGENTS.md</code> consolidates cross-cutting concerns and serves as the canonical starting point.</p>"},{"location":"appendix/agent-instructions/#contributing","title":"Contributing","text":"<p>When adding documentation:</p> <ol> <li>Place in appropriate category directory</li> <li>Update this README index</li> <li>Link from root <code>/AGENTS.md</code> if broadly applicable</li> <li>Follow existing formatting patterns</li> </ol>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/","title":"Data Pipeline Architecture (dev-health-ops)","text":""},{"location":"appendix/agent-instructions/architecture/data-pipeline/#pipeline-overview","title":"Pipeline Overview","text":"<p>The dev-health-ops backend follows a strict unidirectional pipeline:</p> <pre><code>Connectors \u2192 Processors \u2192 Sinks \u2192 Metrics \u2192 Visualization\n</code></pre> <p>Each stage has clear responsibilities. Do not collapse layers or bypass stages.</p>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#1-connectors-connectors","title":"1. Connectors (<code>connectors/</code>)","text":"<p>Purpose: Fetch raw data from external providers.</p>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#supported-providers","title":"Supported Providers","text":"Provider Module Sync Targets Local Git <code>connectors/local.py</code> git, blame GitHub <code>connectors/github.py</code> git, prs, cicd, deployments, incidents, work-items GitLab <code>connectors/gitlab.py</code> git, prs, cicd, deployments, incidents, work-items Jira <code>connectors/jira.py</code> work-items Synthetic <code>connectors/synthetic.py</code> fixtures generation"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#rules","title":"Rules","text":"<ul> <li>Network I/O should be async and batch-friendly</li> <li>Respect rate limits and backoff mechanisms</li> <li>Return raw provider data (minimal transformation)</li> <li>Handle pagination completely (never assume single page)</li> </ul>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#2-processors-processors","title":"2. Processors (<code>processors/</code>)","text":"<p>Purpose: Normalize and transform connector outputs into internal models.</p>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#key-processor","title":"Key Processor","text":"<ul> <li><code>processors/local.py</code> \u2014 Primary processor for local git data</li> </ul>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#responsibilities","title":"Responsibilities","text":"<ul> <li>Map provider-specific fields to unified models</li> <li>Normalize timestamps to UTC</li> <li>Resolve identities across providers</li> <li>Enrich with computed fields (e.g., commit size buckets)</li> </ul>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#rules_1","title":"Rules","text":"<ul> <li>No network I/O</li> <li>No persistence logic</li> <li>Transform only, no business decisions</li> <li>Output must match models in <code>models/</code></li> </ul>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#3-storage-sinks-metricssinks","title":"3. Storage / Sinks (<code>metrics/sinks/</code>)","text":"<p>Purpose: Persist processed data to storage backends.</p>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#supported-backends","title":"Supported Backends","text":"Backend Connection Use Case PostgreSQL <code>postgresql+asyncpg://</code> Relational, migrations ClickHouse <code>clickhouse://</code> Analytics queries MongoDB <code>mongodb://</code> Document storage SQLite <code>sqlite+aiosqlite://</code> Local dev/test"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#rules_2","title":"Rules","text":"<ul> <li>No file exports. No debug dumps. No JSON/YAML output paths.</li> <li>All persistence goes through sink modules</li> <li>Backend selection via <code>--db</code> flag or <code>DATABASE_URI</code></li> <li>Secondary sink via <code>SECONDARY_DATABASE_URI</code> with <code>sink='both'</code></li> </ul>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#sink-interface","title":"Sink Interface","text":"<pre><code>async def write_batch(records: List[Model], session: AsyncSession) -&gt; int:\n    \"\"\"Write a batch of records. Returns count written.\"\"\"\n</code></pre>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#4-metrics-metrics","title":"4. Metrics (<code>metrics/</code>)","text":"<p>Purpose: Compute higher-level rollups and aggregates from persisted data.</p>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#key-metric-tables","title":"Key Metric Tables","text":"Table Key Content <code>repo_metrics_daily</code> <code>(repo_id, day)</code> Commits, LOC, PR cycle time <code>user_metrics_daily</code> <code>(repo_id, author_email, day)</code> User activity <code>work_item_metrics_daily</code> <code>(day, provider, work_scope_id, team_id)</code> Throughput, WIP, cycle time <code>team_metrics_daily</code> <code>(team_id, day)</code> After-hours, weekend ratios"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#computation-model","title":"Computation Model","text":"<ul> <li>Metrics are append-only with <code>computed_at</code> versioning</li> <li>Use <code>argMax(&lt;metric&gt;, computed_at)</code> to get latest value</li> <li>Re-computation is safe (idempotent via compound keys)</li> </ul>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#5-visualization-grafana-dev-health-web","title":"5. Visualization (Grafana + dev-health-web)","text":"<p>Purpose: Render persisted data for exploration.</p>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#grafana","title":"Grafana","text":"<ul> <li>Dashboards provisioned via <code>grafana/</code> directory</li> <li>Query conventions:</li> <li>Prefer table format with stable time ordering</li> <li>Handle <code>team_id</code> null/empty normalization</li> <li>Avoid ClickHouse <code>WITH name = expr</code> syntax; use <code>WITH ... AS</code></li> </ul>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#dev-health-web","title":"dev-health-web","text":"<ul> <li>Visualization-only \u2014 Must not become source of truth</li> <li>Consumes data via GraphQL API from dev-health-ops</li> <li>No category recomputation at UX time</li> </ul>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#storage-schema-highlights","title":"Storage Schema Highlights","text":""},{"location":"appendix/agent-instructions/architecture/data-pipeline/#clickhouse-tables","title":"ClickHouse Tables","text":"<p>Tables are <code>MergeTree</code> partitioned by <code>toYYYYMM(day)</code>:</p> <pre><code>CREATE TABLE repo_metrics_daily (\n    repo_id UUID,\n    day Date,\n    computed_at DateTime,\n    -- metrics columns\n) ENGINE = MergeTree()\nPARTITION BY toYYYYMM(day)\nORDER BY (repo_id, day);\n</code></pre>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#postgresql-tables","title":"PostgreSQL Tables","text":"<p>Managed via Alembic migrations in <code>alembic/</code>:</p> <pre><code># Generate migration\nalembic revision --autogenerate -m \"description\"\n\n# Apply migrations\nalembic upgrade head\n</code></pre>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#environment-variables","title":"Environment Variables","text":"Variable Purpose Required <code>DATABASE_URI</code> Primary database connection Yes <code>SECONDARY_DATABASE_URI</code> Secondary sink (with <code>--sink both</code>) No <code>DB_ECHO</code> Enable SQL logging No <code>BATCH_SIZE</code> Records per batch insert No (default: 100) <code>MAX_WORKERS</code> Parallel workers No (default: 4)"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#adding-new-pipeline-components","title":"Adding New Pipeline Components","text":""},{"location":"appendix/agent-instructions/architecture/data-pipeline/#new-connector","title":"New Connector","text":"<ol> <li>Create <code>connectors/newprovider.py</code></li> <li>Implement async fetch methods</li> <li>Register in <code>connectors/__init__.py</code></li> <li>Add CLI integration in <code>cli.py</code></li> </ol>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#new-metric","title":"New Metric","text":"<ol> <li>Define model in <code>models/</code></li> <li>Add sink in <code>metrics/sinks/</code></li> <li>Implement computation in <code>metrics/</code></li> <li>Create Alembic migration if using Postgres</li> <li>Update Grafana dashboards</li> </ol>"},{"location":"appendix/agent-instructions/architecture/data-pipeline/#rules-when-modifying","title":"Rules When Modifying","text":"<ul> <li>Never bypass sinks for persistence</li> <li>Always handle pagination</li> <li>Add tests under <code>tests/</code></li> <li>Respect existing async patterns</li> </ul>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/","title":"Frontend Architecture (dev-health-web)","text":""},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#technology-stack","title":"Technology Stack","text":"<ul> <li>Framework: Next.js (App Router)</li> <li>Language: TypeScript</li> <li>Styling: Tailwind CSS</li> <li>Testing: Vitest (unit), Playwright (e2e)</li> <li>Data: GraphQL client to dev-health-ops API</li> </ul>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#directory-structure","title":"Directory Structure","text":"<pre><code>src/\n\u251c\u2500\u2500 app/           # Next.js pages and routes\n\u251c\u2500\u2500 components/    # Reusable UI components\n\u2502   \u251c\u2500\u2500 charts/    # Visualization components\n\u2502   \u251c\u2500\u2500 filters/   # Filter controls\n\u2502   \u2514\u2500\u2500 navigation/# Nav components\n\u251c\u2500\u2500 lib/           # Data transforms, mappers, helpers\n\u251c\u2500\u2500 data/          # Sample data for demos/tests\n\u2514\u2500\u2500 types/         # TypeScript type definitions\n\ntests/             # Playwright e2e tests\ntest-results/      # Test artifacts\n</code></pre>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#key-architectural-patterns","title":"Key Architectural Patterns","text":""},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#1-server-vs-client-components","title":"1. Server vs Client Components","text":"<ul> <li>Server Components: Default for pages, data fetching colocated with page</li> <li>Client Components: Interactive elements, charts, filters</li> <li>Mark client components with <code>'use client'</code> directive</li> </ul>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#2-data-flow","title":"2. Data Flow","text":"<pre><code>GraphQL API \u2192 src/lib transforms \u2192 Chart components \u2192 Rendered UI\n</code></pre> <ul> <li>Sample data in <code>src/data/</code> for demos and unit tests</li> <li>Real data from dev-health-ops GraphQL API</li> <li>Test mode uses <code>DEV_HEALTH_TEST_MODE</code> env var</li> </ul>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#3-type-safety","title":"3. Type Safety","text":"<ul> <li>All components use TypeScript</li> <li>Shared types in <code>src/types/</code></li> <li>Filter types must use union types (e.g., <code>\"repo\" | \"org\" | \"team\"</code>)</li> </ul>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#development-conventions","title":"Development Conventions","text":""},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#component-guidelines","title":"Component Guidelines","text":"<pre><code>// Good: Typed props, clear interface\ninterface MetricCardProps {\n  metric: Metric;\n  scope: MetricFilter;\n  onDrillDown?: (id: string) =&gt; void;\n}\n\nexport function MetricCard({ metric, scope, onDrillDown }: MetricCardProps) {\n  // Implementation\n}\n</code></pre>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#state-management","title":"State Management","text":"<ul> <li>Avoid synchronous <code>setState</code> in effects</li> <li>Derive sample data via memo + computed loading</li> <li>Use React Server Components for data fetching when possible</li> </ul>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#eslint-rules","title":"ESLint Rules","text":"<p>Active rules to follow: - <code>react-hooks/set-state-in-effect</code> \u2014 No sync setState in effects - <code>react-hooks/exhaustive-deps</code> \u2014 Include all dependencies</p>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#testing-strategy","title":"Testing Strategy","text":""},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#unit-tests-vitest","title":"Unit Tests (Vitest)","text":"<ul> <li>Located next to modules: <code>src/lib/__tests__/</code></li> <li>Test transforms, mappers, helpers independently</li> </ul> <pre><code>// src/lib/__tests__/transforms.test.ts\nimport { transformMetrics } from '../transforms';\n\ndescribe('transformMetrics', () =&gt; {\n  it('converts hours to days', () =&gt; {\n    expect(transformMetrics({ hours: 24 })).toEqual({ days: 1 });\n  });\n});\n</code></pre>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#e2e-tests-playwright","title":"E2E Tests (Playwright)","text":"<ul> <li>Located in <code>tests/</code> directory</li> <li>Use test mode environment variables</li> <li>Include visual regression when possible</li> </ul> <pre><code>// tests/dashboard.spec.ts\ntest('dashboard loads metrics', async ({ page }) =&gt; {\n  await page.goto('/dashboard');\n  await expect(page.getByTestId('metric-card')).toBeVisible();\n});\n</code></pre>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#test-environment","title":"Test Environment","text":"<p>Playwright config sets these env vars: - <code>DEV_HEALTH_TEST_MODE=1</code> - <code>NEXT_PUBLIC_DEV_HEALTH_TEST_MODE=1</code></p> <p>Components should support sample data when these are set.</p>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#graphql-client","title":"GraphQL Client","text":""},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#setup","title":"Setup","text":"<pre><code>// src/lib/graphql-client.ts\nimport { createClient } from '@urql/core';\n\nexport const client = createClient({\n  url: process.env.NEXT_PUBLIC_GRAPHQL_URL || 'http://localhost:8000/graphql',\n});\n</code></pre>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#query-patterns","title":"Query Patterns","text":"<pre><code>// Fetch with typing\nconst result = await client.query&lt;MetricsQuery&gt;(METRICS_QUERY, {\n  scope: { type: 'team', id: teamId },\n  range: { days: 14 },\n});\n</code></pre> <p>See <code>docs/graphql-client.md</code> for full client documentation.</p>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#visualization-integration","title":"Visualization Integration","text":""},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#chart-components-location","title":"Chart Components Location","text":"<ul> <li><code>src/components/charts/</code> \u2014 All chart implementations</li> <li>Use visualization patterns from <code>docs/visualizations.md</code></li> </ul>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#data-transform-pattern","title":"Data Transform Pattern","text":"<pre><code>// src/lib/transforms/metric-transforms.ts\nexport function toChartData(metrics: Metric[]): ChartDataPoint[] {\n  return metrics.map(m =&gt; ({\n    x: m.day,\n    y: m.value,\n    label: m.label,\n  }));\n}\n</code></pre>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#pr-review-guidelines","title":"PR &amp; Review Guidelines","text":"<ul> <li>Use descriptive PR titles referencing related tests</li> <li>Keep changes scoped to one feature or bugfix</li> <li>Include screenshot or Playwright trace for visual changes</li> <li>Run <code>npm run lint</code> and <code>npm run test</code> before submitting</li> </ul>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#common-gotchas","title":"Common Gotchas","text":""},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#1-filter-type-unions","title":"1. Filter Type Unions","text":"<pre><code>// Wrong: string type loses type safety\nconst scope: { level: string } = { level: 'team' };\n\n// Correct: Use union type\nconst scope: { level: 'repo' | 'org' | 'team' } = { level: 'team' };\n</code></pre>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#2-test-mode-detection","title":"2. Test Mode Detection","text":"<pre><code>// Check for test mode in components\nconst isTestMode = process.env.NEXT_PUBLIC_DEV_HEALTH_TEST_MODE === '1';\nconst data = isTestMode ? sampleData : await fetchRealData();\n</code></pre>"},{"location":"appendix/agent-instructions/architecture/frontend-architecture/#3-hydration-mismatches","title":"3. Hydration Mismatches","text":"<ul> <li>Server and client must render same initial content</li> <li>Use <code>useEffect</code> for client-only logic</li> <li>Check <code>typeof window !== 'undefined'</code> when needed</li> </ul>"},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/","title":"Atlassian GraphQL Gateway (AGG) Client","text":""},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#overview","title":"Overview","text":"<p>The <code>atlassian/</code> directory implements a production-grade Atlassian GraphQL Gateway client for Jira and related Atlassian products.</p> <p>Core principle: Schema-driven, not guess-driven.</p>"},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#architecture","title":"Architecture","text":""},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#1-api-layer-fast-moving-generated","title":"1. API Layer (Fast-moving, Generated)","text":"<p>Purpose: Represent Atlassian GraphQL schema as it exists today.</p> <p>Locations: - <code>python/atlassian/graph/gen/</code> - <code>python/atlassian/rest/gen/</code> - <code>go/atlassian/graph/gen/</code> - <code>go/atlassian/rest/gen/</code></p> <p>Source: <code>graphql/schema.introspection.json</code></p> <p>Rules: - Never hand-edit generated files - Regeneration must be deterministic - Missing schema fields \u2192 generator failure</p>"},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#2-transport-client-layer-stable-defensive","title":"2. Transport &amp; Client Layer (Stable, Defensive)","text":"<p>Purpose: Safely execute GraphQL operations.</p> <p>Locations: - <code>python/atlassian/graph/client.py</code> - <code>python/atlassian/rest/client.py</code> - <code>go/atlassian/graph/client.go</code> - <code>go/atlassian/rest/client.go</code></p> <p>Responsibilities: - Authentication - Rate limiting - Retries (429 only) - Logging - Strict vs non-strict error handling - Beta headers (<code>X-ExperimentalApi</code>)</p>"},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#3-canonical-analytics-layer-slow-moving-stable","title":"3. Canonical Analytics Layer (Slow-moving, Stable)","text":"<p>Purpose: Define data model for developer health metrics.</p> <p>Source of truth: <code>openapi/jira-developer-health.canonical.openapi.yaml</code></p> <p>Characteristics: - API-agnostic - Versioned by this repo - Backward-compatible - Designed for analytics, not transport</p> <p>Examples: - JiraUser - JiraProject - JiraIssue - JiraChangelogEvent - JiraWorklog</p> <p>Rules: - Must NOT leak API shapes (edges, nodes, cursors) - IDs are strings - Timestamps are RFC3339 - Prefer optional fields over brittle requirements</p>"},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#4-mapping-layer-explicit-validated","title":"4. Mapping Layer (Explicit, Validated)","text":"<p>Purpose: Convert API models \u2192 canonical analytics models.</p> <p>Locations: - <code>python/atlassian/graph/mappers/</code> - <code>python/atlassian/rest/mappers/</code> - <code>go/atlassian/graph/mappers/</code> - <code>go/atlassian/rest/mappers/</code></p> <p>Rules: - Required canonical fields MUST be validated - Missing required data \u2192 explicit error - No implicit defaults for semantic fields - Positive conditionals preferred - No business logic \u2014 mapping only</p>"},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#rate-limiting-non-negotiable","title":"Rate Limiting (Non-Negotiable)","text":"<p>Atlassian AGG uses cost-based rate limiting:</p> Parameter Value Default budget 10,000 points/minute Enforcement HTTP 429 <code>Retry-After</code> format TIMESTAMP (not seconds)"},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#required-behavior","title":"Required Behavior","text":"<pre><code>Retry ONLY on HTTP 429\nCompute wait = retry_after_timestamp - now\nCap wait using MaxWait\nAfter MaxRetries429 \u2192 fail with RateLimitError\nDo NOT retry 4xx (except 429)\nDo NOT retry &gt;= 500\n</code></pre> <p>Violating this will get your code reverted.</p>"},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#pagination-rules","title":"Pagination Rules","text":"<ul> <li>Assume every connection paginates</li> <li>Support <code>pageInfo.hasNextPage</code> and <code>pageInfo.endCursor</code></li> <li>Handle nested pagination (e.g., projects \u2192 opsgenie teams)</li> <li>Never assume single page</li> <li>Never hardcode page sizes</li> </ul>"},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#authentication","title":"Authentication","text":""},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#supported-modes","title":"Supported Modes","text":"Mode Header Use Case OAuth Bearer <code>Authorization: Bearer &lt;token&gt;</code> Primary Basic Auth <code>Authorization: Basic &lt;base64&gt;</code> Tenant gateway Cookie Auth <code>Cookie: &lt;value&gt;</code> Explicit opt-in"},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#rules","title":"Rules","text":"<ul> <li>Auth MUST be injectable</li> <li>Never hardcode tokens</li> <li>Never log auth headers or cookies</li> <li>Tests MUST mock auth</li> </ul>"},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#testing-contract","title":"Testing Contract","text":""},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#unit-tests","title":"Unit Tests","text":"<p>Must mock HTTP and cover: - Pagination - Rate limiting (429) - Beta headers - Mapping validation - Error paths</p>"},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#integration-tests","title":"Integration Tests","text":"<ul> <li>Must be env-gated</li> <li>Skip cleanly if env vars missing</li> <li>Do NOT intentionally trigger rate limits</li> <li>Assert shape, not volume</li> </ul>"},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#required-env-vars-integration","title":"Required Env Vars (Integration)","text":"<ul> <li><code>ATLASSIAN_GQL_BASE_URL</code></li> <li>One of:</li> <li><code>ATLASSIAN_OAUTH_ACCESS_TOKEN</code></li> <li><code>ATLASSIAN_EMAIL</code> + <code>ATLASSIAN_API_TOKEN</code></li> <li><code>ATLASSIAN_COOKIES_JSON</code></li> </ul>"},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#ai-agent-rules","title":"AI Agent Rules","text":"<p>If you are an AI agent:</p> DO DON'T Fetch introspection Invent GraphQL fields Inspect schema Assume schema stability Generate models Collapse API \u2192 analytics models Map explicitly Remove rate-limiting safeguards Weaken error handling Introduce silent fallbacks <p>If schema details are unclear:</p> <ol> <li>Fetch introspection</li> <li>Inspect schema</li> <li>Generate models</li> <li>Map explicitly</li> </ol> <p>Guessing is a failure.</p>"},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#schema-evolution","title":"Schema Evolution","text":""},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#when-schema-changes","title":"When Schema Changes","text":"<ol> <li>Fetch new introspection</li> <li>Regenerate API models</li> <li>Update mappers if needed</li> <li>Update tests</li> <li>Verify canonical layer unchanged (or version bump if changed)</li> </ol>"},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#canonical-layer-changes","title":"Canonical Layer Changes","text":"<ul> <li>Require explicit versioning</li> <li>Maintain backward compatibility when possible</li> <li>Document breaking changes</li> <li>Update all downstream consumers</li> </ul>"},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#what-this-repo-is-not","title":"What This Repo Is NOT","text":"<ul> <li>Not a thin demo client</li> <li>Not a static schema wrapper</li> <li>Not Jira-only</li> <li>Not tolerant of silent data corruption</li> </ul>"},{"location":"appendix/agent-instructions/connectors/atlassian-graphql/#authority","title":"Authority","text":"<p>If there is conflict between code comments, README, and <code>AGENTS.md</code>:</p> <p>AGENTS.md wins.</p> <p>If unsure: - Preserve correctness - Preserve explicitness - Preserve future schema evolution</p>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/","title":"GitHub &amp; GitLab Connectors","text":""},{"location":"appendix/agent-instructions/connectors/github-gitlab/#overview","title":"Overview","text":"<p>The CLI exposes target-specific sync jobs with provider selection:</p> <p>Sync targets: <code>git</code>, <code>prs</code>, <code>blame</code>, <code>cicd</code>, <code>deployments</code>, <code>incidents</code>, <code>work-items</code></p> <p>Providers: <code>local</code>, <code>github</code>, <code>gitlab</code>, <code>synthetic</code></p>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#authentication","title":"Authentication","text":""},{"location":"appendix/agent-instructions/connectors/github-gitlab/#github","title":"GitHub","text":"Method Variable Notes Token <code>GITHUB_TOKEN</code> Personal access token CLI override <code>--auth \"$GITHUB_TOKEN\"</code> Takes precedence <p>Required scopes: - <code>repo</code> \u2014 Full control (required for private repos) - <code>read:org</code> \u2014 Read org membership (recommended for org repos)</p>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#gitlab","title":"GitLab","text":"Method Variable Notes Token <code>GITLAB_TOKEN</code> Private token CLI override <code>--auth \"$GITLAB_TOKEN\"</code> Takes precedence Self-hosted <code>--gitlab-url</code> Default: <code>https://gitlab.com</code> <p>Required scopes: - <code>read_api</code> \u2014 Read access to API (required for private projects) - <code>read_repository</code> \u2014 Read repository data (required for private projects)</p>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#local-repository-mode","title":"Local Repository Mode","text":"<p>Analyze a local git repository.</p> <pre><code># Environment variables\nexport DATABASE_URI=\"postgresql+asyncpg://localhost:5432/mergestat\"\nexport REPO_PATH=\"/path/to/repo\"\npython cli.py sync git --provider local\n\n# Command-line arguments\npython cli.py sync git --provider local \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --repo-path \"/path/to/repo\"\n\n# With date filtering\npython cli.py sync git --provider local \\\n  --db \"sqlite+aiosqlite:///mergestat.db\" \\\n  --repo-path \"/path/to/repo\" \\\n  --since 2024-01-01\n</code></pre>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#github-connector-mode","title":"GitHub Connector Mode","text":"<p>Fetch data directly from GitHub without cloning. Supports both public and private repos.</p>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#single-repository","title":"Single Repository","text":"<pre><code># Public repository\npython cli.py sync git --provider github \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner torvalds \\\n  --repo linux\n\n# Private repository (token must have 'repo' scope)\npython cli.py sync git --provider github \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner your-org \\\n  --repo your-private-repo\n</code></pre>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#batch-processing","title":"Batch Processing","text":"<pre><code>python cli.py sync git --provider github \\\n  --db \"sqlite+aiosqlite:///mergestat.db\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  -s \"myorg/api-*\" \\\n  --group myorg \\\n  --batch-size 10 \\\n  --max-concurrent 4 \\\n  --max-repos 50 \\\n  --use-async\n</code></pre>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#what-gets-stored","title":"What Gets Stored","text":"<ul> <li>Repository metadata (name, URL, default branch)</li> <li>Commits (up to 100 most recent)</li> <li>Commit statistics (additions, deletions per file for last 50 commits)</li> </ul>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#gitlab-connector-mode","title":"GitLab Connector Mode","text":"<p>Fetch data from GitLab (including self-hosted). Supports public and private projects.</p>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#single-project","title":"Single Project","text":"<pre><code># GitLab.com\npython cli.py sync git --provider gitlab \\\n  --db \"postgresql+asyncpg://localhost:5432/mergestat\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --project-id 278964\n\n# Self-hosted GitLab\npython cli.py sync git --provider gitlab \\\n  --db \"postgresql://...\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --gitlab-url \"https://gitlab.example.com\" \\\n  --project-id 123\n</code></pre>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#batch-processing_1","title":"Batch Processing","text":"<pre><code>python cli.py sync git --provider gitlab \\\n  --db \"sqlite+aiosqlite:///mergestat.db\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --gitlab-url \"https://gitlab.com\" \\\n  --group mygroup \\\n  -s \"mygroup/service-*\" \\\n  --batch-size 10 \\\n  --max-concurrent 4 \\\n  --max-repos 50 \\\n  --use-async\n</code></pre>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#finding-project-id","title":"Finding Project ID","text":"<ol> <li>Go to project page on GitLab</li> <li>Look under project name: \"Project ID: 12345\"</li> <li>Or via API: <code>curl \"https://gitlab.com/api/v4/projects/owner%2Fproject\" --header \"PRIVATE-TOKEN: &lt;token&gt;\"</code></li> </ol>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#command-line-reference","title":"Command-Line Reference","text":""},{"location":"appendix/agent-instructions/connectors/github-gitlab/#common-arguments","title":"Common Arguments","text":"Argument Description <code>--db DB</code> Database connection string (required) <code>--auth TOKEN</code> Authentication token <code>--since DATE</code> Filter commits after date <code>--date DATE</code> Specific date filter <code>--backfill N</code> Days to backfill"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#github-specific","title":"GitHub-Specific","text":"Argument Description <code>--owner OWNER</code> Repository owner <code>--repo REPO</code> Repository name"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#gitlab-specific","title":"GitLab-Specific","text":"Argument Description <code>--project-id ID</code> Numeric project ID <code>--gitlab-url URL</code> GitLab instance URL"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#batch-processing_2","title":"Batch Processing","text":"Argument Description <code>-s, --search PATTERN</code> Glob pattern for repos <code>--group NAME</code> Organization/group name <code>--batch-size N</code> Records per batch <code>--max-concurrent N</code> Concurrent workers <code>--max-repos N</code> Maximum repos to process <code>--use-async</code> Enable async workers <code>--rate-limit-delay SECONDS</code> Delay between requests"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#rate-limits","title":"Rate Limits","text":"Provider Limit Handling GitHub 5,000 requests/hour Automatic retry with backoff GitLab 10 requests/second Automatic retry with backoff <p>Both connectors: - Include automatic retry with exponential backoff - Coordinate concurrent workers via shared backoff gate - Honor server-provided <code>Retry-After</code> / reset delays</p>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#limitations","title":"Limitations","text":""},{"location":"appendix/agent-instructions/connectors/github-gitlab/#github_1","title":"GitHub","text":"<ul> <li>Commits: Up to 100 most recent</li> <li>Commit stats: Last 50 commits (API rate limits)</li> <li>No blame data (requires per-file API calls)</li> <li>No file contents</li> </ul>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#gitlab_1","title":"GitLab","text":"<ul> <li>Commits: Up to 100 most recent</li> <li>Commit stats: Last 50 commits (aggregate per commit, not per-file)</li> <li>No blame data</li> <li>No file contents</li> </ul>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#performance-comparison","title":"Performance Comparison","text":"Mode Best For Trade-offs Local Complete analysis (files, blame, full history) Requires clone GitHub/GitLab Quick commits and stats Limited depth, rate limits"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#troubleshooting","title":"Troubleshooting","text":""},{"location":"appendix/agent-instructions/connectors/github-gitlab/#connectors-are-not-available","title":"\"Connectors are not available\"","text":"<p>Install dependencies: <pre><code>pip install PyGithub python-gitlab\n# or\npip install -r requirements.txt\n</code></pre></p>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#github-404-errors-on-private-repos","title":"GitHub 404 Errors on Private Repos","text":"<p>Token must have <code>repo</code> scope. Verify at: https://github.com/settings/tokens</p>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#gitlab-permission-errors","title":"GitLab Permission Errors","text":"<p>Token must have <code>read_api</code> and <code>read_repository</code> scopes. Check at: Settings \u2192 Access Tokens</p>"},{"location":"appendix/agent-instructions/connectors/github-gitlab/#environment-variables-summary","title":"Environment Variables Summary","text":"Variable Description Default <code>DATABASE_URI</code> Database connection string Required <code>SECONDARY_DATABASE_URI</code> Secondary sink None <code>DB_ECHO</code> Enable SQL logging <code>false</code> <code>BATCH_SIZE</code> Records per batch <code>100</code> <code>MAX_WORKERS</code> Parallel workers <code>4</code> <code>GITHUB_TOKEN</code> GitHub PAT None <code>GITLAB_TOKEN</code> GitLab token None <code>REPO_PATH</code> Local repo path <code>.</code>"},{"location":"appendix/agent-instructions/connectors/jira/","title":"Jira Connector","text":""},{"location":"appendix/agent-instructions/connectors/jira/#overview","title":"Overview","text":"<p>The Jira connector fetches work items from Jira Cloud and normalizes them into the unified <code>WorkItem</code> model for throughput, WIP, and cycle time analysis.</p> <p>Important: Jira is for planning/throughput/WIP metrics. PR metrics (cycle time, merge frequency) come from Git provider sync.</p>"},{"location":"appendix/agent-instructions/connectors/jira/#authentication","title":"Authentication","text":""},{"location":"appendix/agent-instructions/connectors/jira/#required-environment-variables","title":"Required Environment Variables","text":"Variable Description Example <code>JIRA_BASE_URL</code> Jira instance URL <code>your-org.atlassian.net</code> <code>JIRA_EMAIL</code> Account email <code>user@example.com</code> <code>JIRA_API_TOKEN</code> API token From Atlassian account <p>The URL is normalized to <code>https://</code> automatically.</p>"},{"location":"appendix/agent-instructions/connectors/jira/#optional-configuration","title":"Optional Configuration","text":"Variable Description Example <code>JIRA_PROJECT_KEYS</code> Comma-separated project keys <code>ABC,XYZ</code> <code>JIRA_JQL</code> Custom JQL override <code>project = ABC AND ...</code> <code>JIRA_FETCH_ALL</code> Fetch all issues (slow) <code>1</code> <code>JIRA_FETCH_COMMENTS</code> Include comment metadata <code>1</code> (default)"},{"location":"appendix/agent-instructions/connectors/jira/#custom-field-mappings","title":"Custom Field Mappings","text":"Variable Default Description <code>JIRA_STORY_POINTS_FIELD</code> None e.g., <code>customfield_10016</code> <code>JIRA_SPRINT_FIELD</code> <code>customfield_10020</code> Sprint field ID <code>JIRA_EPIC_LINK_FIELD</code> None e.g., <code>customfield_10014</code>"},{"location":"appendix/agent-instructions/connectors/jira/#usage","title":"Usage","text":""},{"location":"appendix/agent-instructions/connectors/jira/#sync-work-items","title":"Sync Work Items","text":"<pre><code>python cli.py sync work-items --provider jira \\\n  --date 2025-02-01 \\\n  --backfill 30 \\\n  --db \"clickhouse://localhost:8123/default\"\n</code></pre>"},{"location":"appendix/agent-instructions/connectors/jira/#quick-api-smoke-test","title":"Quick API Smoke Test","text":"<p>Jira Cloud uses <code>GET /rest/api/3/search/jql</code> (not <code>/search</code>):</p> <pre><code>curl -sS -u \"$JIRA_EMAIL:$JIRA_API_TOKEN\" \\\n  --get \"https://$JIRA_BASE_URL/rest/api/3/search/jql\" \\\n  --data-urlencode \"jql=(updated &gt;= '2025-01-01' OR (statusCategory != Done AND created &lt;= '2025-02-01')) ORDER BY updated DESC\" \\\n  --data-urlencode \"maxResults=5\" \\\n  --data-urlencode \"fields=key,summary,updated,status\"\n</code></pre>"},{"location":"appendix/agent-instructions/connectors/jira/#status-normalization","title":"Status Normalization","text":"<p>Jira statuses are normalized to canonical categories via <code>config/status_mapping.yaml</code>.</p>"},{"location":"appendix/agent-instructions/connectors/jira/#canonical-categories","title":"Canonical Categories","text":"Category Meaning <code>backlog</code> Not yet scheduled <code>todo</code> Scheduled but not started <code>in_progress</code> Active work <code>in_review</code> Awaiting review <code>blocked</code> Work cannot proceed <code>done</code> Successfully completed <code>canceled</code> Abandoned"},{"location":"appendix/agent-instructions/connectors/jira/#configuration","title":"Configuration","text":"<p>In <code>config/status_mapping.yaml</code>:</p> <pre><code>providers:\n  jira:\n    statuses:\n      \"To Do\": todo\n      \"In Progress\": in_progress\n      \"In Review\": in_review\n      \"Blocked\": blocked\n      \"Done\": done\n      \"Canceled\": canceled\n</code></pre>"},{"location":"appendix/agent-instructions/connectors/jira/#identity-mapping","title":"Identity Mapping","text":"<p>To keep user metrics consistent across providers, populate <code>config/identity_mapping.yaml</code>:</p> <pre><code>identities:\n  - canonical: \"jane@example.com\"\n    aliases:\n      - \"jira:accountid:abcd123\"\n      - \"github:jane-dev\"\n      - \"gitlab:jane.developer\"\n</code></pre>"},{"location":"appendix/agent-instructions/connectors/jira/#team-mapping","title":"Team Mapping","text":""},{"location":"appendix/agent-instructions/connectors/jira/#config-based","title":"Config-Based","text":"<p>Create <code>config/team_mapping.yaml</code>:</p> <pre><code>teams:\n  - team_id: \"platform\"\n    team_name: \"Platform Team\"\n    members:\n      - \"jane@example.com\"\n      - \"bob@example.com\"\n</code></pre> <p>Sync: <pre><code>python cli.py sync teams --path config/team_mapping.yaml\n</code></pre></p>"},{"location":"appendix/agent-instructions/connectors/jira/#jira-project-based","title":"Jira Project-Based","text":"<p>Auto-import Jira projects as teams:</p> <pre><code>python cli.py sync teams --provider jira\n</code></pre>"},{"location":"appendix/agent-instructions/connectors/jira/#what-gets-stored","title":"What Gets Stored","text":""},{"location":"appendix/agent-instructions/connectors/jira/#work-item-facts-work_item_cycle_times","title":"Work Item Facts (<code>work_item_cycle_times</code>)","text":"Field Description <code>provider</code> <code>jira</code> <code>work_item_id</code> Jira issue key <code>work_scope_id</code> Project key <code>team_id</code> Assigned team <code>created_at</code> Issue creation <code>started_at</code> First in_progress transition <code>completed_at</code> First done/canceled transition <code>lead_time_hours</code> completed_at - created_at <code>cycle_time_hours</code> completed_at - started_at"},{"location":"appendix/agent-instructions/connectors/jira/#daily-aggregates-work_item_metrics_daily","title":"Daily Aggregates (<code>work_item_metrics_daily</code>)","text":"Metric Description <code>items_started</code> Transitioned to in_progress <code>items_completed</code> Transitioned to done/canceled <code>wip_count_end_of_day</code> Active items at day end <code>cycle_time_p50_hours</code> Median cycle time <code>story_points_completed</code> Sum of story points"},{"location":"appendix/agent-instructions/connectors/jira/#time-in-state-tracking","title":"Time-in-State Tracking","text":"<p>When Jira changelog is available, the connector computes time spent in each status.</p>"},{"location":"appendix/agent-instructions/connectors/jira/#table-work_item_state_durations_daily","title":"Table: <code>work_item_state_durations_daily</code>","text":"Key Description <code>day</code> Date <code>provider</code> <code>jira</code> <code>work_scope_id</code> Project key <code>team_id</code> Team <code>status</code> Normalized status <code>duration_hours</code> Time spent in status <p>Note: Items without changelog history contribute no rows.</p>"},{"location":"appendix/agent-instructions/connectors/jira/#troubleshooting","title":"Troubleshooting","text":""},{"location":"appendix/agent-instructions/connectors/jira/#authentication-errors","title":"Authentication Errors","text":"<ol> <li>Verify API token at: https://id.atlassian.com/manage/api-tokens</li> <li>Ensure email matches account</li> <li>Check URL format (no trailing slash)</li> </ol>"},{"location":"appendix/agent-instructions/connectors/jira/#no-issues-returned","title":"No Issues Returned","text":"<ol> <li>Verify <code>JIRA_PROJECT_KEYS</code> are correct</li> <li>Check JQL syntax if using <code>JIRA_JQL</code></li> <li>Try the curl smoke test above</li> </ol>"},{"location":"appendix/agent-instructions/connectors/jira/#missing-fields","title":"Missing Fields","text":"<p>Custom fields vary by Jira instance. Use Jira admin or API to find field IDs:</p> <pre><code>curl -sS -u \"$JIRA_EMAIL:$JIRA_API_TOKEN\" \\\n  \"https://$JIRA_BASE_URL/rest/api/3/field\" | jq '.[] | {name, id}'\n</code></pre>"},{"location":"appendix/agent-instructions/connectors/jira/#story-points-not-appearing","title":"Story Points Not Appearing","text":"<ol> <li>Find the correct custom field ID for your instance</li> <li>Set <code>JIRA_STORY_POINTS_FIELD=customfield_XXXXX</code></li> </ol>"},{"location":"appendix/agent-instructions/llm/categorization-contract/","title":"LLM Categorization Contract","text":"<p>Rules and specifications for LLM usage in the Dev Health platform.</p>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#overview","title":"Overview","text":"<p>LLMs are used in two contexts:</p> Context When Purpose Constraints Compute-time During data processing Categorize work into investment themes Strict schema, persisted UX-time On user request Explain persisted categorizations Read-only, no recomputation"},{"location":"appendix/agent-instructions/llm/categorization-contract/#compute-time-categorization","title":"Compute-Time Categorization","text":""},{"location":"appendix/agent-instructions/llm/categorization-contract/#purpose","title":"Purpose","text":"<p>Map messy human text to canonical investment categories with subcategory distributions.</p>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#schema-compliance","title":"Schema Compliance","text":"<p>Output MUST be strict JSON matching: <pre><code>work_graph/investment/llm_schema.py\n</code></pre></p>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#output-requirements","title":"Output Requirements","text":"Requirement Details Keys From canonical subcategory registry only Probabilities Valid (0\u20131), normalized Evidence Extractive substrings from input text Theme roll-up Computed deterministically from subcategories"},{"location":"appendix/agent-instructions/llm/categorization-contract/#example-output","title":"Example Output","text":"<pre><code>{\n  \"subcategories\": {\n    \"operational.external\": 0.45,\n    \"operational.incident\": 0.25,\n    \"feature_delivery.customer\": 0.20,\n    \"maintenance.refactor\": 0.10\n  },\n  \"evidence\": {\n    \"operational.external\": [\"customer-facing issue\", \"support ticket\"],\n    \"operational.incident\": [\"incident response\", \"outage window\"],\n    \"feature_delivery.customer\": [\"requested by customer\"],\n    \"maintenance.refactor\": [\"cleanup\", \"technical debt\"]\n  },\n  \"uncertainty\": \"moderate\"\n}\n</code></pre>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#two-stage-process","title":"Two-Stage Process","text":"<ol> <li>LLM Stage: Map text \u2192 subcategory distribution</li> <li>Deterministic Stage: Roll subcategories \u2192 themes (no LLM)</li> </ol> <p>This separation prevents category drift.</p>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#retry-policy","title":"Retry Policy","text":""},{"location":"appendix/agent-instructions/llm/categorization-contract/#when-to-retry","title":"When to Retry","text":"<ul> <li>Whitespace/empty response</li> <li>Truncated response (<code>finish_reason=length</code>)</li> <li>Invalid JSON structure</li> <li>Missing required keys</li> </ul>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#retry-strategy","title":"Retry Strategy","text":"<ol> <li>First attempt: Standard prompt, standard tokens</li> <li>Retry attempt:</li> <li>Double <code>max_completion_tokens</code> (minimum 512)</li> <li>Simplify and harden JSON prompt</li> <li>Add explicit JSON instruction in system AND user message</li> </ol>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#failure-handling","title":"Failure Handling","text":"<p>After one retry failure: 1. Mark categorization as invalid 2. Apply deterministic fallback 3. Persist with <code>fallback_applied=true</code></p>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#audit-fields","title":"Audit Fields","text":"<p>Every categorization run must persist:</p> Field Description <code>categorized_at</code> Timestamp <code>model_version</code> LLM model identifier <code>prompt_hash</code> Hash of prompt template <code>raw_response</code> Original LLM output <code>fallback_applied</code> Boolean <code>retry_count</code> Number of retries <code>finish_reason</code> OpenAI finish reason <code>token_usage</code> Tokens consumed"},{"location":"appendix/agent-instructions/llm/categorization-contract/#openai-specific-handling","title":"OpenAI-Specific Handling","text":""},{"location":"appendix/agent-instructions/llm/categorization-contract/#json-mode","title":"JSON Mode","text":"<p>Include explicit JSON instruction in both: - System message - User message</p>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#token-configuration","title":"Token Configuration","text":"<ul> <li>Use <code>max_completion_tokens</code> (not <code>max_tokens</code>)</li> <li>Minimum: 512 tokens</li> <li>Double on retry</li> </ul>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#observability","title":"Observability","text":"<p>Log on every call: - <code>finish_reason</code> - <code>content_length</code> - Token parameters - Response time</p>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#ux-time-explanation","title":"UX-Time Explanation","text":""},{"location":"appendix/agent-instructions/llm/categorization-contract/#purpose_1","title":"Purpose","text":"<p>Generate human-readable explanations of persisted categorizations.</p>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#constraints","title":"Constraints","text":"Allowed Forbidden Read persisted distributions Recompute categories Read stored evidence Change edges/weights Generate narrative text Introduce new conclusions Cite specific evidence Modify persisted decisions"},{"location":"appendix/agent-instructions/llm/categorization-contract/#required-labeling","title":"Required Labeling","text":"<p>All explanation output MUST be labeled as AI-generated.</p>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#explanation-prompt","title":"Explanation Prompt","text":"<p>Canonical prompt (use verbatim):</p> <pre><code>You are explaining a precomputed investment view.\n\nYou are not allowed to:\n- Recalculate scores\n- Change categories\n- Introduce new conclusions\n- Be conversational (no \"Hello\", \"As an AI\", or interactive follow-ups)\n\nExplain the investment view in three distinct sections:\n\n1. **SUMMARY**: Provide a high-level narrative (max 3 sentences) using\n   probabilistic language (appears, leans, suggests) explaining why\n   the work leans toward the primary categories.\n\n2. **REASONS**: List the specific evidence (structural, contextual,\n   textual) that contributed most to this interpretation.\n\n3. **UNCERTAINTY**: Disclose where uncertainty exists based on the\n   evidence quality and evidence mix.\n\nAlways include evidence quality level and limits.\n</code></pre>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#language-rules","title":"Language Rules","text":""},{"location":"appendix/agent-instructions/llm/categorization-contract/#allowed-language","title":"Allowed Language","text":"<p>Use probabilistic, uncertain phrasing:</p> <ul> <li>appears</li> <li>leans</li> <li>suggests</li> <li>indicates</li> <li>may be</li> </ul>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#forbidden-language","title":"Forbidden Language","text":"<p>Avoid definitive, deterministic phrasing:</p> <ul> <li>is</li> <li>was</li> <li>detected</li> <li>determined</li> <li>definitely</li> <li>clearly</li> </ul>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#rationale","title":"Rationale","text":"<p>The distinction maintains appropriate uncertainty. LLM categorization is inference, not detection.</p>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#evidence-handling","title":"Evidence Handling","text":""},{"location":"appendix/agent-instructions/llm/categorization-contract/#extractive-quotes","title":"Extractive Quotes","text":"<p>Evidence quotes MUST be: - Direct substrings from input text - Not paraphrased - Not summarized - Traceable to source</p>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#evidence-types","title":"Evidence Types","text":"Type Source Example Textual Issue/PR title, description, commits \"hotfix for production bug\" Structural Relationships, links \"Linked to incident #123\" Contextual Timing, patterns \"Merged during outage window\""},{"location":"appendix/agent-instructions/llm/categorization-contract/#forbidden-patterns","title":"Forbidden Patterns","text":""},{"location":"appendix/agent-instructions/llm/categorization-contract/#do-not","title":"Do Not","text":"<ul> <li>Invent categories not in canonical list</li> <li>Use free-form reasoning in output</li> <li>Override canonical vocabulary</li> <li>Return \"unknown\" or \"uncategorized\"</li> <li>Hallucinate evidence not in input</li> <li>Apply categories based on author identity</li> </ul>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#immediate-failure-conditions","title":"Immediate Failure Conditions","text":"<ul> <li>Output contains non-canonical keys</li> <li>Probabilities don't sum correctly</li> <li>Evidence quotes not found in input</li> <li>Missing required output sections</li> </ul>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#testing","title":"Testing","text":""},{"location":"appendix/agent-instructions/llm/categorization-contract/#unit-tests-must-cover","title":"Unit Tests Must Cover","text":"<ul> <li>Valid JSON output parsing</li> <li>Probability normalization</li> <li>Evidence extraction validation</li> <li>Retry logic</li> <li>Fallback application</li> <li>Audit field persistence</li> </ul>"},{"location":"appendix/agent-instructions/llm/categorization-contract/#mock-requirements","title":"Mock Requirements","text":"<ul> <li>Mock LLM API responses</li> <li>Test various failure modes</li> <li>Verify retry behavior</li> <li>Test fallback categorization</li> </ul>"},{"location":"appendix/agent-instructions/metrics/canonical-metrics/","title":"Canonical Metrics Reference","text":"<p>This document defines the authoritative mapping between API views, metric keys, and database schema.</p>"},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#metric-registry","title":"Metric Registry","text":""},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#teamrepo-level-metrics","title":"Team/Repo Level Metrics","text":"Metric Key Label Unit Table Column Aggregation Description <code>cycle_time</code> Cycle Time days <code>work_item_metrics_daily</code> <code>cycle_time_p50_hours</code> Avg (\u2192 days) Time from work start to completion <code>review_latency</code> Review Latency hours <code>repo_metrics_daily</code> <code>pr_first_review_p50_hours</code> Avg Time from PR creation to first review <code>throughput</code> Throughput items <code>work_item_metrics_daily</code> <code>items_completed</code> Sum Count of completed work items <code>deploy_freq</code> Deploy Frequency deploys <code>deploy_metrics_daily</code> <code>deployments_count</code> Sum Number of production deployments <code>churn</code> Code Churn loc <code>repo_metrics_daily</code> <code>total_loc_touched</code> Sum Total lines of code modified <code>wip_saturation</code> WIP Saturation % <code>work_item_metrics_daily</code> <code>wip_congestion_ratio</code> Avg Active items / developer capacity <code>blocked_work</code> Blocked Work hours <code>work_item_state_durations_daily</code> <code>duration_hours</code> Sum Time items spent blocked <code>change_failure_rate</code> Change Failure Rate % <code>repo_metrics_daily</code> <code>change_failure_rate</code> Avg Deployments causing failure <code>rework_ratio</code> Rework Ratio % <code>repo_metrics_daily</code> <code>rework_churn_ratio_30d</code> Avg Churn in recently modified code <code>ci_success</code> CI Success Rate % <code>cicd_metrics_daily</code> <code>success_rate</code> Avg Successful CI pipeline runs"},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#person-level-metrics","title":"Person-Level Metrics","text":"<p>Uses <code>identity_id</code> or <code>user_identity</code> to filter and group.</p> Metric Key Label Unit Table Column Aggregation Description <code>cycle_time</code> Cycle Time days <code>work_item_user_metrics_daily</code> <code>cycle_time_p50_hours</code> Avg Individual cycle time <code>review_latency</code> Review Latency hours <code>user_metrics_daily</code> <code>pr_first_review_p50_hours</code> Avg Time to review assigned PRs <code>throughput</code> Throughput items <code>work_item_user_metrics_daily</code> <code>items_completed</code> Sum Items completed by user <code>churn</code> Code Churn loc <code>user_metrics_daily</code> <code>loc_touched</code> Sum LOC touched by user <code>wip_overlap</code> WIP Overlap items <code>work_item_user_metrics_daily</code> <code>wip_count_end_of_day</code> Avg Concurrent items in progress <code>blocked_work</code> Blocked Work items <code>work_item_cycle_times</code> <code>status='blocked'</code> Sum Items blocked while assigned"},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#database-schema-reference","title":"Database Schema Reference","text":""},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#core-tables","title":"Core Tables","text":"Table Purpose Key Columns <code>repo_metrics_daily</code> Repo-level git/PR stats <code>repo_id</code>, <code>day</code> <code>user_metrics_daily</code> Developer activity <code>repo_id</code>, <code>author_email</code>, <code>day</code> <code>work_item_metrics_daily</code> Work tracking aggregates <code>day</code>, <code>provider</code>, <code>work_scope_id</code>, <code>team_id</code> <code>work_item_user_metrics_daily</code> Developer delivery <code>day</code>, <code>provider</code>, <code>user_identity</code>, <code>team_id</code> <code>team_metrics_daily</code> Team well-being <code>team_id</code>, <code>day</code> <code>work_item_cycle_times</code> Per-item fact rows <code>provider</code>, <code>work_item_id</code> <code>work_item_state_durations_daily</code> Time in state <code>day</code>, <code>provider</code>, <code>work_scope_id</code>, <code>status</code> <code>deploy_metrics_daily</code> Deployment frequency <code>repo_id</code>, <code>day</code> <code>cicd_metrics_daily</code> CI/CD pipeline stats <code>repo_id</code>, <code>day</code>"},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#metric-calculation-details","title":"Metric Calculation Details","text":""},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#commit-size-bucketing","title":"Commit Size Bucketing","text":"<pre><code>total_loc = additions + deletions\n</code></pre> Bucket Range Small total_loc \u2264 50 Medium 51\u2013300 Large &gt; 300"},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#pr-cycle-time","title":"PR Cycle Time","text":"<p>For PRs merged on day D: <pre><code>cycle_time_hours = (merged_at - created_at) / 3600\n</code></pre></p> <p>Distribution fields: - <code>median_pr_cycle_hours</code> (p50) - <code>pr_cycle_p75_hours</code> - <code>pr_cycle_p90_hours</code></p>"},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#work-item-cycle-time","title":"Work Item Cycle Time","text":"<pre><code>lead_time_hours = completed_at - created_at\ncycle_time_hours = completed_at - started_at\n</code></pre> <ul> <li><code>started_at</code>: First transition to <code>in_progress</code></li> <li><code>completed_at</code>: First transition to <code>done</code> or <code>canceled</code></li> <li>Items missing <code>started_at</code> excluded from cycle-time distributions</li> </ul>"},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#large-change-thresholds","title":"Large Change Thresholds","text":"Type Threshold Large commit total_loc &gt; 300 Large PR additions + deletions \u2265 1000 (default)"},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#bus-factor","title":"Bus Factor","text":"<p>Smallest number of developers accounting for \u2265 50% of code churn.</p>"},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#code-ownership-gini","title":"Code Ownership Gini","text":"<p>Gini coefficient (0\u20131) of code contribution inequality. Higher = more concentrated ownership.</p>"},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#api-endpoints","title":"API Endpoints","text":""},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#apiv1explain","title":"<code>/api/v1/explain</code>","text":"<p>Detailed metric breakdown.</p> <p>Parameters: - <code>metric</code>: Key from registry - <code>scope_type</code>: <code>org</code>, <code>team</code>, or <code>repo</code> - <code>scope_id</code>: Identifier for scope - <code>range_days</code>: Lookback period - <code>compare_days</code>: Comparison period</p> <p>Response: - Current value - Delta % - Top drivers (teams/repos contributing) - Individual contributors (PRs/Issues)</p>"},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#apiv1home","title":"<code>/api/v1/home</code>","text":"<p>Executive dashboard with health signals.</p> <ul> <li>Computes deltas for all metrics</li> <li>Highlights \"Constraint of the Week\" (worst regression)</li> </ul>"},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#apiv1peopleperson_idmetric","title":"<code>/api/v1/people/{person_id}/metric</code>","text":"<p>Single person metric detail.</p> <p>Response: - Timeseries - Breakdown by dimension (repo, work_type, stage)</p>"},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#filter-patterns","title":"Filter Patterns","text":""},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#time-window","title":"Time Window","text":"Parameter Usage <code>start_day</code>, <code>end_day</code> Explicit date range (inclusive) <code>range_days</code> Lookback from today <code>compare_days</code> Comparison period duration"},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#scope-types","title":"Scope Types","text":"Type Filters On Notes <code>org</code> Entire organization No filtering <code>team</code> <code>team_id</code> column Join <code>repos</code> on <code>owner_team_id</code> for repo metrics <code>repo</code> <code>repo_id</code> column Direct filter on repo tables"},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#clickhouse-query-patterns","title":"ClickHouse Query Patterns","text":""},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#latest-value-per-key","title":"Latest Value per Key","text":"<p>Tables are append-only with <code>computed_at</code> versioning:</p> <pre><code>SELECT\n  day,\n  argMax(items_completed, computed_at) AS items_completed\nFROM work_item_metrics_daily\nWHERE provider = 'jira'\nGROUP BY day, provider, work_scope_id, team_id\nORDER BY day;\n</code></pre>"},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#daily-totals-two-step","title":"Daily Totals (Two-Step)","text":"<pre><code>SELECT\n  day,\n  sum(items_completed_latest) AS items_completed\nFROM (\n  SELECT\n    day,\n    argMax(items_completed, computed_at) AS items_completed_latest\n  FROM work_item_metrics_daily\n  GROUP BY day, provider, work_scope_id, team_id\n)\nGROUP BY day\nORDER BY day;\n</code></pre>"},{"location":"appendix/agent-instructions/metrics/canonical-metrics/#null-behavior","title":"Null Behavior","text":"<ul> <li>Null fields indicate data unavailable, not zero</li> <li>Items without <code>started_at</code> excluded from cycle-time distributions</li> <li>Items without status history contribute no state duration rows</li> <li>Missing review facts leave pickup/review-time fields NULL</li> </ul>"},{"location":"appendix/agent-instructions/metrics/metric-calculations/","title":"Metric Calculation Details","text":"<p>Deep-dive into how metrics are computed, including formulas, edge cases, and implementation notes.</p>"},{"location":"appendix/agent-instructions/metrics/metric-calculations/#git-repository-metrics","title":"Git &amp; Repository Metrics","text":""},{"location":"appendix/agent-instructions/metrics/metric-calculations/#daily-user-metrics-user_metrics_daily","title":"Daily User Metrics (<code>user_metrics_daily</code>)","text":"<p>Key: <code>(repo_id, author_email, day)</code></p> <p>Note: <code>author_email</code> falls back to <code>author_name</code> when email is missing.</p> Metric Calculation <code>commits_count</code> Count of commits on day <code>loc_added</code> Sum of additions <code>loc_deleted</code> Sum of deletions <code>loc_touched</code> <code>loc_added + loc_deleted</code> <code>files_changed</code> Distinct files changed (union across day) <code>large_commits</code> Commits where <code>total_loc &gt; 300</code> <code>avg_commit_size</code> <code>loc_touched / commits_count</code> <p>PR Metrics (for PRs merged that day):</p> Metric Calculation <code>prs_authored</code> PRs created on day <code>prs_merged</code> PRs merged on day <code>pr_cycle_p50_hours</code> Median of <code>(merged_at - created_at)</code> <code>pr_cycle_p75_hours</code> 75th percentile <code>pr_cycle_p90_hours</code> 90th percentile <p>Collaboration Metrics (nullable):</p> Metric Calculation <code>pr_pickup_time_p50_hours</code> PR created \u2192 first interaction <code>pr_first_review_p50_hours</code> PR created \u2192 first review <code>pr_review_time_p50_hours</code> First review \u2192 merge <code>reviews_given</code> Count of review submissions <code>changes_requested_given</code> Count of change requests"},{"location":"appendix/agent-instructions/metrics/metric-calculations/#daily-repo-metrics-repo_metrics_daily","title":"Daily Repo Metrics (<code>repo_metrics_daily</code>)","text":"<p>Key: <code>(repo_id, day)</code></p> Metric Calculation <code>commits_count</code> Total commits <code>loc_touched</code> Sum of all additions + deletions <code>avg_commit_size</code> <code>loc_touched / commits_count</code> <code>large_commit_ratio</code> Large commits / total commits <code>prs_merged</code> PRs merged on day <code>pr_cycle_p50_hours</code> Median PR cycle time <code>pr_cycle_p75_hours</code> 75th percentile <code>pr_cycle_p90_hours</code> 90th percentile <p>Quality Metrics (best-effort):</p> Metric Calculation <code>large_pr_ratio</code> Large PRs / total PRs <code>pr_rework_ratio</code> PRs with multiple review rounds / total <code>bus_factor</code> Minimum developers for 50% of churn <code>code_ownership_gini</code> Gini coefficient of contribution"},{"location":"appendix/agent-instructions/metrics/metric-calculations/#work-item-metrics","title":"Work Item Metrics","text":""},{"location":"appendix/agent-instructions/metrics/metric-calculations/#normalization","title":"Normalization","text":"<p>Work items from all providers normalize to unified <code>WorkItem</code> model:</p> <pre><code>class WorkItem:\n    provider: str          # jira, github, gitlab\n    work_item_id: str      # Provider-native ID\n    work_scope_id: str     # Project/repo identifier\n    team_id: Optional[str] # Assigned team\n    created_at: datetime\n    started_at: Optional[datetime]   # First in_progress\n    completed_at: Optional[datetime] # First done/canceled\n</code></pre>"},{"location":"appendix/agent-instructions/metrics/metric-calculations/#status-normalization","title":"Status Normalization","text":"<p>Statuses map to canonical categories:</p> Category Meaning <code>backlog</code> Not yet scheduled <code>todo</code> Scheduled but not started <code>in_progress</code> Active work <code>in_review</code> Awaiting review <code>blocked</code> Work cannot proceed <code>done</code> Successfully completed <code>canceled</code> Abandoned"},{"location":"appendix/agent-instructions/metrics/metric-calculations/#daily-work-item-metrics-work_item_metrics_daily","title":"Daily Work Item Metrics (<code>work_item_metrics_daily</code>)","text":"<p>Key: <code>(day, provider, work_scope_id, team_id)</code></p> Metric Calculation <code>items_started</code> Items transitioning to <code>in_progress</code> <code>items_completed</code> Items transitioning to <code>done</code> or <code>canceled</code> <code>items_completed_unassigned</code> Completed items with no assignee <code>wip_count_end_of_day</code> Items in <code>in_progress</code>, <code>in_review</code>, <code>blocked</code> <code>cycle_time_p50_hours</code> Median <code>completed_at - started_at</code> <code>cycle_time_p90_hours</code> 90th percentile cycle time <code>lead_time_p50_hours</code> Median <code>completed_at - created_at</code> <code>lead_time_p90_hours</code> 90th percentile lead time <code>wip_age_p50_hours</code> Median age of WIP items <code>wip_age_p90_hours</code> 90th percentile WIP age <code>bug_completed_ratio</code> Bugs / total completed <code>story_points_completed</code> Sum of story points (Jira only) <code>predictability_score</code> <code>items_completed / (items_completed + wip_count)</code>"},{"location":"appendix/agent-instructions/metrics/metric-calculations/#work-scope-id-convention","title":"Work Scope ID Convention","text":"<p>Provider-native identifier for work scope:</p> Provider Format Example Jira Project key <code>ABC</code> GitHub Issues Repository full name <code>owner/repo</code> GitHub Projects v2 <code>ghprojv2:&lt;org&gt;#&lt;number&gt;</code> <code>ghprojv2:myorg#3</code> GitLab Project full path <code>group/project</code>"},{"location":"appendix/agent-instructions/metrics/metric-calculations/#team-well-being-metrics","title":"Team Well-Being Metrics","text":""},{"location":"appendix/agent-instructions/metrics/metric-calculations/#daily-team-metrics-team_metrics_daily","title":"Daily Team Metrics (<code>team_metrics_daily</code>)","text":"<p>Key: <code>(team_id, day)</code></p> <p>Computed from commits (deduplicated by hash) with team mapping.</p> Metric Calculation <code>after_hours_commit_ratio</code> Commits outside business hours on weekdays <code>weekend_commit_ratio</code> Commits on weekends <p>Configuration:</p> Variable Default Description <code>BUSINESS_TIMEZONE</code> <code>UTC</code> Timezone for hour calculation <code>BUSINESS_HOURS_START</code> <code>9</code> Start of business hours <code>BUSINESS_HOURS_END</code> <code>17</code> End of business hours"},{"location":"appendix/agent-instructions/metrics/metric-calculations/#ic-landscape-metrics","title":"IC Landscape Metrics","text":""},{"location":"appendix/agent-instructions/metrics/metric-calculations/#rolling-30-day-metrics-ic_landscape_rolling_30d","title":"Rolling 30-Day Metrics (<code>ic_landscape_rolling_30d</code>)","text":"<p>Normalized by team percentiles (0\u20131) for visualization.</p> Metric Calculation <code>churn_loc_30d</code> Rolling 30d sum of LOC touched <code>delivery_units_30d</code> <code>prs_merged + work_items_completed</code> <code>cycle_p50_30d_hours</code> Median of daily median PR cycle times <code>wip_max_30d</code> Max active work items over 30d"},{"location":"appendix/agent-instructions/metrics/metric-calculations/#landscape-maps","title":"Landscape Maps","text":"<p>Map 1: Churn \u00d7 Throughput - X: <code>log(churn_loc_30d)</code> - Y: <code>delivery_units_30d</code></p> <p>Map 2: Cycle Time \u00d7 Throughput - X: <code>log(cycle_p50_30d_hours)</code> - Y: <code>delivery_units_30d</code></p> <p>Map 3: WIP \u00d7 Throughput - X: <code>wip_max_30d</code> - Y: <code>delivery_units_30d</code></p> <p>Each coordinate normalized to team percentile rank.</p>"},{"location":"appendix/agent-instructions/metrics/metric-calculations/#code-complexity","title":"Code Complexity","text":""},{"location":"appendix/agent-instructions/metrics/metric-calculations/#daily-complexity-metrics-repo_complexity_daily","title":"Daily Complexity Metrics (<code>repo_complexity_daily</code>)","text":"<p>Computed via <code>radon</code> scanning historical git references.</p> Metric Calculation <code>cyclomatic_total</code> Sum of CC for all functions <code>cyclomatic_avg</code> Mean CC per function <code>high_complexity_functions</code> Functions with CC &gt; 15 <code>very_high_complexity_functions</code> Functions with CC &gt; 25"},{"location":"appendix/agent-instructions/metrics/metric-calculations/#investment-metrics","title":"Investment Metrics","text":""},{"location":"appendix/agent-instructions/metrics/metric-calculations/#daily-investment-investment_metrics_daily","title":"Daily Investment (<code>investment_metrics_daily</code>)","text":"<p>Categorizes effort into investment areas via rule-based classifier.</p> <p>Artifacts classified: - Work items: Based on labels, components, title keywords - Commits (churn): Based on file path patterns</p> Metric Calculation <code>investment_area</code> Assigned category <code>project_stream</code> Secondary grouping <code>delivery_units</code> Story points or item count <code>churn_loc</code> LOC associated with area <p>Configuration: <code>config/investment_areas.yaml</code></p>"},{"location":"appendix/agent-instructions/metrics/metric-calculations/#edge-cases-null-handling","title":"Edge Cases &amp; Null Handling","text":""},{"location":"appendix/agent-instructions/metrics/metric-calculations/#missing-data","title":"Missing Data","text":"Scenario Behavior No <code>started_at</code> Excluded from cycle-time distributions No status history No state duration rows No review facts Pickup/review fields remain NULL No assignee Counted under <code>team_id=''</code> and <code>user_identity='unassigned'</code>"},{"location":"appendix/agent-instructions/metrics/metric-calculations/#recomputation","title":"Recomputation","text":"<ul> <li>All metrics are append-only with <code>computed_at</code> timestamp</li> <li>Use <code>argMax(..., computed_at)</code> to get latest value</li> <li>Safe to recompute any day (idempotent via compound keys)</li> </ul>"},{"location":"appendix/agent-instructions/product/investment-view/","title":"Investment View \u2014 Product Specification","text":""},{"location":"appendix/agent-instructions/product/investment-view/#status","title":"Status","text":"<ul> <li>Signals: POC only, retired</li> <li>Investment View: Canonical model going forward</li> </ul>"},{"location":"appendix/agent-instructions/product/investment-view/#primary-question","title":"Primary Question","text":"<p>\"Where is human effort actually being invested across the organization, and what is the cost to people when certain kinds of work dominate?\"</p> <p>This is: - NOT a work taxonomy - NOT a reporting layer - An organizational mirror for leadership</p>"},{"location":"appendix/agent-instructions/product/investment-view/#core-design-principles-non-negotiable","title":"Core Design Principles (Non-Negotiable)","text":""},{"location":"appendix/agent-instructions/product/investment-view/#1-messy-human-inputs-are-normalized","title":"1. Messy Human Inputs Are Normalized","text":"<ul> <li>Jira labels, issue types, GitHub labels are inputs only</li> <li>They are never surfaced as truth</li> <li>Provider-native categories are normalized away</li> </ul>"},{"location":"appendix/agent-instructions/product/investment-view/#2-investment-categories-are-canonical","title":"2. Investment Categories Are Canonical","text":"<ul> <li>Fixed vocabulary (see below)</li> <li>Not user-configurable</li> <li>Comparable across teams, tools, and time</li> </ul>"},{"location":"appendix/agent-instructions/product/investment-view/#3-investment-categorization-is-decisive","title":"3. Investment Categorization Is Decisive","text":"<ul> <li>Always produces a distribution</li> <li>Never returns \"unknown\"</li> <li>Weak evidence lowers evidence quality, not output existence</li> </ul>"},{"location":"appendix/agent-instructions/product/investment-view/#4-evidence-quality-correctness","title":"4. Evidence Quality \u2260 Correctness","text":"<ul> <li>Indicates corroboration strength</li> <li>Does not block visibility</li> <li>Low quality means \"inferred from weak signals,\" not \"probably wrong\"</li> </ul>"},{"location":"appendix/agent-instructions/product/investment-view/#5-the-output-is-about-people","title":"5. The Output Is About People","text":"<ul> <li>Effort concentration</li> <li>Support load</li> <li>Long-term cost of neglect</li> </ul>"},{"location":"appendix/agent-instructions/product/investment-view/#canonical-investment-categories","title":"Canonical Investment Categories","text":""},{"location":"appendix/agent-instructions/product/investment-view/#themes-fixed-leadership-facing","title":"Themes (Fixed, Leadership-Facing)","text":"Theme Description Feature Delivery New value, customer-requested, roadmap items Operational / Support External support, internal support, incident response Maintenance / Tech Debt Refactoring, upgrades, cleanup Quality / Reliability Testing, observability, stability work Risk / Security Security fixes, compliance, vulnerability remediation <p>Rules: - No synonyms - No overrides - No per-team customization</p>"},{"location":"appendix/agent-instructions/product/investment-view/#subcategories-fixed-per-theme","title":"Subcategories (Fixed Per Theme)","text":"<p>Each theme has a curated subcategory set providing resolution without fragmenting language.</p> <p>Example structure: <pre><code>Operational\n \u251c\u2500\u2500 External-facing\n \u251c\u2500\u2500 Internal support\n \u251c\u2500\u2500 Incident response\n \u2514\u2500\u2500 On-call / reactive\n\nFeature Delivery\n \u251c\u2500\u2500 Customer-requested\n \u251c\u2500\u2500 Strategic / roadmap\n \u2514\u2500\u2500 Enablement / platform\n</code></pre></p> <p>Rules: - Subcategories are not user-defined - Subcategories are comparable across orgs - Subcategories roll up cleanly into themes</p>"},{"location":"appendix/agent-instructions/product/investment-view/#categorization-hierarchy","title":"Categorization Hierarchy","text":"<pre><code>Evidence (WorkUnits)\n   \u2193\nSubcategory (what flavor of work within a theme)\n   \u2193\nTheme (investment category)\n</code></pre> <p>Where: - Theme answers: \"What kind of organizational investment is this?\" - Subcategory answers: \"What flavor of that investment is consuming people?\" - Evidence (WorkUnits) answers: \"What concrete activity supports this inference?\"</p> <p>Critical distinction: - WorkUnits are evidence containers, NOT categories - WorkUnits never appear as categorization layers - WorkUnits never appear as peers to themes/subcategories in UI</p>"},{"location":"appendix/agent-instructions/product/investment-view/#data-model","title":"Data Model","text":""},{"location":"appendix/agent-instructions/product/investment-view/#investment-categorization-output","title":"Investment Categorization Output","text":"<pre><code>{\n  \"entity_id\": \"work_unit_id\",\n  \"investment\": {\n    \"themes\": {\n      \"operational\": 0.47,\n      \"feature_delivery\": 0.33,\n      \"maintenance\": 0.15,\n      \"quality\": 0.05,\n      \"risk\": 0.0\n    },\n    \"subcategories\": {\n      \"operational.external\": 0.29,\n      \"operational.internal\": 0.18,\n      \"feature_delivery.customer\": 0.21,\n      \"feature_delivery.platform\": 0.12,\n      \"maintenance.refactor\": 0.15,\n      \"quality.testing\": 0.05\n    }\n  },\n  \"evidence_quality\": {\n    \"value\": 0.64,\n    \"band\": \"moderate\"\n  },\n  \"evidence\": {\n    \"textual\": [\"Matched phrases: 'hotfix', 'incident response'\"],\n    \"structural\": [\"Linked to on-call incident issue\"],\n    \"contextual\": [\"PR merged directly to main during outage window\"]\n  }\n}\n</code></pre>"},{"location":"appendix/agent-instructions/product/investment-view/#guarantees","title":"Guarantees","text":"<ul> <li>Theme probabilities sum to ~1.0</li> <li>Subcategory probabilities sum to theme probabilities</li> <li>Evidence arrays always present (may be empty)</li> <li>Evidence quality always emitted</li> <li>Categorization never returns \"unknown\"</li> <li>WorkUnit never appears as a category</li> </ul>"},{"location":"appendix/agent-instructions/product/investment-view/#categorization-logic","title":"Categorization Logic","text":""},{"location":"appendix/agent-instructions/product/investment-view/#signal-priority-order","title":"Signal Priority Order","text":"<ol> <li>Textual intent (primary)</li> <li>Issue title &amp; description</li> <li>PR title &amp; description</li> <li> <p>Commit messages</p> </li> <li> <p>Provider metadata (supporting)</p> </li> <li>Jira issue type</li> <li>Labels</li> <li> <p>Milestones</p> </li> <li> <p>Structural context (corroboration)</p> </li> <li>Relationships</li> <li>Timing</li> <li>Repo scope</li> </ol> <p>Text drives the category. Structure adjusts and corroborates.</p>"},{"location":"appendix/agent-instructions/product/investment-view/#evidence-quality","title":"Evidence Quality","text":""},{"location":"appendix/agent-instructions/product/investment-view/#definition","title":"Definition","text":"<p>Evidence quality indicates how strongly the investment categorization is corroborated by independent signals.</p>"},{"location":"appendix/agent-instructions/product/investment-view/#quality-bands","title":"Quality Bands","text":"Band Range Meaning High 0.8+ Strong corroboration from multiple sources Moderate 0.5-0.8 Reasonable corroboration Low &lt;0.5 Inferred from sparse signals"},{"location":"appendix/agent-instructions/product/investment-view/#contributors","title":"Contributors","text":"<ul> <li>Presence of descriptive text</li> <li>Agreement between issue, PR, and commits</li> <li>Presence of provider metadata</li> <li>Structural alignment</li> </ul> <p>Low evidence quality means: - \"This is inferred from weak or sparse signals\"</p> <p>It does NOT mean: - \"This is probably wrong\"</p>"},{"location":"appendix/agent-instructions/product/investment-view/#investment-views-ux","title":"Investment Views (UX)","text":""},{"location":"appendix/agent-instructions/product/investment-view/#1-treemap-investment-composition","title":"1. Treemap \u2014 Investment Composition","text":"<ul> <li>Nodes: Investment category (theme-level by default)</li> <li>Optional split: category \u00d7 repo_scope or team</li> <li>Size: Probability-weighted effort</li> <li>Opacity: Evidence quality</li> </ul> <p>Purpose: Show where effort is going, regardless of how work was labeled.</p>"},{"location":"appendix/agent-instructions/product/investment-view/#2-sunburst-investment-distribution","title":"2. Sunburst \u2014 Investment Distribution","text":"<p>Hierarchy: <pre><code>Investment Category (Theme)\n \u2514\u2500\u2500 Repo scope / Team\n     \u2514\u2500\u2500 Work clusters (optional drill-down)\n</code></pre></p> <p>Purpose: Show concentration of investment and who absorbs it.</p>"},{"location":"appendix/agent-instructions/product/investment-view/#3-sankey-flow-of-investment-pressure","title":"3. Sankey \u2014 Flow of Investment Pressure","text":"<ul> <li>Source: Investment category</li> <li>Target: Repo scope / Team</li> <li>Weight: Probability-weighted effort</li> </ul> <p>Purpose: Make support load, maintenance drag, and feature pressure visible.</p>"},{"location":"appendix/agent-instructions/product/investment-view/#drill-down-contract","title":"Drill-Down Contract","text":"<ol> <li>Default: Theme-only (leadership readable)</li> <li>Drill: Theme \u2192 Subcategory \u2192 Evidence (WorkUnits)</li> </ol> <p>Never show WorkUnits as top-level segments or peers to categories.</p>"},{"location":"appendix/agent-instructions/product/investment-view/#what-is-explicitly-not-shown","title":"What Is Explicitly Not Shown","text":"<ul> <li>Jira issue types</li> <li>GitHub labels</li> <li>Raw ticket metadata</li> <li>\"Bug vs Feature\" debates</li> </ul> <p>These are inputs, not outcomes.</p>"},{"location":"appendix/agent-instructions/product/work-graph/","title":"Work Graph &amp; Investment Materialization","text":""},{"location":"appendix/agent-instructions/product/work-graph/#core-contract-non-negotiable","title":"Core Contract (Non-Negotiable)","text":"<p>These rules are architectural constraints. Violations are regressions.</p> Rule Explanation WorkUnits are evidence containers Not categories, not classification layers LLM categorizes at compute-time only Never at query/render time Theme roll-up is deterministic From subcategories, no LLM involvement UX renders persisted data only No recomputation of categories/edges/weights LLM explanations are read-only May not alter persisted decisions Sinks only for persistence No static files, no export paths"},{"location":"appendix/agent-instructions/product/work-graph/#workunit-definition","title":"WorkUnit Definition","text":""},{"location":"appendix/agent-instructions/product/work-graph/#what-a-workunit-is","title":"What a WorkUnit Is","text":"<ul> <li>An evidence container</li> <li>A unit of aggregation</li> <li>An attribution boundary</li> </ul>"},{"location":"appendix/agent-instructions/product/work-graph/#what-a-workunit-is-not","title":"What a WorkUnit Is NOT","text":"<ul> <li>A category</li> <li>A theme</li> <li>A subcategory</li> <li>A visible classification layer</li> </ul> <p>This distinction is subtle but critical. Treating WorkUnits as categories breaks the normalization model.</p>"},{"location":"appendix/agent-instructions/product/work-graph/#investment-taxonomy-fixed","title":"Investment Taxonomy (Fixed)","text":""},{"location":"appendix/agent-instructions/product/work-graph/#themes","title":"Themes","text":"<p>The five canonical themes (see Investment View):</p> <ul> <li>Feature Delivery</li> <li>Operational / Support</li> <li>Maintenance / Tech Debt</li> <li>Quality / Reliability</li> <li>Risk / Security</li> </ul>"},{"location":"appendix/agent-instructions/product/work-graph/#subcategories","title":"Subcategories","text":"<p>Fixed set per theme, curated for comparability across organizations.</p> <p>Rules: - Themes and subcategories are canonical and non-configurable - Provider labels/types (Jira/GitHub/GitLab) are inputs only - Provider-native labels are normalized away</p>"},{"location":"appendix/agent-instructions/product/work-graph/#compute-time-llm-categorization","title":"Compute-Time LLM Categorization","text":""},{"location":"appendix/agent-instructions/product/work-graph/#schema-compliance","title":"Schema Compliance","text":"<p>LLM output MUST be strict JSON matching: <pre><code>work_graph/investment/llm_schema.py\n</code></pre></p>"},{"location":"appendix/agent-instructions/product/work-graph/#output-requirements","title":"Output Requirements","text":"Requirement Details Keys Must come from canonical subcategory registry Probabilities Must be valid (0-1) and normalized Evidence quotes Must be extractive substrings from provided inputs Sum Subcategory probs sum to theme probs; theme probs sum to ~1.0"},{"location":"appendix/agent-instructions/product/work-graph/#two-stage-mapping","title":"Two-Stage Mapping","text":"<ol> <li>LLM outputs subcategories only \u2014 Maps text to subcategory distribution</li> <li>Theme roll-up is deterministic \u2014 Non-LLM, computed from subcategories</li> </ol> <p>This prevents category drift and maintains normalization.</p>"},{"location":"appendix/agent-instructions/product/work-graph/#retry-policy","title":"Retry Policy","text":"<ul> <li>One repair attempt only</li> <li>On continued failure: mark invalid, apply deterministic fallback</li> <li>Persist audit fields for every categorization run</li> </ul>"},{"location":"appendix/agent-instructions/product/work-graph/#audit-fields","title":"Audit Fields","text":"<p>Always persist: - <code>categorized_at</code> \u2014 Timestamp of categorization - <code>model_version</code> \u2014 LLM model used - <code>prompt_hash</code> \u2014 Hash of prompt template - <code>raw_response</code> \u2014 Original LLM output (for debugging) - <code>fallback_applied</code> \u2014 Boolean if fallback was used</p>"},{"location":"appendix/agent-instructions/product/work-graph/#ux-time-llm-explanation-allowed","title":"UX-Time LLM Explanation (Allowed)","text":""},{"location":"appendix/agent-instructions/product/work-graph/#constraints","title":"Constraints","text":"<p>LLM may generate explanation text only from: - Persisted distributions - Stored evidence</p> <p>LLM must not: - Recompute categories - Change edges - Modify weights - Alter distributions</p>"},{"location":"appendix/agent-instructions/product/work-graph/#labeling","title":"Labeling","text":"<p>All explanation output must be labeled as AI-generated.</p>"},{"location":"appendix/agent-instructions/product/work-graph/#canonical-explanation-prompt","title":"Canonical Explanation Prompt","text":"<pre><code>You are explaining a precomputed investment view.\n\nYou are not allowed to:\n- Recalculate scores\n- Change categories\n- Introduce new conclusions\n- Be conversational (no \"Hello\", \"As an AI\", or interactive follow-ups)\n\nExplain the investment view in three distinct sections:\n\n1. **SUMMARY**: Provide a high-level narrative (max 3 sentences) using\n   probabilistic language (appears, leans, suggests) explaining why\n   the work leans toward the primary categories.\n\n2. **REASONS**: List the specific evidence (structural, contextual,\n   textual) that contributed most to this interpretation.\n\n3. **UNCERTAINTY**: Disclose where uncertainty exists based on the\n   evidence quality and evidence mix.\n\nAlways include evidence quality level and limits.\n</code></pre>"},{"location":"appendix/agent-instructions/product/work-graph/#language-rules","title":"Language Rules","text":""},{"location":"appendix/agent-instructions/product/work-graph/#allowed-language","title":"Allowed Language","text":"<ul> <li>appears</li> <li>leans</li> <li>suggests</li> </ul>"},{"location":"appendix/agent-instructions/product/work-graph/#forbidden-language","title":"Forbidden Language","text":"<ul> <li>is</li> <li>was</li> <li>detected</li> <li>determined</li> </ul> <p>The distinction maintains appropriate uncertainty and avoids false precision.</p>"},{"location":"appendix/agent-instructions/product/work-graph/#persistence-contract","title":"Persistence Contract","text":""},{"location":"appendix/agent-instructions/product/work-graph/#required","title":"Required","text":"<ul> <li>All compute outputs persisted via <code>metrics/sinks/*</code> only</li> </ul>"},{"location":"appendix/agent-instructions/product/work-graph/#forbidden","title":"Forbidden","text":"<ul> <li>JSON/YAML dumps</li> <li>Output file paths</li> <li>Debug files under <code>work_graph/</code> or investment modules</li> </ul>"},{"location":"appendix/agent-instructions/product/work-graph/#openai-response-handling","title":"OpenAI Response Handling","text":"<p>Production bug fix for blank responses (content_length=0, finish_reason=length).</p>"},{"location":"appendix/agent-instructions/product/work-graph/#implementation-checklist","title":"Implementation Checklist","text":"Item Requirement JSON mode Include explicit JSON instruction in system and user message Tokens Use <code>max_completion_tokens</code> (minimum 512) Retry Automatic single retry on whitespace, truncated, or invalid JSON Retry strategy Double tokens and simplify/harden JSON prompt on retry Observability Log <code>finish_reason</code>, <code>content_length</code>, and token params"},{"location":"appendix/agent-instructions/product/work-graph/#migration-notes","title":"Migration Notes","text":""},{"location":"appendix/agent-instructions/product/work-graph/#backend-tasks-if-touching-investment-code","title":"Backend Tasks (if touching Investment code)","text":"<ul> <li>Remove any logic treating WorkUnits as categorization layers</li> <li>Ensure LLM outputs subcategory distributions only</li> <li>Roll subcategories into themes deterministically</li> <li>Update schemas and APIs accordingly</li> </ul>"},{"location":"appendix/agent-instructions/product/work-graph/#frontend-tasks-if-touching-investment-ui","title":"Frontend Tasks (if touching Investment UI)","text":"<ul> <li>Make theme the default and primary view</li> <li>Support drill-down: theme \u2192 subcategory \u2192 evidence</li> <li>Remove any visualization treating WorkUnits as peers to categories</li> <li>Ensure charts always roll up by theme unless explicitly drilled</li> </ul>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/","title":"Visualization Patterns","text":"<p>Guidelines for selecting and implementing visualizations in Dev Health.</p>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#chart-selection-matrix","title":"Chart Selection Matrix","text":"Need Chart Type When to Use Pattern detection Heatmap Cyclical congestion, fragmentation, risk build-up Single trend/KPI Line chart Directional movement, simple comparisons System state Quadrant Mode classification, trajectory analysis Single item diagnosis Flame diagram Understanding delays in one work item Investment composition Treemap Where effort is allocated by theme Investment distribution Sunburst Concentration and who absorbs it Investment flow Sankey Pressure flows between categories and teams"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#heatmaps","title":"Heatmaps","text":""},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#when-to-use","title":"When to Use","text":"<ul> <li>Cyclical congestion: Hour-of-day \u00d7 weekday patterns</li> <li>Fragmentation: Work spread across repos/modules/systems</li> <li>Risk build-up: Hotspots intensifying over time</li> </ul>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#when-not-to-use","title":"When NOT to Use","text":"<ul> <li>Single trend line needed</li> <li>Executive summaries without dimensional breakdown</li> <li>Cannot trace cells to real artifacts</li> </ul>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#implementation-rules","title":"Implementation Rules","text":"<ul> <li>Every cell must link to evidence for that bucket</li> <li>If you cannot trace a cell to artifacts, use line chart instead</li> <li>Use consistent color scales across related views</li> <li>Include legends and axis labels</li> </ul>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#line-charts","title":"Line Charts","text":""},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#when-to-use_1","title":"When to Use","text":"<ul> <li>Clear directional movement over time (e.g., median cycle time)</li> <li>Simple before/after comparisons</li> <li>Executive-level summaries</li> <li>KPI tracking</li> </ul>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#when-not-to-use_1","title":"When NOT to Use","text":"<ul> <li>Multi-dimensional pattern detection needed</li> <li>Averages would hide important patterns</li> <li>Need to identify specific problematic periods</li> </ul>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#quadrants","title":"Quadrants","text":""},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#purpose","title":"Purpose","text":"<p>Classify system modes without ranking or judging performance.</p>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#questions-they-answer","title":"Questions They Answer","text":"<ul> <li>What mode is this team/repo operating in right now?</li> <li>Which direction are we moving across recent windows?</li> <li>Where do effort and outcomes drift out of alignment?</li> </ul>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#usage-rules","title":"Usage Rules","text":"<ul> <li>Use when reasoning about system behavior, not single trends</li> <li>Quadrants are hypothesis starters, not conclusions</li> <li>Always link to evidence</li> <li>Follow up with heatmaps or flame diagrams for diagnosis</li> </ul>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#required-quadrants","title":"Required Quadrants","text":""},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#1-churn-throughput","title":"1. Churn \u00d7 Throughput","text":"Quadrant Interpretation High churn + High throughput Deliberate refactors, major changes High churn + Low throughput Thrashing, rework loops Low churn + High throughput Efficient delivery Low churn + Low throughput Constraints or hidden blocks"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#2-cycle-time-throughput","title":"2. Cycle Time \u00d7 Throughput","text":"Quadrant Interpretation Rising cycle + Stable throughput Coordination debt accumulating Fast cycle + High throughput Healthy flow Slow cycle + Low throughput System under stress Fast cycle + Low throughput Small batch, capacity available"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#3-wip-throughput","title":"3. WIP \u00d7 Throughput","text":"Quadrant Interpretation High WIP + Flat throughput Early saturation warning High WIP + High throughput At capacity, watch for overload Low WIP + High throughput Efficient, focused Low WIP + Low throughput Underutilized or blocked"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#4-review-load-review-latency","title":"4. Review Load \u00d7 Review Latency","text":"Quadrant Interpretation High load + High latency Structural bottleneck High load + Low latency Reviewer doing well, watch for burnout Low load + High latency Disengagement or competing priorities Low load + Low latency Healthy review process"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#zone-maps-experimental","title":"Zone Maps (Experimental)","text":"<p>Optional overlays highlighting fuzzy, overlapping regions with common constraint patterns.</p> <p>Rules: - Zone maps are heuristics - If they feel prescriptive, turn them off - Never use for performance judgment</p>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#flame-diagrams","title":"Flame Diagrams","text":""},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#when-to-use_2","title":"When to Use","text":"<ul> <li>Understanding where time was spent in single work item</li> <li>Visualizing waiting, rework, or handoffs</li> <li>Diagnosing whether delays were sequential or overlapped</li> <li>Identifying rework loops hidden in aggregate metrics</li> </ul>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#when-not-to-use_2","title":"When NOT to Use","text":"<ul> <li>Fleet-wide signals needed</li> <li>Aggregate trend analysis</li> <li>Comparing across many items</li> </ul>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#relationship-to-cycle-time","title":"Relationship to Cycle Time","text":"Tool Scope Purpose Flame diagram Single item Diagnostic, explain outlier Cycle time Portfolio Strategic, prove trend <p>Use flames to explain why a single item took long. Use cycle time to prove the trend across the system.</p>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#investment-views","title":"Investment Views","text":""},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#treemap","title":"Treemap","text":"<ul> <li>Nodes: Theme-level (default) or theme \u00d7 scope/team</li> <li>Size: Probability-weighted effort</li> <li>Opacity: Evidence quality</li> <li>Purpose: Show where effort goes, regardless of labels</li> </ul>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#sunburst","title":"Sunburst","text":"<p>Hierarchical drill-down: <pre><code>Theme\n \u2514\u2500\u2500 Repo scope / Team\n     \u2514\u2500\u2500 Work clusters (optional)\n</code></pre></p> <p>Purpose: Show concentration and who absorbs investment.</p>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#sankey","title":"Sankey","text":"<ul> <li>Source: Investment category (theme)</li> <li>Target: Repo scope / Team</li> <li>Weight: Probability-weighted effort</li> <li>Purpose: Make pressure flows visible</li> </ul>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#scope-guardrails","title":"Scope Guardrails","text":""},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#organizationteamrepo-views","title":"Organization/Team/Repo Views","text":"<ul> <li>Show teams, repos, or services only</li> <li>Never show individual rankings</li> <li>Aggregation is the default</li> </ul>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#individual-views","title":"Individual Views","text":"<ul> <li>Show one person across time windows</li> <li>Optionally show as a path (trajectory)</li> <li>Purpose: Reflection and coaching</li> <li>Never comparative</li> </ul>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#forbidden-patterns","title":"Forbidden Patterns","text":"<ul> <li>Person-to-person rankings or comparisons</li> <li>\"Top performers\" / \"Bottom performers\" lists</li> <li>Stack-ranking visualizations</li> <li>Public leaderboards</li> </ul>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#guardrails-summary","title":"Guardrails Summary","text":"Rule Rationale No person-to-person rankings Prevents misuse for judgment Individual views are single-person only For reflection, not comparison Heatmap cells trace to evidence Maintains inspectability Quadrants show raw values only No percentiles or scoring Flames for diagnosis, cycle time for trends Right tool for right question"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#drill-down-patterns","title":"Drill-Down Patterns","text":""},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#recommended-flow","title":"Recommended Flow","text":"<pre><code>Quadrant \u2192 Heatmap slice \u2192 Flame diagram \u2192 Evidence\n</code></pre> <ol> <li>Quadrant: \"What state are we in?\"</li> <li>Heatmap: \"Where and when is pressure accumulating?\"</li> <li>Flame: \"Why did this specific item take so long?\"</li> <li>Evidence: The actual artifacts (PRs, issues, commits)</li> </ol>"},{"location":"appendix/agent-instructions/visualizations/visualization-patterns/#implementation","title":"Implementation","text":"<ul> <li>Every visualization should support drilling to the next level</li> <li>Evidence should always be the terminal destination</li> <li>Maintain context as user drills down</li> </ul>"},{"location":"appendix/agent-instructions/workflows/cli-reference/","title":"CLI Reference","text":"<p>Complete reference for the dev-health-ops command-line interface.</p>"},{"location":"appendix/agent-instructions/workflows/cli-reference/#overview","title":"Overview","text":"<p>The CLI is implemented in <code>cli.py</code> and orchestrates: - Data synchronization from providers - Metric computation - Fixture generation - Team management</p>"},{"location":"appendix/agent-instructions/workflows/cli-reference/#global-arguments","title":"Global Arguments","text":"Argument Environment Variable Description <code>--db</code> <code>DATABASE_URI</code> Database connection string <code>--sink</code> \u2014 Output target: <code>primary</code>, <code>secondary</code>, <code>both</code>"},{"location":"appendix/agent-instructions/workflows/cli-reference/#database-connection-strings","title":"Database Connection Strings","text":"Backend Format Example PostgreSQL <code>postgresql+asyncpg://</code> <code>postgresql+asyncpg://localhost:5432/db</code> ClickHouse <code>clickhouse://</code> <code>clickhouse://localhost:8123/default</code> MongoDB <code>mongodb://</code> <code>mongodb://localhost:27017/db</code> SQLite <code>sqlite+aiosqlite://</code> <code>sqlite+aiosqlite:///./data.db</code>"},{"location":"appendix/agent-instructions/workflows/cli-reference/#sync-commands","title":"Sync Commands","text":""},{"location":"appendix/agent-instructions/workflows/cli-reference/#sync-git","title":"<code>sync git</code>","text":"<p>Sync git repository data.</p> <pre><code># Local repository\npython cli.py sync git --provider local \\\n  --db \"$DATABASE_URI\" \\\n  --repo-path /path/to/repo\n\n# GitHub\npython cli.py sync git --provider github \\\n  --db \"$DATABASE_URI\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner torvalds \\\n  --repo linux\n\n# GitLab\npython cli.py sync git --provider gitlab \\\n  --db \"$DATABASE_URI\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --project-id 278964\n</code></pre> <p>Options: | Option | Description | |--------|-------------| | <code>--provider</code> | <code>local</code>, <code>github</code>, <code>gitlab</code> | | <code>--repo-path</code> | Path to local repo | | <code>--owner</code>, <code>--repo</code> | GitHub owner/repo | | <code>--project-id</code> | GitLab project ID | | <code>--since</code>, <code>--date</code> | Date filter | | <code>--backfill N</code> | Days to backfill |</p>"},{"location":"appendix/agent-instructions/workflows/cli-reference/#sync-prs","title":"<code>sync prs</code>","text":"<p>Sync pull request data.</p> <pre><code>python cli.py sync prs --provider github \\\n  --db \"$DATABASE_URI\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner org \\\n  --repo repo\n</code></pre>"},{"location":"appendix/agent-instructions/workflows/cli-reference/#sync-work-items","title":"<code>sync work-items</code>","text":"<p>Sync work items from issue trackers.</p> <pre><code># All providers\npython cli.py sync work-items --provider all \\\n  --db \"$DATABASE_URI\" \\\n  --date 2025-02-01 \\\n  --backfill 30\n\n# Jira only\npython cli.py sync work-items --provider jira \\\n  --db \"$DATABASE_URI\"\n\n# GitHub with pattern\npython cli.py sync work-items --provider github \\\n  --db \"$DATABASE_URI\" \\\n  -s \"org/*\"\n</code></pre> <p>Providers: <code>jira</code>, <code>github</code>, <code>gitlab</code>, <code>synthetic</code>, <code>all</code></p>"},{"location":"appendix/agent-instructions/workflows/cli-reference/#sync-cicd","title":"<code>sync cicd</code>","text":"<p>Sync CI/CD pipeline data.</p> <pre><code># GitHub\npython cli.py sync cicd --provider github \\\n  --db \"$DATABASE_URI\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner org \\\n  --repo repo\n\n# GitLab\npython cli.py sync cicd --provider gitlab \\\n  --db \"$DATABASE_URI\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --gitlab-url \"https://gitlab.com\" \\\n  --project-id 123\n</code></pre>"},{"location":"appendix/agent-instructions/workflows/cli-reference/#sync-deployments","title":"<code>sync deployments</code>","text":"<p>Sync deployment events.</p> <pre><code>python cli.py sync deployments --provider github \\\n  --db \"$DATABASE_URI\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner org \\\n  --repo repo\n</code></pre>"},{"location":"appendix/agent-instructions/workflows/cli-reference/#sync-incidents","title":"<code>sync incidents</code>","text":"<p>Sync incident data.</p> <pre><code>python cli.py sync incidents --provider github \\\n  --db \"$DATABASE_URI\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner org \\\n  --repo repo\n</code></pre>"},{"location":"appendix/agent-instructions/workflows/cli-reference/#sync-teams","title":"<code>sync teams</code>","text":"<p>Sync team definitions.</p> <pre><code># From config file\npython cli.py sync teams --path config/team_mapping.yaml\n\n# From Jira projects\npython cli.py sync teams --provider jira\n\n# Synthetic teams\npython cli.py sync teams --provider synthetic\n</code></pre>"},{"location":"appendix/agent-instructions/workflows/cli-reference/#metrics-commands","title":"Metrics Commands","text":""},{"location":"appendix/agent-instructions/workflows/cli-reference/#metrics-daily","title":"<code>metrics daily</code>","text":"<p>Compute daily metrics.</p> <pre><code># Single day\npython cli.py metrics daily \\\n  --db \"$DATABASE_URI\" \\\n  --date 2025-02-01\n\n# With backfill\npython cli.py metrics daily \\\n  --db \"$DATABASE_URI\" \\\n  --date 2025-02-01 \\\n  --backfill 7\n\n# Filter to one repo\npython cli.py metrics daily \\\n  --db \"$DATABASE_URI\" \\\n  --date 2025-02-01 \\\n  --repo-id &lt;uuid&gt;\n</code></pre> <p>Options: | Option | Description | |--------|-------------| | <code>--date</code> | Target date | | <code>--backfill N</code> | Compute N days ending at date | | <code>--repo-id</code> | Filter to specific repository |</p>"},{"location":"appendix/agent-instructions/workflows/cli-reference/#fixtures-commands","title":"Fixtures Commands","text":""},{"location":"appendix/agent-instructions/workflows/cli-reference/#fixtures-generate","title":"<code>fixtures generate</code>","text":"<p>Generate synthetic test data.</p> <pre><code>python cli.py fixtures generate \\\n  --db \"$DATABASE_URI\" \\\n  --days 30\n</code></pre> <p>Options: | Option | Description | |--------|-------------| | <code>--days N</code> | Number of days to generate | | <code>--teams N</code> | Number of teams | | <code>--repos-per-team N</code> | Repos per team |</p>"},{"location":"appendix/agent-instructions/workflows/cli-reference/#batch-processing-options","title":"Batch Processing Options","text":"<p>For GitHub/GitLab batch operations:</p> Option Description <code>-s, --search PATTERN</code> Glob pattern for repos <code>--group NAME</code> Organization/group name <code>--batch-size N</code> Records per batch <code>--max-concurrent N</code> Concurrent workers <code>--max-repos N</code> Maximum repos to process <code>--use-async</code> Enable async workers <code>--rate-limit-delay SECONDS</code> Delay between requests"},{"location":"appendix/agent-instructions/workflows/cli-reference/#environment-variables","title":"Environment Variables","text":""},{"location":"appendix/agent-instructions/workflows/cli-reference/#database","title":"Database","text":"Variable Description <code>DATABASE_URI</code> Primary database connection <code>SECONDARY_DATABASE_URI</code> Secondary sink (with <code>--sink both</code>) <code>DB_ECHO</code> Enable SQL logging"},{"location":"appendix/agent-instructions/workflows/cli-reference/#provider-auth","title":"Provider Auth","text":"Variable Provider <code>GITHUB_TOKEN</code> GitHub <code>GITLAB_TOKEN</code> GitLab <code>JIRA_EMAIL</code> Jira <code>JIRA_API_TOKEN</code> Jira <code>JIRA_BASE_URL</code> Jira"},{"location":"appendix/agent-instructions/workflows/cli-reference/#tuning","title":"Tuning","text":"Variable Default Description <code>BATCH_SIZE</code> 100 Records per batch <code>MAX_WORKERS</code> 4 Parallel workers"},{"location":"appendix/agent-instructions/workflows/cli-reference/#workflow-examples","title":"Workflow Examples","text":""},{"location":"appendix/agent-instructions/workflows/cli-reference/#full-sync-pipeline","title":"Full Sync Pipeline","text":"<pre><code># 1. Sync git data\npython cli.py sync git --provider github \\\n  --db \"$DATABASE_URI\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner myorg \\\n  --repo myrepo\n\n# 2. Sync work items\npython cli.py sync work-items --provider jira \\\n  --db \"$DATABASE_URI\" \\\n  --date 2025-02-01 \\\n  --backfill 30\n\n# 3. Compute metrics\npython cli.py metrics daily \\\n  --db \"$DATABASE_URI\" \\\n  --date 2025-02-01 \\\n  --backfill 30\n</code></pre>"},{"location":"appendix/agent-instructions/workflows/cli-reference/#local-development","title":"Local Development","text":"<pre><code># Generate synthetic data\npython cli.py fixtures generate \\\n  --db \"sqlite+aiosqlite:///./dev.db\" \\\n  --days 30\n\n# Compute metrics\npython cli.py metrics daily \\\n  --db \"sqlite+aiosqlite:///./dev.db\" \\\n  --backfill 30\n</code></pre>"},{"location":"appendix/agent-instructions/workflows/cli-reference/#batch-organization-sync","title":"Batch Organization Sync","text":"<pre><code># Sync all repos in org\npython cli.py sync git --provider github \\\n  --db \"$DATABASE_URI\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  -s \"myorg/*\" \\\n  --group myorg \\\n  --max-concurrent 4 \\\n  --use-async\n</code></pre>"},{"location":"appendix/agent-instructions/workflows/cli-reference/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 General error 2 Configuration error 3 Authentication error 4 Rate limit exceeded"},{"location":"appendix/legacy/","title":"Legacy and deprecated docs","text":"<p>These documents are kept for historical context and may be out of date.</p> <ul> <li>GraphQL migration notes: <code>graphql-migration.md</code></li> <li>GraphQL subscriptions notes: <code>graphql-subscriptions.md</code></li> <li>Performance notes: <code>perf_notes.md</code></li> <li>Integrations index (old): <code>integrations/index.md</code></li> </ul>"},{"location":"appendix/legacy/graphql-migration/","title":"GraphQL Migration Guide","text":"<p>This guide covers migrating from the REST API to the GraphQL API for dev-health-ops.</p>"},{"location":"appendix/legacy/graphql-migration/#overview","title":"Overview","text":"<p>The GraphQL API provides a modern, type-safe alternative to the REST API with several advantages:</p> <ul> <li>Batch queries - Request multiple data types in a single request</li> <li>Real-time subscriptions - Get notified of data changes via WebSocket</li> <li>DataLoaders - Automatic batching and caching of database queries</li> <li>Strong typing - Full schema validation and IDE autocompletion</li> <li>Flexible responses - Request only the fields you need</li> </ul>"},{"location":"appendix/legacy/graphql-migration/#endpoints","title":"Endpoints","text":"Type Endpoint Protocol Queries/Mutations <code>/graphql</code> HTTP POST Subscriptions <code>/graphql</code> WebSocket (graphql-ws) GraphiQL IDE <code>/graphql</code> HTTP GET"},{"location":"appendix/legacy/graphql-migration/#rest-to-graphql-mapping","title":"REST to GraphQL Mapping","text":""},{"location":"appendix/legacy/graphql-migration/#home-metrics","title":"Home Metrics","text":"<p>REST: <pre><code>POST /api/v1/home\n</code></pre></p> <p>GraphQL: <pre><code>query HomeMetrics($orgId: String!) {\n  home(orgId: $orgId) {\n    freshness {\n      lastIngestedAt\n      coverage {\n        reposCoveredPct\n        prsLinkedToIssuesPct\n      }\n    }\n    deltas {\n      metric\n      label\n      value\n      unit\n      deltaPct\n    }\n  }\n}\n</code></pre></p>"},{"location":"appendix/legacy/graphql-migration/#analytics-breakdowns-timeseries-sankey","title":"Analytics (Breakdowns, Timeseries, Sankey)","text":"<p>REST: <pre><code>POST /api/v1/investment\nPOST /api/v1/sankey\n</code></pre></p> <p>GraphQL: <pre><code>query InvestmentAnalytics($orgId: String!, $batch: AnalyticsRequestInput!) {\n  analytics(orgId: $orgId, batch: $batch) {\n    breakdowns {\n      dimension\n      measure\n      items {\n        key\n        value\n      }\n    }\n    sankey {\n      nodes { id label dimension value }\n      edges { source target value }\n      coverage { teamCoverage repoCoverage }\n    }\n  }\n}\n</code></pre></p>"},{"location":"appendix/legacy/graphql-migration/#catalog-dimensions-measures","title":"Catalog (Dimensions, Measures)","text":"<p>REST: <pre><code>GET /api/v1/filters/options\n</code></pre></p> <p>GraphQL: <pre><code>query Catalog($orgId: String!, $dimension: DimensionInput) {\n  catalog(orgId: $orgId, dimension: $dimension) {\n    dimensions { name description }\n    measures { name description }\n    limits { maxDays maxBuckets maxTopN }\n    values { value count }  # If dimension specified\n  }\n}\n</code></pre></p>"},{"location":"appendix/legacy/graphql-migration/#using-persisted-queries","title":"Using Persisted Queries","text":"<p>For production use, prefer persisted queries to reduce payload size and enable server-side caching:</p> <pre><code>curl -X POST /graphql \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-Org-Id: your-org\" \\\n  -H \"X-Persisted-Query-Id: investment-breakdown\" \\\n  -d '{\"variables\": {\"orgId\": \"your-org\", \"batch\": {...}}}'\n</code></pre> <p>Available persisted query IDs: - <code>catalog-dimensions</code> - <code>team-values</code> - <code>repo-values</code> - <code>investment-breakdown</code> - <code>investment-sankey</code> - <code>investment-full</code> - <code>timeseries-weekly</code> - <code>team-analytics-bundle</code> - <code>repo-analytics-bundle</code></p>"},{"location":"appendix/legacy/graphql-migration/#real-time-subscriptions","title":"Real-time Subscriptions","text":"<p>Subscribe to real-time updates via WebSocket:</p> <pre><code>subscription MetricsUpdates($orgId: String!) {\n  metricsUpdated(orgId: $orgId) {\n    orgId\n    day\n    updatedAt\n    message\n  }\n}\n\nsubscription TaskProgress($taskId: String!) {\n  taskStatus(taskId: $taskId) {\n    taskId\n    status\n    progress\n    message\n  }\n}\n</code></pre>"},{"location":"appendix/legacy/graphql-migration/#error-handling","title":"Error Handling","text":"<p>GraphQL errors are returned in the <code>errors</code> array:</p> <pre><code>{\n  \"data\": null,\n  \"errors\": [\n    {\n      \"message\": \"Cost limit exceeded: max_buckets is 100\",\n      \"locations\": [{\"line\": 2, \"column\": 3}],\n      \"path\": [\"analytics\"],\n      \"extensions\": {\n        \"code\": \"COST_LIMIT_EXCEEDED\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"appendix/legacy/graphql-migration/#cost-limits","title":"Cost Limits","text":"<p>The GraphQL API enforces the same cost limits as REST:</p> Limit Value Max date range 3650 days Max buckets per timeseries 100 Max top-N items 100 Max sankey nodes 100 Max sankey edges 500 Max sub-requests per batch 10 Query timeout 30 seconds"},{"location":"appendix/legacy/graphql-migration/#migration-checklist","title":"Migration Checklist","text":"<ol> <li>\u2705 Update frontend to use GraphQL client (urql recommended)</li> <li>\u2705 Replace REST calls with GraphQL queries</li> <li>\u2705 Add WebSocket support for subscriptions</li> <li>\u2705 Enable Zod validation for response types</li> <li>\u2705 Test with GraphiQL IDE at <code>/graphql</code></li> <li>\u2705 Monitor for deprecation warnings on REST endpoints</li> </ol>"},{"location":"appendix/legacy/graphql-subscriptions/","title":"GraphQL Subscriptions","text":"<p>Real-time data updates via GraphQL subscriptions.</p>"},{"location":"appendix/legacy/graphql-subscriptions/#overview","title":"Overview","text":"<p>Subscriptions provide real-time notifications when data changes, eliminating the need for polling. They use WebSocket connections with the <code>graphql-ws</code> protocol.</p>"},{"location":"appendix/legacy/graphql-subscriptions/#connection","title":"Connection","text":"<p>Connect to the GraphQL endpoint via WebSocket:</p> <pre><code>ws://localhost:8000/graphql\nwss://your-domain.com/graphql\n</code></pre> <p>The connection uses the <code>graphql-ws</code> subprotocol.</p>"},{"location":"appendix/legacy/graphql-subscriptions/#available-subscriptions","title":"Available Subscriptions","text":""},{"location":"appendix/legacy/graphql-subscriptions/#metrics-updated","title":"Metrics Updated","text":"<p>Get notified when daily metrics are computed:</p> <pre><code>subscription MetricsUpdated($orgId: String!) {\n  metricsUpdated(orgId: $orgId) {\n    orgId\n    day\n    updatedAt\n    message\n  }\n}\n</code></pre> <p>Use case: Refresh dashboard when new metrics are available.</p>"},{"location":"appendix/legacy/graphql-subscriptions/#task-status","title":"Task Status","text":"<p>Monitor background task progress:</p> <pre><code>subscription TaskStatus($taskId: String!) {\n  taskStatus(taskId: $taskId) {\n    taskId\n    status\n    progress\n    message\n    result\n    updatedAt\n  }\n}\n</code></pre> <p>Status values: - <code>pending</code> - Task queued - <code>running</code> - Task in progress - <code>completed</code> - Task finished successfully - <code>failed</code> - Task failed</p> <p>Use case: Show progress bar for long-running operations.</p>"},{"location":"appendix/legacy/graphql-subscriptions/#sync-progress","title":"Sync Progress","text":"<p>Track data synchronization from external providers:</p> <pre><code>subscription SyncProgress($orgId: String!) {\n  syncProgress(orgId: $orgId) {\n    orgId\n    provider\n    status\n    itemsProcessed\n    itemsTotal\n    message\n    updatedAt\n  }\n}\n</code></pre> <p>Provider values: <code>github</code>, <code>gitlab</code>, <code>jira</code></p> <p>Status values: - <code>starting</code> - Sync beginning - <code>syncing</code> - Sync in progress - <code>completed</code> - Sync finished - <code>failed</code> - Sync failed</p>"},{"location":"appendix/legacy/graphql-subscriptions/#client-implementation","title":"Client Implementation","text":""},{"location":"appendix/legacy/graphql-subscriptions/#using-urql-react","title":"Using urql (React)","text":"<pre><code>import { useSubscription } from 'urql';\n\nconst METRICS_SUBSCRIPTION = `\n  subscription MetricsUpdated($orgId: String!) {\n    metricsUpdated(orgId: $orgId) {\n      orgId\n      day\n      updatedAt\n      message\n    }\n  }\n`;\n\nfunction Dashboard({ orgId }) {\n  const [result] = useSubscription({\n    query: METRICS_SUBSCRIPTION,\n    variables: { orgId },\n  });\n\n  useEffect(() =&gt; {\n    if (result.data?.metricsUpdated) {\n      // Refresh data when metrics update\n      refetchDashboard();\n    }\n  }, [result.data]);\n\n  return &lt;div&gt;...&lt;/div&gt;;\n}\n</code></pre>"},{"location":"appendix/legacy/graphql-subscriptions/#using-javascript-websocket","title":"Using JavaScript WebSocket","text":"<pre><code>import { createClient } from 'graphql-ws';\n\nconst client = createClient({\n  url: 'ws://localhost:8000/graphql',\n});\n\n// Subscribe to metrics updates\nconst unsubscribe = client.subscribe(\n  {\n    query: `\n      subscription MetricsUpdated($orgId: String!) {\n        metricsUpdated(orgId: $orgId) {\n          day\n          message\n        }\n      }\n    `,\n    variables: { orgId: 'my-org' },\n  },\n  {\n    next: (data) =&gt; console.log('Update:', data),\n    error: (err) =&gt; console.error('Error:', err),\n    complete: () =&gt; console.log('Subscription complete'),\n  }\n);\n\n// Later: unsubscribe()\n</code></pre>"},{"location":"appendix/legacy/graphql-subscriptions/#publishing-events","title":"Publishing Events","text":"<p>Backend code can publish subscription events:</p> <pre><code>from api.graphql.subscriptions import (\n    publish_metrics_update,\n    publish_task_status,\n    publish_sync_progress,\n)\n\n# After computing metrics\nawait publish_metrics_update(\n    org_id=\"my-org\",\n    day=\"2024-01-15\",\n    message=\"Daily metrics computed\",\n)\n\n# During task execution\nawait publish_task_status(\n    task_id=\"abc123\",\n    status=\"running\",\n    progress=50.0,\n    message=\"Processing files...\",\n)\n\n# During data sync\nawait publish_sync_progress(\n    org_id=\"my-org\",\n    provider=\"github\",\n    status=\"syncing\",\n    items_processed=150,\n    items_total=500,\n)\n</code></pre>"},{"location":"appendix/legacy/graphql-subscriptions/#pubsub-backend","title":"PubSub Backend","text":"<p>Subscriptions use Redis PubSub for distributed deployments:</p> <ul> <li>With Redis: Events broadcast to all API instances</li> <li>Without Redis: In-memory channels (single instance only)</li> </ul> <p>Configure via <code>REDIS_URL</code> environment variable.</p>"},{"location":"appendix/legacy/graphql-subscriptions/#best-practices","title":"Best Practices","text":"<ol> <li>Reconnection: Implement automatic reconnection for dropped WebSocket connections</li> <li>Heartbeats: The server sends periodic heartbeats to keep connections alive</li> <li>Unsubscribe: Always unsubscribe when components unmount</li> <li>Error handling: Handle subscription errors gracefully</li> <li>Debouncing: Debounce UI updates if events arrive rapidly</li> </ol>"},{"location":"appendix/legacy/perf_notes/","title":"ClickHouse performance assessment note","text":""},{"location":"appendix/legacy/perf_notes/#target-queries","title":"Target queries","text":"<ul> <li>Grafana work tracking rollups using <code>argMax</code> + <code>GROUP BY</code> on <code>stats.work_item_metrics_daily</code> and <code>stats.work_item_state_durations_daily</code> (see <code>grafana/dashboards/work_tracking.json</code>).</li> <li>Grafana advanced work tracking rollups on <code>stats.work_item_metrics_daily</code>, <code>stats.work_item_state_durations_daily</code>, and <code>stats.work_item_cycle_times</code> (see <code>grafana/dashboards/advanced_work_tracking.json</code>).</li> <li>Hotspot explorer joins between <code>stats.file_metrics_daily</code> and <code>stats.file_hotspot_daily</code> (see <code>grafana/dashboards/code_hotspots.json</code>).</li> <li>Investment flow edges from <code>stats.v_investment_flow_edges</code> (see <code>grafana/dashboards/investment_areas.json</code>).</li> <li>IC landscape scatter queries on <code>stats.v_ic_landscape_points</code> (see <code>grafana/dashboards/developer_landscape.json</code>).</li> </ul>"},{"location":"appendix/legacy/perf_notes/#current-timings","title":"Current timings","text":"<ul> <li>No ClickHouse query timings or <code>system.query_log</code> samples are checked into the repo.</li> <li>No performance benchmarks or issue notes reference slow ClickHouse queries.</li> <li>Timing status: unchecked (no evidence to confirm slow queries).</li> <li>Use <code>python cli.py audit perf --db &lt;clickhouse_uri&gt;</code> to sample slow queries from <code>system.query_log</code> (requires <code>log_queries</code> enabled).</li> </ul>"},{"location":"appendix/legacy/perf_notes/#decision","title":"Decision","text":"<p>Defer ClickHouse perf migrations until we have concrete timings that show slow queries. This task is non-blocking for Phase 0.</p>"},{"location":"appendix/legacy/integrations/","title":"Integrations","text":"<p>Integration is split into: - Connectors (fetch provider data) - Processors (normalize to internal models) - Sinks (persist) - Metrics (materialize outputs)</p> <p>Provider deep dives: - GitHub/GitLab: <code>docs/90-appendix/agent-instructions/connectors/github-gitlab.md</code> - Jira: <code>docs/90-appendix/agent-instructions/connectors/jira.md</code> - Atlassian GraphQL: <code>docs/90-appendix/agent-instructions/connectors/atlassian-graphql.md</code></p>"},{"location":"architecture/data-pipeline/","title":"Data Pipeline Architecture (dev-health-ops)","text":""},{"location":"architecture/data-pipeline/#pipeline-overview","title":"Pipeline Overview","text":"<p>The dev-health-ops backend follows a strict unidirectional pipeline:</p> <pre><code>Connectors \u2192 Processors \u2192 Sinks \u2192 Metrics \u2192 Visualization\n</code></pre> <p>Each stage has clear responsibilities. Do not collapse layers or bypass stages.</p>"},{"location":"architecture/data-pipeline/#1-connectors-connectors","title":"1. Connectors (<code>connectors/</code>)","text":"<p>Purpose: Fetch raw data from external providers.</p>"},{"location":"architecture/data-pipeline/#supported-providers","title":"Supported Providers","text":"Provider Module Sync Targets Local Git <code>connectors/local.py</code> git, blame GitHub <code>connectors/github.py</code> git, prs, cicd, deployments, incidents, work-items GitLab <code>connectors/gitlab.py</code> git, prs, cicd, deployments, incidents, work-items Jira <code>connectors/jira.py</code> work-items Synthetic <code>connectors/synthetic.py</code> fixtures generation"},{"location":"architecture/data-pipeline/#rules","title":"Rules","text":"<ul> <li>Network I/O should be async and batch-friendly</li> <li>Respect rate limits and backoff mechanisms</li> <li>Return raw provider data (minimal transformation)</li> <li>Handle pagination completely (never assume single page)</li> </ul>"},{"location":"architecture/data-pipeline/#2-processors-processors","title":"2. Processors (<code>processors/</code>)","text":"<p>Purpose: Normalize and transform connector outputs into internal models.</p>"},{"location":"architecture/data-pipeline/#key-processor","title":"Key Processor","text":"<ul> <li><code>processors/local.py</code> \u2014 Primary processor for local git data</li> </ul>"},{"location":"architecture/data-pipeline/#responsibilities","title":"Responsibilities","text":"<ul> <li>Map provider-specific fields to unified models</li> <li>Normalize timestamps to UTC</li> <li>Resolve identities across providers</li> <li>Enrich with computed fields (e.g., commit size buckets)</li> </ul>"},{"location":"architecture/data-pipeline/#rules_1","title":"Rules","text":"<ul> <li>No network I/O</li> <li>No persistence logic</li> <li>Transform only, no business decisions</li> <li>Output must match models in <code>models/</code></li> </ul>"},{"location":"architecture/data-pipeline/#3-storage-sinks-metricssinks","title":"3. Storage / Sinks (<code>metrics/sinks/</code>)","text":"<p>Purpose: Persist processed data to storage backends.</p>"},{"location":"architecture/data-pipeline/#supported-backends","title":"Supported Backends","text":"Backend Connection Use Case PostgreSQL <code>postgresql+asyncpg://</code> Relational, migrations ClickHouse <code>clickhouse://</code> Analytics queries MongoDB <code>mongodb://</code> Document storage SQLite <code>sqlite+aiosqlite://</code> Local dev/test"},{"location":"architecture/data-pipeline/#rules_2","title":"Rules","text":"<ul> <li>No file exports. No debug dumps. No JSON/YAML output paths.</li> <li>All persistence goes through sink modules</li> <li>Backend selection via <code>--db</code> flag or <code>DATABASE_URI</code></li> <li>Secondary sink via <code>SECONDARY_DATABASE_URI</code> with <code>sink='both'</code></li> </ul>"},{"location":"architecture/data-pipeline/#sink-interface","title":"Sink Interface","text":"<pre><code>async def write_batch(records: List[Model], session: AsyncSession) -&gt; int:\n    \"\"\"Write a batch of records. Returns count written.\"\"\"\n</code></pre>"},{"location":"architecture/data-pipeline/#4-metrics-metrics","title":"4. Metrics (<code>metrics/</code>)","text":"<p>Purpose: Compute higher-level rollups and aggregates from persisted data.</p>"},{"location":"architecture/data-pipeline/#key-metric-tables","title":"Key Metric Tables","text":"Table Key Content <code>repo_metrics_daily</code> <code>(repo_id, day)</code> Commits, LOC, PR cycle time <code>user_metrics_daily</code> <code>(repo_id, author_email, day)</code> User activity <code>work_item_metrics_daily</code> <code>(day, provider, work_scope_id, team_id)</code> Throughput, WIP, cycle time <code>team_metrics_daily</code> <code>(team_id, day)</code> After-hours, weekend ratios"},{"location":"architecture/data-pipeline/#computation-model","title":"Computation Model","text":"<ul> <li>Metrics are append-only with <code>computed_at</code> versioning</li> <li>Use <code>argMax(&lt;metric&gt;, computed_at)</code> to get latest value</li> <li>Re-computation is safe (idempotent via compound keys)</li> </ul>"},{"location":"architecture/data-pipeline/#5-visualization-grafana-dev-health-web","title":"5. Visualization (Grafana + dev-health-web)","text":"<p>Purpose: Render persisted data for exploration.</p>"},{"location":"architecture/data-pipeline/#grafana","title":"Grafana","text":"<ul> <li>Dashboards provisioned via <code>grafana/</code> directory</li> <li>Query conventions:</li> <li>Prefer table format with stable time ordering</li> <li>Handle <code>team_id</code> null/empty normalization</li> <li>Avoid ClickHouse <code>WITH name = expr</code> syntax; use <code>WITH ... AS</code></li> </ul>"},{"location":"architecture/data-pipeline/#dev-health-web","title":"dev-health-web","text":"<ul> <li>Visualization-only \u2014 Must not become source of truth</li> <li>Consumes data via GraphQL API from dev-health-ops</li> <li>No category recomputation at UX time</li> </ul>"},{"location":"architecture/data-pipeline/#storage-schema-highlights","title":"Storage Schema Highlights","text":""},{"location":"architecture/data-pipeline/#clickhouse-tables","title":"ClickHouse Tables","text":"<p>Tables are <code>MergeTree</code> partitioned by <code>toYYYYMM(day)</code>:</p> <pre><code>CREATE TABLE repo_metrics_daily (\n    repo_id UUID,\n    day Date,\n    computed_at DateTime,\n    -- metrics columns\n) ENGINE = MergeTree()\nPARTITION BY toYYYYMM(day)\nORDER BY (repo_id, day);\n</code></pre>"},{"location":"architecture/data-pipeline/#postgresql-tables","title":"PostgreSQL Tables","text":"<p>Managed via Alembic migrations in <code>alembic/</code>:</p> <pre><code># Generate migration\nalembic revision --autogenerate -m \"description\"\n\n# Apply migrations\nalembic upgrade head\n</code></pre>"},{"location":"architecture/data-pipeline/#environment-variables","title":"Environment Variables","text":"Variable Purpose Required <code>DATABASE_URI</code> Primary database connection Yes <code>SECONDARY_DATABASE_URI</code> Secondary sink (with <code>--sink both</code>) No <code>DB_ECHO</code> Enable SQL logging No <code>BATCH_SIZE</code> Records per batch insert No (default: 100) <code>MAX_WORKERS</code> Parallel workers No (default: 4)"},{"location":"architecture/data-pipeline/#adding-new-pipeline-components","title":"Adding New Pipeline Components","text":""},{"location":"architecture/data-pipeline/#new-connector","title":"New Connector","text":"<ol> <li>Create <code>connectors/newprovider.py</code></li> <li>Implement async fetch methods</li> <li>Register in <code>connectors/__init__.py</code></li> <li>Add CLI integration in <code>cli.py</code></li> </ol>"},{"location":"architecture/data-pipeline/#new-metric","title":"New Metric","text":"<ol> <li>Define model in <code>models/</code></li> <li>Add sink in <code>metrics/sinks/</code></li> <li>Implement computation in <code>metrics/</code></li> <li>Create Alembic migration if using Postgres</li> <li>Update Grafana dashboards</li> </ol>"},{"location":"architecture/data-pipeline/#rules-when-modifying","title":"Rules When Modifying","text":"<ul> <li>Never bypass sinks for persistence</li> <li>Always handle pagination</li> <li>Add tests under <code>tests/</code></li> <li>Respect existing async patterns</li> </ul>"},{"location":"architecture/frontend-architecture/","title":"Frontend Architecture (dev-health-web)","text":""},{"location":"architecture/frontend-architecture/#technology-stack","title":"Technology Stack","text":"<ul> <li>Framework: Next.js (App Router)</li> <li>Language: TypeScript</li> <li>Styling: Tailwind CSS</li> <li>Testing: Vitest (unit), Playwright (e2e)</li> <li>Data: GraphQL client to dev-health-ops API</li> </ul>"},{"location":"architecture/frontend-architecture/#directory-structure","title":"Directory Structure","text":"<pre><code>src/\n\u251c\u2500\u2500 app/           # Next.js pages and routes\n\u251c\u2500\u2500 components/    # Reusable UI components\n\u2502   \u251c\u2500\u2500 charts/    # Visualization components\n\u2502   \u251c\u2500\u2500 filters/   # Filter controls\n\u2502   \u2514\u2500\u2500 navigation/# Nav components\n\u251c\u2500\u2500 lib/           # Data transforms, mappers, helpers\n\u251c\u2500\u2500 data/          # Sample data for demos/tests\n\u2514\u2500\u2500 types/         # TypeScript type definitions\n\ntests/             # Playwright e2e tests\ntest-results/      # Test artifacts\n</code></pre>"},{"location":"architecture/frontend-architecture/#key-architectural-patterns","title":"Key Architectural Patterns","text":""},{"location":"architecture/frontend-architecture/#1-server-vs-client-components","title":"1. Server vs Client Components","text":"<ul> <li>Server Components: Default for pages, data fetching colocated with page</li> <li>Client Components: Interactive elements, charts, filters</li> <li>Mark client components with <code>'use client'</code> directive</li> </ul>"},{"location":"architecture/frontend-architecture/#2-data-flow","title":"2. Data Flow","text":"<pre><code>GraphQL API \u2192 src/lib transforms \u2192 Chart components \u2192 Rendered UI\n</code></pre> <ul> <li>Sample data in <code>src/data/</code> for demos and unit tests</li> <li>Real data from dev-health-ops GraphQL API</li> <li>Test mode uses <code>DEV_HEALTH_TEST_MODE</code> env var</li> </ul>"},{"location":"architecture/frontend-architecture/#3-type-safety","title":"3. Type Safety","text":"<ul> <li>All components use TypeScript</li> <li>Shared types in <code>src/types/</code></li> <li>Filter types must use union types (e.g., <code>\"repo\" | \"org\" | \"team\"</code>)</li> </ul>"},{"location":"architecture/frontend-architecture/#development-conventions","title":"Development Conventions","text":""},{"location":"architecture/frontend-architecture/#component-guidelines","title":"Component Guidelines","text":"<pre><code>// Good: Typed props, clear interface\ninterface MetricCardProps {\n  metric: Metric;\n  scope: MetricFilter;\n  onDrillDown?: (id: string) =&gt; void;\n}\n\nexport function MetricCard({ metric, scope, onDrillDown }: MetricCardProps) {\n  // Implementation\n}\n</code></pre>"},{"location":"architecture/frontend-architecture/#state-management","title":"State Management","text":"<ul> <li>Avoid synchronous <code>setState</code> in effects</li> <li>Derive sample data via memo + computed loading</li> <li>Use React Server Components for data fetching when possible</li> </ul>"},{"location":"architecture/frontend-architecture/#eslint-rules","title":"ESLint Rules","text":"<p>Active rules to follow: - <code>react-hooks/set-state-in-effect</code> \u2014 No sync setState in effects - <code>react-hooks/exhaustive-deps</code> \u2014 Include all dependencies</p>"},{"location":"architecture/frontend-architecture/#testing-strategy","title":"Testing Strategy","text":""},{"location":"architecture/frontend-architecture/#unit-tests-vitest","title":"Unit Tests (Vitest)","text":"<ul> <li>Located next to modules: <code>src/lib/__tests__/</code></li> <li>Test transforms, mappers, helpers independently</li> </ul> <pre><code>// src/lib/__tests__/transforms.test.ts\nimport { transformMetrics } from '../transforms';\n\ndescribe('transformMetrics', () =&gt; {\n  it('converts hours to days', () =&gt; {\n    expect(transformMetrics({ hours: 24 })).toEqual({ days: 1 });\n  });\n});\n</code></pre>"},{"location":"architecture/frontend-architecture/#e2e-tests-playwright","title":"E2E Tests (Playwright)","text":"<ul> <li>Located in <code>tests/</code> directory</li> <li>Use test mode environment variables</li> <li>Include visual regression when possible</li> </ul> <pre><code>// tests/dashboard.spec.ts\ntest('dashboard loads metrics', async ({ page }) =&gt; {\n  await page.goto('/dashboard');\n  await expect(page.getByTestId('metric-card')).toBeVisible();\n});\n</code></pre>"},{"location":"architecture/frontend-architecture/#test-environment","title":"Test Environment","text":"<p>Playwright config sets these env vars: - <code>DEV_HEALTH_TEST_MODE=1</code> - <code>NEXT_PUBLIC_DEV_HEALTH_TEST_MODE=1</code></p> <p>Components should support sample data when these are set.</p>"},{"location":"architecture/frontend-architecture/#graphql-client","title":"GraphQL Client","text":""},{"location":"architecture/frontend-architecture/#setup","title":"Setup","text":"<pre><code>// src/lib/graphql-client.ts\nimport { createClient } from '@urql/core';\n\nexport const client = createClient({\n  url: process.env.NEXT_PUBLIC_GRAPHQL_URL || 'http://localhost:8000/graphql',\n});\n</code></pre>"},{"location":"architecture/frontend-architecture/#query-patterns","title":"Query Patterns","text":"<pre><code>// Fetch with typing\nconst result = await client.query&lt;MetricsQuery&gt;(METRICS_QUERY, {\n  scope: { type: 'team', id: teamId },\n  range: { days: 14 },\n});\n</code></pre> <p>See <code>docs/graphql-client.md</code> for full client documentation.</p>"},{"location":"architecture/frontend-architecture/#visualization-integration","title":"Visualization Integration","text":""},{"location":"architecture/frontend-architecture/#chart-components-location","title":"Chart Components Location","text":"<ul> <li><code>src/components/charts/</code> \u2014 All chart implementations</li> <li>Use visualization patterns from <code>docs/visualizations.md</code></li> </ul>"},{"location":"architecture/frontend-architecture/#data-transform-pattern","title":"Data Transform Pattern","text":"<pre><code>// src/lib/transforms/metric-transforms.ts\nexport function toChartData(metrics: Metric[]): ChartDataPoint[] {\n  return metrics.map(m =&gt; ({\n    x: m.day,\n    y: m.value,\n    label: m.label,\n  }));\n}\n</code></pre>"},{"location":"architecture/frontend-architecture/#pr-review-guidelines","title":"PR &amp; Review Guidelines","text":"<ul> <li>Use descriptive PR titles referencing related tests</li> <li>Keep changes scoped to one feature or bugfix</li> <li>Include screenshot or Playwright trace for visual changes</li> <li>Run <code>npm run lint</code> and <code>npm run test</code> before submitting</li> </ul>"},{"location":"architecture/frontend-architecture/#common-gotchas","title":"Common Gotchas","text":""},{"location":"architecture/frontend-architecture/#1-filter-type-unions","title":"1. Filter Type Unions","text":"<pre><code>// Wrong: string type loses type safety\nconst scope: { level: string } = { level: 'team' };\n\n// Correct: Use union type\nconst scope: { level: 'repo' | 'org' | 'team' } = { level: 'team' };\n</code></pre>"},{"location":"architecture/frontend-architecture/#2-test-mode-detection","title":"2. Test Mode Detection","text":"<pre><code>// Check for test mode in components\nconst isTestMode = process.env.NEXT_PUBLIC_DEV_HEALTH_TEST_MODE === '1';\nconst data = isTestMode ? sampleData : await fetchRealData();\n</code></pre>"},{"location":"architecture/frontend-architecture/#3-hydration-mismatches","title":"3. Hydration Mismatches","text":"<ul> <li>Server and client must render same initial content</li> <li>Use <code>useEffect</code> for client-only logic</li> <li>Check <code>typeof window !== 'undefined'</code> when needed</li> </ul>"},{"location":"architecture/repo-layout/","title":"Repo layout and stability","text":""},{"location":"architecture/repo-layout/#dev-health-ops","title":"dev-health-ops","text":"<p>Current repo layout places Python packages at repo root (e.g., <code>api/</code>, <code>metrics/</code>, <code>work_graph/</code>).</p> <p>A planned update will move Python code under <code>./src</code>. This should not impact these docs because: - Docs reference logical modules and stable public entry points (CLI, GraphQL schema, sinks contracts). - File-path references are used for navigation only; treat them as \u201cas of commit\u201d.</p>"},{"location":"architecture/repo-layout/#key-module-roots-today","title":"Key module roots (today)","text":"<ul> <li><code>cli.py</code> (entry point)</li> <li><code>connectors/</code> (provider sync jobs)</li> <li><code>processors/</code> (normalization)</li> <li><code>metrics/</code> (compute + sinks)</li> <li><code>work_graph/</code> (investment compute + materialization)</li> <li><code>api/graphql/</code> (GraphQL analytics API)</li> </ul>"},{"location":"architecture/repo-layout/#dev-health-web","title":"dev-health-web","text":"<ul> <li><code>src/app</code> (routes/pages)</li> <li><code>src/lib/graphql</code> (client provider, hooks)</li> <li><code>src/components</code> (charts and UI)</li> </ul>"},{"location":"computations/","title":"Computations","text":"<p>All computations must be: - Reproducible - Traceable to evidence - Materialized (not recomputed in the UI)</p> <p>Canonical metric registry: - <code>docs/20-computations/02-canonical-metrics.md</code> - <code>docs/20-computations/03-metric-calculations.md</code></p> <p>Investment compute: - <code>work_graph/investment/categorize.py</code> - <code>work_graph/investment/materialize.py</code></p>"},{"location":"computations/canonical-metrics-prd/","title":"Canonical Metrics Reference","text":"<p>This document defines the authoritative mapping between API views, metric keys, and database schema.</p>"},{"location":"computations/canonical-metrics-prd/#metric-registry","title":"Metric Registry","text":""},{"location":"computations/canonical-metrics-prd/#teamrepo-level-metrics","title":"Team/Repo Level Metrics","text":"Metric Key Label Unit Table Column Aggregation Description <code>cycle_time</code> Cycle Time days <code>work_item_metrics_daily</code> <code>cycle_time_p50_hours</code> Avg (\u2192 days) Time from work start to completion <code>review_latency</code> Review Latency hours <code>repo_metrics_daily</code> <code>pr_first_review_p50_hours</code> Avg Time from PR creation to first review <code>throughput</code> Throughput items <code>work_item_metrics_daily</code> <code>items_completed</code> Sum Count of completed work items <code>deploy_freq</code> Deploy Frequency deploys <code>deploy_metrics_daily</code> <code>deployments_count</code> Sum Number of production deployments <code>churn</code> Code Churn loc <code>repo_metrics_daily</code> <code>total_loc_touched</code> Sum Total lines of code modified <code>wip_saturation</code> WIP Saturation % <code>work_item_metrics_daily</code> <code>wip_congestion_ratio</code> Avg Active items / developer capacity <code>blocked_work</code> Blocked Work hours <code>work_item_state_durations_daily</code> <code>duration_hours</code> Sum Time items spent blocked <code>change_failure_rate</code> Change Failure Rate % <code>repo_metrics_daily</code> <code>change_failure_rate</code> Avg Deployments causing failure <code>rework_ratio</code> Rework Ratio % <code>repo_metrics_daily</code> <code>rework_churn_ratio_30d</code> Avg Churn in recently modified code <code>ci_success</code> CI Success Rate % <code>cicd_metrics_daily</code> <code>success_rate</code> Avg Successful CI pipeline runs"},{"location":"computations/canonical-metrics-prd/#person-level-metrics","title":"Person-Level Metrics","text":"<p>Uses <code>identity_id</code> or <code>user_identity</code> to filter and group.</p> Metric Key Label Unit Table Column Aggregation Description <code>cycle_time</code> Cycle Time days <code>work_item_user_metrics_daily</code> <code>cycle_time_p50_hours</code> Avg Individual cycle time <code>review_latency</code> Review Latency hours <code>user_metrics_daily</code> <code>pr_first_review_p50_hours</code> Avg Time to review assigned PRs <code>throughput</code> Throughput items <code>work_item_user_metrics_daily</code> <code>items_completed</code> Sum Items completed by user <code>churn</code> Code Churn loc <code>user_metrics_daily</code> <code>loc_touched</code> Sum LOC touched by user <code>wip_overlap</code> WIP Overlap items <code>work_item_user_metrics_daily</code> <code>wip_count_end_of_day</code> Avg Concurrent items in progress <code>blocked_work</code> Blocked Work items <code>work_item_cycle_times</code> <code>status='blocked'</code> Sum Items blocked while assigned"},{"location":"computations/canonical-metrics-prd/#database-schema-reference","title":"Database Schema Reference","text":""},{"location":"computations/canonical-metrics-prd/#core-tables","title":"Core Tables","text":"Table Purpose Key Columns <code>repo_metrics_daily</code> Repo-level git/PR stats <code>repo_id</code>, <code>day</code> <code>user_metrics_daily</code> Developer activity <code>repo_id</code>, <code>author_email</code>, <code>day</code> <code>work_item_metrics_daily</code> Work tracking aggregates <code>day</code>, <code>provider</code>, <code>work_scope_id</code>, <code>team_id</code> <code>work_item_user_metrics_daily</code> Developer delivery <code>day</code>, <code>provider</code>, <code>user_identity</code>, <code>team_id</code> <code>team_metrics_daily</code> Team well-being <code>team_id</code>, <code>day</code> <code>work_item_cycle_times</code> Per-item fact rows <code>provider</code>, <code>work_item_id</code> <code>work_item_state_durations_daily</code> Time in state <code>day</code>, <code>provider</code>, <code>work_scope_id</code>, <code>status</code> <code>deploy_metrics_daily</code> Deployment frequency <code>repo_id</code>, <code>day</code> <code>cicd_metrics_daily</code> CI/CD pipeline stats <code>repo_id</code>, <code>day</code>"},{"location":"computations/canonical-metrics-prd/#metric-calculation-details","title":"Metric Calculation Details","text":""},{"location":"computations/canonical-metrics-prd/#commit-size-bucketing","title":"Commit Size Bucketing","text":"<pre><code>total_loc = additions + deletions\n</code></pre> Bucket Range Small total_loc \u2264 50 Medium 51\u2013300 Large &gt; 300"},{"location":"computations/canonical-metrics-prd/#pr-cycle-time","title":"PR Cycle Time","text":"<p>For PRs merged on day D: <pre><code>cycle_time_hours = (merged_at - created_at) / 3600\n</code></pre></p> <p>Distribution fields: - <code>median_pr_cycle_hours</code> (p50) - <code>pr_cycle_p75_hours</code> - <code>pr_cycle_p90_hours</code></p>"},{"location":"computations/canonical-metrics-prd/#work-item-cycle-time","title":"Work Item Cycle Time","text":"<pre><code>lead_time_hours = completed_at - created_at\ncycle_time_hours = completed_at - started_at\n</code></pre> <ul> <li><code>started_at</code>: First transition to <code>in_progress</code></li> <li><code>completed_at</code>: First transition to <code>done</code> or <code>canceled</code></li> <li>Items missing <code>started_at</code> excluded from cycle-time distributions</li> </ul>"},{"location":"computations/canonical-metrics-prd/#large-change-thresholds","title":"Large Change Thresholds","text":"Type Threshold Large commit total_loc &gt; 300 Large PR additions + deletions \u2265 1000 (default)"},{"location":"computations/canonical-metrics-prd/#bus-factor","title":"Bus Factor","text":"<p>Smallest number of developers accounting for \u2265 50% of code churn.</p>"},{"location":"computations/canonical-metrics-prd/#code-ownership-gini","title":"Code Ownership Gini","text":"<p>Gini coefficient (0\u20131) of code contribution inequality. Higher = more concentrated ownership.</p>"},{"location":"computations/canonical-metrics-prd/#api-endpoints","title":"API Endpoints","text":""},{"location":"computations/canonical-metrics-prd/#apiv1explain","title":"<code>/api/v1/explain</code>","text":"<p>Detailed metric breakdown.</p> <p>Parameters: - <code>metric</code>: Key from registry - <code>scope_type</code>: <code>org</code>, <code>team</code>, or <code>repo</code> - <code>scope_id</code>: Identifier for scope - <code>range_days</code>: Lookback period - <code>compare_days</code>: Comparison period</p> <p>Response: - Current value - Delta % - Top drivers (teams/repos contributing) - Individual contributors (PRs/Issues)</p>"},{"location":"computations/canonical-metrics-prd/#apiv1home","title":"<code>/api/v1/home</code>","text":"<p>Executive dashboard with health signals.</p> <ul> <li>Computes deltas for all metrics</li> <li>Highlights \"Constraint of the Week\" (worst regression)</li> </ul>"},{"location":"computations/canonical-metrics-prd/#apiv1peopleperson_idmetric","title":"<code>/api/v1/people/{person_id}/metric</code>","text":"<p>Single person metric detail.</p> <p>Response: - Timeseries - Breakdown by dimension (repo, work_type, stage)</p>"},{"location":"computations/canonical-metrics-prd/#filter-patterns","title":"Filter Patterns","text":""},{"location":"computations/canonical-metrics-prd/#time-window","title":"Time Window","text":"Parameter Usage <code>start_day</code>, <code>end_day</code> Explicit date range (inclusive) <code>range_days</code> Lookback from today <code>compare_days</code> Comparison period duration"},{"location":"computations/canonical-metrics-prd/#scope-types","title":"Scope Types","text":"Type Filters On Notes <code>org</code> Entire organization No filtering <code>team</code> <code>team_id</code> column Join <code>repos</code> on <code>owner_team_id</code> for repo metrics <code>repo</code> <code>repo_id</code> column Direct filter on repo tables"},{"location":"computations/canonical-metrics-prd/#clickhouse-query-patterns","title":"ClickHouse Query Patterns","text":""},{"location":"computations/canonical-metrics-prd/#latest-value-per-key","title":"Latest Value per Key","text":"<p>Tables are append-only with <code>computed_at</code> versioning:</p> <pre><code>SELECT\n  day,\n  argMax(items_completed, computed_at) AS items_completed\nFROM work_item_metrics_daily\nWHERE provider = 'jira'\nGROUP BY day, provider, work_scope_id, team_id\nORDER BY day;\n</code></pre>"},{"location":"computations/canonical-metrics-prd/#daily-totals-two-step","title":"Daily Totals (Two-Step)","text":"<pre><code>SELECT\n  day,\n  sum(items_completed_latest) AS items_completed\nFROM (\n  SELECT\n    day,\n    argMax(items_completed, computed_at) AS items_completed_latest\n  FROM work_item_metrics_daily\n  GROUP BY day, provider, work_scope_id, team_id\n)\nGROUP BY day\nORDER BY day;\n</code></pre>"},{"location":"computations/canonical-metrics-prd/#null-behavior","title":"Null Behavior","text":"<ul> <li>Null fields indicate data unavailable, not zero</li> <li>Items without <code>started_at</code> excluded from cycle-time distributions</li> <li>Items without status history contribute no state duration rows</li> <li>Missing review facts leave pickup/review-time fields NULL</li> </ul>"},{"location":"computations/metric-calculations/","title":"Metric Calculation Details","text":"<p>Deep-dive into how metrics are computed, including formulas, edge cases, and implementation notes.</p>"},{"location":"computations/metric-calculations/#git-repository-metrics","title":"Git &amp; Repository Metrics","text":""},{"location":"computations/metric-calculations/#daily-user-metrics-user_metrics_daily","title":"Daily User Metrics (<code>user_metrics_daily</code>)","text":"<p>Key: <code>(repo_id, author_email, day)</code></p> <p>Note: <code>author_email</code> falls back to <code>author_name</code> when email is missing.</p> Metric Calculation <code>commits_count</code> Count of commits on day <code>loc_added</code> Sum of additions <code>loc_deleted</code> Sum of deletions <code>loc_touched</code> <code>loc_added + loc_deleted</code> <code>files_changed</code> Distinct files changed (union across day) <code>large_commits</code> Commits where <code>total_loc &gt; 300</code> <code>avg_commit_size</code> <code>loc_touched / commits_count</code> <p>PR Metrics (for PRs merged that day):</p> Metric Calculation <code>prs_authored</code> PRs created on day <code>prs_merged</code> PRs merged on day <code>pr_cycle_p50_hours</code> Median of <code>(merged_at - created_at)</code> <code>pr_cycle_p75_hours</code> 75th percentile <code>pr_cycle_p90_hours</code> 90th percentile <p>Collaboration Metrics (nullable):</p> Metric Calculation <code>pr_pickup_time_p50_hours</code> PR created \u2192 first interaction <code>pr_first_review_p50_hours</code> PR created \u2192 first review <code>pr_review_time_p50_hours</code> First review \u2192 merge <code>reviews_given</code> Count of review submissions <code>changes_requested_given</code> Count of change requests"},{"location":"computations/metric-calculations/#daily-repo-metrics-repo_metrics_daily","title":"Daily Repo Metrics (<code>repo_metrics_daily</code>)","text":"<p>Key: <code>(repo_id, day)</code></p> Metric Calculation <code>commits_count</code> Total commits <code>loc_touched</code> Sum of all additions + deletions <code>avg_commit_size</code> <code>loc_touched / commits_count</code> <code>large_commit_ratio</code> Large commits / total commits <code>prs_merged</code> PRs merged on day <code>pr_cycle_p50_hours</code> Median PR cycle time <code>pr_cycle_p75_hours</code> 75th percentile <code>pr_cycle_p90_hours</code> 90th percentile <p>Quality Metrics (best-effort):</p> Metric Calculation <code>large_pr_ratio</code> Large PRs / total PRs <code>pr_rework_ratio</code> PRs with multiple review rounds / total <code>bus_factor</code> Minimum developers for 50% of churn <code>code_ownership_gini</code> Gini coefficient of contribution"},{"location":"computations/metric-calculations/#work-item-metrics","title":"Work Item Metrics","text":""},{"location":"computations/metric-calculations/#normalization","title":"Normalization","text":"<p>Work items from all providers normalize to unified <code>WorkItem</code> model:</p> <pre><code>class WorkItem:\n    provider: str          # jira, github, gitlab\n    work_item_id: str      # Provider-native ID\n    work_scope_id: str     # Project/repo identifier\n    team_id: Optional[str] # Assigned team\n    created_at: datetime\n    started_at: Optional[datetime]   # First in_progress\n    completed_at: Optional[datetime] # First done/canceled\n</code></pre>"},{"location":"computations/metric-calculations/#status-normalization","title":"Status Normalization","text":"<p>Statuses map to canonical categories:</p> Category Meaning <code>backlog</code> Not yet scheduled <code>todo</code> Scheduled but not started <code>in_progress</code> Active work <code>in_review</code> Awaiting review <code>blocked</code> Work cannot proceed <code>done</code> Successfully completed <code>canceled</code> Abandoned"},{"location":"computations/metric-calculations/#daily-work-item-metrics-work_item_metrics_daily","title":"Daily Work Item Metrics (<code>work_item_metrics_daily</code>)","text":"<p>Key: <code>(day, provider, work_scope_id, team_id)</code></p> Metric Calculation <code>items_started</code> Items transitioning to <code>in_progress</code> <code>items_completed</code> Items transitioning to <code>done</code> or <code>canceled</code> <code>items_completed_unassigned</code> Completed items with no assignee <code>wip_count_end_of_day</code> Items in <code>in_progress</code>, <code>in_review</code>, <code>blocked</code> <code>cycle_time_p50_hours</code> Median <code>completed_at - started_at</code> <code>cycle_time_p90_hours</code> 90th percentile cycle time <code>lead_time_p50_hours</code> Median <code>completed_at - created_at</code> <code>lead_time_p90_hours</code> 90th percentile lead time <code>wip_age_p50_hours</code> Median age of WIP items <code>wip_age_p90_hours</code> 90th percentile WIP age <code>bug_completed_ratio</code> Bugs / total completed <code>story_points_completed</code> Sum of story points (Jira only) <code>predictability_score</code> <code>items_completed / (items_completed + wip_count)</code>"},{"location":"computations/metric-calculations/#work-scope-id-convention","title":"Work Scope ID Convention","text":"<p>Provider-native identifier for work scope:</p> Provider Format Example Jira Project key <code>ABC</code> GitHub Issues Repository full name <code>owner/repo</code> GitHub Projects v2 <code>ghprojv2:&lt;org&gt;#&lt;number&gt;</code> <code>ghprojv2:myorg#3</code> GitLab Project full path <code>group/project</code>"},{"location":"computations/metric-calculations/#team-well-being-metrics","title":"Team Well-Being Metrics","text":""},{"location":"computations/metric-calculations/#daily-team-metrics-team_metrics_daily","title":"Daily Team Metrics (<code>team_metrics_daily</code>)","text":"<p>Key: <code>(team_id, day)</code></p> <p>Computed from commits (deduplicated by hash) with team mapping.</p> Metric Calculation <code>after_hours_commit_ratio</code> Commits outside business hours on weekdays <code>weekend_commit_ratio</code> Commits on weekends <p>Configuration:</p> Variable Default Description <code>BUSINESS_TIMEZONE</code> <code>UTC</code> Timezone for hour calculation <code>BUSINESS_HOURS_START</code> <code>9</code> Start of business hours <code>BUSINESS_HOURS_END</code> <code>17</code> End of business hours"},{"location":"computations/metric-calculations/#ic-landscape-metrics","title":"IC Landscape Metrics","text":""},{"location":"computations/metric-calculations/#rolling-30-day-metrics-ic_landscape_rolling_30d","title":"Rolling 30-Day Metrics (<code>ic_landscape_rolling_30d</code>)","text":"<p>Normalized by team percentiles (0\u20131) for visualization.</p> Metric Calculation <code>churn_loc_30d</code> Rolling 30d sum of LOC touched <code>delivery_units_30d</code> <code>prs_merged + work_items_completed</code> <code>cycle_p50_30d_hours</code> Median of daily median PR cycle times <code>wip_max_30d</code> Max active work items over 30d"},{"location":"computations/metric-calculations/#landscape-maps","title":"Landscape Maps","text":"<p>Map 1: Churn \u00d7 Throughput - X: <code>log(churn_loc_30d)</code> - Y: <code>delivery_units_30d</code></p> <p>Map 2: Cycle Time \u00d7 Throughput - X: <code>log(cycle_p50_30d_hours)</code> - Y: <code>delivery_units_30d</code></p> <p>Map 3: WIP \u00d7 Throughput - X: <code>wip_max_30d</code> - Y: <code>delivery_units_30d</code></p> <p>Each coordinate normalized to team percentile rank.</p>"},{"location":"computations/metric-calculations/#code-complexity","title":"Code Complexity","text":""},{"location":"computations/metric-calculations/#daily-complexity-metrics-repo_complexity_daily","title":"Daily Complexity Metrics (<code>repo_complexity_daily</code>)","text":"<p>Computed via <code>radon</code> scanning historical git references.</p> Metric Calculation <code>cyclomatic_total</code> Sum of CC for all functions <code>cyclomatic_avg</code> Mean CC per function <code>high_complexity_functions</code> Functions with CC &gt; 15 <code>very_high_complexity_functions</code> Functions with CC &gt; 25"},{"location":"computations/metric-calculations/#investment-metrics","title":"Investment Metrics","text":""},{"location":"computations/metric-calculations/#daily-investment-investment_metrics_daily","title":"Daily Investment (<code>investment_metrics_daily</code>)","text":"<p>Categorizes effort into investment areas via rule-based classifier.</p> <p>Artifacts classified: - Work items: Based on labels, components, title keywords - Commits (churn): Based on file path patterns</p> Metric Calculation <code>investment_area</code> Assigned category <code>project_stream</code> Secondary grouping <code>delivery_units</code> Story points or item count <code>churn_loc</code> LOC associated with area <p>Configuration: <code>config/investment_areas.yaml</code></p>"},{"location":"computations/metric-calculations/#edge-cases-null-handling","title":"Edge Cases &amp; Null Handling","text":""},{"location":"computations/metric-calculations/#missing-data","title":"Missing Data","text":"Scenario Behavior No <code>started_at</code> Excluded from cycle-time distributions No status history No state duration rows No review facts Pickup/review fields remain NULL No assignee Counted under <code>team_id=''</code> and <code>user_identity='unassigned'</code>"},{"location":"computations/metric-calculations/#recomputation","title":"Recomputation","text":"<ul> <li>All metrics are append-only with <code>computed_at</code> timestamp</li> <li>Use <code>argMax(..., computed_at)</code> to get latest value</li> <li>Safe to recompute any day (idempotent via compound keys)</li> </ul>"},{"location":"llm/categorization-contract/","title":"LLM Categorization Contract","text":"<p>Rules and specifications for LLM usage in the Dev Health platform.</p>"},{"location":"llm/categorization-contract/#overview","title":"Overview","text":"<p>LLMs are used in two contexts:</p> Context When Purpose Constraints Compute-time During data processing Categorize work into investment themes Strict schema, persisted UX-time On user request Explain persisted categorizations Read-only, no recomputation"},{"location":"llm/categorization-contract/#compute-time-categorization","title":"Compute-Time Categorization","text":""},{"location":"llm/categorization-contract/#purpose","title":"Purpose","text":"<p>Map messy human text to canonical investment categories with subcategory distributions.</p>"},{"location":"llm/categorization-contract/#schema-compliance","title":"Schema Compliance","text":"<p>Output MUST be strict JSON matching: <pre><code>work_graph/investment/llm_schema.py\n</code></pre></p>"},{"location":"llm/categorization-contract/#output-requirements","title":"Output Requirements","text":"Requirement Details Keys From canonical subcategory registry only Probabilities Valid (0\u20131), normalized Evidence Extractive substrings from input text Theme roll-up Computed deterministically from subcategories"},{"location":"llm/categorization-contract/#example-output","title":"Example Output","text":"<pre><code>{\n  \"subcategories\": {\n    \"operational.external\": 0.45,\n    \"operational.incident\": 0.25,\n    \"feature_delivery.customer\": 0.20,\n    \"maintenance.refactor\": 0.10\n  },\n  \"evidence\": {\n    \"operational.external\": [\"customer-facing issue\", \"support ticket\"],\n    \"operational.incident\": [\"incident response\", \"outage window\"],\n    \"feature_delivery.customer\": [\"requested by customer\"],\n    \"maintenance.refactor\": [\"cleanup\", \"technical debt\"]\n  },\n  \"uncertainty\": \"moderate\"\n}\n</code></pre>"},{"location":"llm/categorization-contract/#two-stage-process","title":"Two-Stage Process","text":"<ol> <li>LLM Stage: Map text \u2192 subcategory distribution</li> <li>Deterministic Stage: Roll subcategories \u2192 themes (no LLM)</li> </ol> <p>This separation prevents category drift.</p>"},{"location":"llm/categorization-contract/#retry-policy","title":"Retry Policy","text":""},{"location":"llm/categorization-contract/#when-to-retry","title":"When to Retry","text":"<ul> <li>Whitespace/empty response</li> <li>Truncated response (<code>finish_reason=length</code>)</li> <li>Invalid JSON structure</li> <li>Missing required keys</li> </ul>"},{"location":"llm/categorization-contract/#retry-strategy","title":"Retry Strategy","text":"<ol> <li>First attempt: Standard prompt, standard tokens</li> <li>Retry attempt:</li> <li>Double <code>max_completion_tokens</code> (minimum 512)</li> <li>Simplify and harden JSON prompt</li> <li>Add explicit JSON instruction in system AND user message</li> </ol>"},{"location":"llm/categorization-contract/#failure-handling","title":"Failure Handling","text":"<p>After one retry failure: 1. Mark categorization as invalid 2. Apply deterministic fallback 3. Persist with <code>fallback_applied=true</code></p>"},{"location":"llm/categorization-contract/#audit-fields","title":"Audit Fields","text":"<p>Every categorization run must persist:</p> Field Description <code>categorized_at</code> Timestamp <code>model_version</code> LLM model identifier <code>prompt_hash</code> Hash of prompt template <code>raw_response</code> Original LLM output <code>fallback_applied</code> Boolean <code>retry_count</code> Number of retries <code>finish_reason</code> OpenAI finish reason <code>token_usage</code> Tokens consumed"},{"location":"llm/categorization-contract/#openai-specific-handling","title":"OpenAI-Specific Handling","text":""},{"location":"llm/categorization-contract/#json-mode","title":"JSON Mode","text":"<p>Include explicit JSON instruction in both: - System message - User message</p>"},{"location":"llm/categorization-contract/#token-configuration","title":"Token Configuration","text":"<ul> <li>Use <code>max_completion_tokens</code> (not <code>max_tokens</code>)</li> <li>Minimum: 512 tokens</li> <li>Double on retry</li> </ul>"},{"location":"llm/categorization-contract/#observability","title":"Observability","text":"<p>Log on every call: - <code>finish_reason</code> - <code>content_length</code> - Token parameters - Response time</p>"},{"location":"llm/categorization-contract/#ux-time-explanation","title":"UX-Time Explanation","text":""},{"location":"llm/categorization-contract/#purpose_1","title":"Purpose","text":"<p>Generate human-readable explanations of persisted categorizations.</p>"},{"location":"llm/categorization-contract/#constraints","title":"Constraints","text":"Allowed Forbidden Read persisted distributions Recompute categories Read stored evidence Change edges/weights Generate narrative text Introduce new conclusions Cite specific evidence Modify persisted decisions"},{"location":"llm/categorization-contract/#required-labeling","title":"Required Labeling","text":"<p>All explanation output MUST be labeled as AI-generated.</p>"},{"location":"llm/categorization-contract/#explanation-prompt","title":"Explanation Prompt","text":"<p>Canonical prompt (use verbatim):</p> <pre><code>You are explaining a precomputed investment view.\n\nYou are not allowed to:\n- Recalculate scores\n- Change categories\n- Introduce new conclusions\n- Be conversational (no \"Hello\", \"As an AI\", or interactive follow-ups)\n\nExplain the investment view in three distinct sections:\n\n1. **SUMMARY**: Provide a high-level narrative (max 3 sentences) using\n   probabilistic language (appears, leans, suggests) explaining why\n   the work leans toward the primary categories.\n\n2. **REASONS**: List the specific evidence (structural, contextual,\n   textual) that contributed most to this interpretation.\n\n3. **UNCERTAINTY**: Disclose where uncertainty exists based on the\n   evidence quality and evidence mix.\n\nAlways include evidence quality level and limits.\n</code></pre>"},{"location":"llm/categorization-contract/#language-rules","title":"Language Rules","text":""},{"location":"llm/categorization-contract/#allowed-language","title":"Allowed Language","text":"<p>Use probabilistic, uncertain phrasing:</p> <ul> <li>appears</li> <li>leans</li> <li>suggests</li> <li>indicates</li> <li>may be</li> </ul>"},{"location":"llm/categorization-contract/#forbidden-language","title":"Forbidden Language","text":"<p>Avoid definitive, deterministic phrasing:</p> <ul> <li>is</li> <li>was</li> <li>detected</li> <li>determined</li> <li>definitely</li> <li>clearly</li> </ul>"},{"location":"llm/categorization-contract/#rationale","title":"Rationale","text":"<p>The distinction maintains appropriate uncertainty. LLM categorization is inference, not detection.</p>"},{"location":"llm/categorization-contract/#evidence-handling","title":"Evidence Handling","text":""},{"location":"llm/categorization-contract/#extractive-quotes","title":"Extractive Quotes","text":"<p>Evidence quotes MUST be: - Direct substrings from input text - Not paraphrased - Not summarized - Traceable to source</p>"},{"location":"llm/categorization-contract/#evidence-types","title":"Evidence Types","text":"Type Source Example Textual Issue/PR title, description, commits \"hotfix for production bug\" Structural Relationships, links \"Linked to incident #123\" Contextual Timing, patterns \"Merged during outage window\""},{"location":"llm/categorization-contract/#forbidden-patterns","title":"Forbidden Patterns","text":""},{"location":"llm/categorization-contract/#do-not","title":"Do Not","text":"<ul> <li>Invent categories not in canonical list</li> <li>Use free-form reasoning in output</li> <li>Override canonical vocabulary</li> <li>Return \"unknown\" or \"uncategorized\"</li> <li>Hallucinate evidence not in input</li> <li>Apply categories based on author identity</li> </ul>"},{"location":"llm/categorization-contract/#immediate-failure-conditions","title":"Immediate Failure Conditions","text":"<ul> <li>Output contains non-canonical keys</li> <li>Probabilities don't sum correctly</li> <li>Evidence quotes not found in input</li> <li>Missing required output sections</li> </ul>"},{"location":"llm/categorization-contract/#testing","title":"Testing","text":""},{"location":"llm/categorization-contract/#unit-tests-must-cover","title":"Unit Tests Must Cover","text":"<ul> <li>Valid JSON output parsing</li> <li>Probability normalization</li> <li>Evidence extraction validation</li> <li>Retry logic</li> <li>Fallback application</li> <li>Audit field persistence</li> </ul>"},{"location":"llm/categorization-contract/#mock-requirements","title":"Mock Requirements","text":"<ul> <li>Mock LLM API responses</li> <li>Test various failure modes</li> <li>Verify retry behavior</li> <li>Test fallback categorization</li> </ul>"},{"location":"ops/cli-reference/","title":"CLI Reference","text":"<p>Complete reference for the dev-health-ops command-line interface.</p>"},{"location":"ops/cli-reference/#overview","title":"Overview","text":"<p>The CLI is implemented in <code>cli.py</code> and orchestrates: - Data synchronization from providers - Metric computation - Fixture generation - Team management</p>"},{"location":"ops/cli-reference/#global-arguments","title":"Global Arguments","text":"Argument Environment Variable Description <code>--db</code> <code>DATABASE_URI</code> Database connection string <code>--sink</code> \u2014 Output target: <code>primary</code>, <code>secondary</code>, <code>both</code>"},{"location":"ops/cli-reference/#database-connection-strings","title":"Database Connection Strings","text":"Backend Format Example PostgreSQL <code>postgresql+asyncpg://</code> <code>postgresql+asyncpg://localhost:5432/db</code> ClickHouse <code>clickhouse://</code> <code>clickhouse://localhost:8123/default</code> MongoDB <code>mongodb://</code> <code>mongodb://localhost:27017/db</code> SQLite <code>sqlite+aiosqlite://</code> <code>sqlite+aiosqlite:///./data.db</code>"},{"location":"ops/cli-reference/#sync-commands","title":"Sync Commands","text":""},{"location":"ops/cli-reference/#sync-git","title":"<code>sync git</code>","text":"<p>Sync git repository data.</p> <pre><code># Local repository\npython cli.py sync git --provider local \\\n  --db \"$DATABASE_URI\" \\\n  --repo-path /path/to/repo\n\n# GitHub\npython cli.py sync git --provider github \\\n  --db \"$DATABASE_URI\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner torvalds \\\n  --repo linux\n\n# GitLab\npython cli.py sync git --provider gitlab \\\n  --db \"$DATABASE_URI\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --project-id 278964\n</code></pre> <p>Options: | Option | Description | |--------|-------------| | <code>--provider</code> | <code>local</code>, <code>github</code>, <code>gitlab</code> | | <code>--repo-path</code> | Path to local repo | | <code>--owner</code>, <code>--repo</code> | GitHub owner/repo | | <code>--project-id</code> | GitLab project ID | | <code>--since</code>, <code>--date</code> | Date filter | | <code>--backfill N</code> | Days to backfill |</p>"},{"location":"ops/cli-reference/#sync-prs","title":"<code>sync prs</code>","text":"<p>Sync pull request data.</p> <pre><code>python cli.py sync prs --provider github \\\n  --db \"$DATABASE_URI\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner org \\\n  --repo repo\n</code></pre>"},{"location":"ops/cli-reference/#sync-work-items","title":"<code>sync work-items</code>","text":"<p>Sync work items from issue trackers.</p> <pre><code># All providers\npython cli.py sync work-items --provider all \\\n  --db \"$DATABASE_URI\" \\\n  --date 2025-02-01 \\\n  --backfill 30\n\n# Jira only\npython cli.py sync work-items --provider jira \\\n  --db \"$DATABASE_URI\"\n\n# GitHub with pattern\npython cli.py sync work-items --provider github \\\n  --db \"$DATABASE_URI\" \\\n  -s \"org/*\"\n</code></pre> <p>Providers: <code>jira</code>, <code>github</code>, <code>gitlab</code>, <code>synthetic</code>, <code>all</code></p>"},{"location":"ops/cli-reference/#sync-cicd","title":"<code>sync cicd</code>","text":"<p>Sync CI/CD pipeline data.</p> <pre><code># GitHub\npython cli.py sync cicd --provider github \\\n  --db \"$DATABASE_URI\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner org \\\n  --repo repo\n\n# GitLab\npython cli.py sync cicd --provider gitlab \\\n  --db \"$DATABASE_URI\" \\\n  --auth \"$GITLAB_TOKEN\" \\\n  --gitlab-url \"https://gitlab.com\" \\\n  --project-id 123\n</code></pre>"},{"location":"ops/cli-reference/#sync-deployments","title":"<code>sync deployments</code>","text":"<p>Sync deployment events.</p> <pre><code>python cli.py sync deployments --provider github \\\n  --db \"$DATABASE_URI\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner org \\\n  --repo repo\n</code></pre>"},{"location":"ops/cli-reference/#sync-incidents","title":"<code>sync incidents</code>","text":"<p>Sync incident data.</p> <pre><code>python cli.py sync incidents --provider github \\\n  --db \"$DATABASE_URI\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner org \\\n  --repo repo\n</code></pre>"},{"location":"ops/cli-reference/#sync-teams","title":"<code>sync teams</code>","text":"<p>Sync team definitions.</p> <pre><code># From config file\npython cli.py sync teams --path config/team_mapping.yaml\n\n# From Jira projects\npython cli.py sync teams --provider jira\n\n# Synthetic teams\npython cli.py sync teams --provider synthetic\n</code></pre>"},{"location":"ops/cli-reference/#metrics-commands","title":"Metrics Commands","text":""},{"location":"ops/cli-reference/#metrics-daily","title":"<code>metrics daily</code>","text":"<p>Compute daily metrics.</p> <pre><code># Single day\npython cli.py metrics daily \\\n  --db \"$DATABASE_URI\" \\\n  --date 2025-02-01\n\n# With backfill\npython cli.py metrics daily \\\n  --db \"$DATABASE_URI\" \\\n  --date 2025-02-01 \\\n  --backfill 7\n\n# Filter to one repo\npython cli.py metrics daily \\\n  --db \"$DATABASE_URI\" \\\n  --date 2025-02-01 \\\n  --repo-id &lt;uuid&gt;\n</code></pre> <p>Options: | Option | Description | |--------|-------------| | <code>--date</code> | Target date | | <code>--backfill N</code> | Compute N days ending at date | | <code>--repo-id</code> | Filter to specific repository |</p>"},{"location":"ops/cli-reference/#fixtures-commands","title":"Fixtures Commands","text":""},{"location":"ops/cli-reference/#fixtures-generate","title":"<code>fixtures generate</code>","text":"<p>Generate synthetic test data.</p> <pre><code>python cli.py fixtures generate \\\n  --db \"$DATABASE_URI\" \\\n  --days 30\n</code></pre> <p>Options: | Option | Description | |--------|-------------| | <code>--days N</code> | Number of days to generate | | <code>--teams N</code> | Number of teams | | <code>--repos-per-team N</code> | Repos per team |</p>"},{"location":"ops/cli-reference/#batch-processing-options","title":"Batch Processing Options","text":"<p>For GitHub/GitLab batch operations:</p> Option Description <code>-s, --search PATTERN</code> Glob pattern for repos <code>--group NAME</code> Organization/group name <code>--batch-size N</code> Records per batch <code>--max-concurrent N</code> Concurrent workers <code>--max-repos N</code> Maximum repos to process <code>--use-async</code> Enable async workers <code>--rate-limit-delay SECONDS</code> Delay between requests"},{"location":"ops/cli-reference/#environment-variables","title":"Environment Variables","text":""},{"location":"ops/cli-reference/#database","title":"Database","text":"Variable Description <code>DATABASE_URI</code> Primary database connection <code>SECONDARY_DATABASE_URI</code> Secondary sink (with <code>--sink both</code>) <code>DB_ECHO</code> Enable SQL logging"},{"location":"ops/cli-reference/#provider-auth","title":"Provider Auth","text":"Variable Provider <code>GITHUB_TOKEN</code> GitHub <code>GITLAB_TOKEN</code> GitLab <code>JIRA_EMAIL</code> Jira <code>JIRA_API_TOKEN</code> Jira <code>JIRA_BASE_URL</code> Jira"},{"location":"ops/cli-reference/#tuning","title":"Tuning","text":"Variable Default Description <code>BATCH_SIZE</code> 100 Records per batch <code>MAX_WORKERS</code> 4 Parallel workers"},{"location":"ops/cli-reference/#workflow-examples","title":"Workflow Examples","text":""},{"location":"ops/cli-reference/#full-sync-pipeline","title":"Full Sync Pipeline","text":"<pre><code># 1. Sync git data\npython cli.py sync git --provider github \\\n  --db \"$DATABASE_URI\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  --owner myorg \\\n  --repo myrepo\n\n# 2. Sync work items\npython cli.py sync work-items --provider jira \\\n  --db \"$DATABASE_URI\" \\\n  --date 2025-02-01 \\\n  --backfill 30\n\n# 3. Compute metrics\npython cli.py metrics daily \\\n  --db \"$DATABASE_URI\" \\\n  --date 2025-02-01 \\\n  --backfill 30\n</code></pre>"},{"location":"ops/cli-reference/#local-development","title":"Local Development","text":"<pre><code># Generate synthetic data\npython cli.py fixtures generate \\\n  --db \"sqlite+aiosqlite:///./dev.db\" \\\n  --days 30\n\n# Compute metrics\npython cli.py metrics daily \\\n  --db \"sqlite+aiosqlite:///./dev.db\" \\\n  --backfill 30\n</code></pre>"},{"location":"ops/cli-reference/#batch-organization-sync","title":"Batch Organization Sync","text":"<pre><code># Sync all repos in org\npython cli.py sync git --provider github \\\n  --db \"$DATABASE_URI\" \\\n  --auth \"$GITHUB_TOKEN\" \\\n  -s \"myorg/*\" \\\n  --group myorg \\\n  --max-concurrent 4 \\\n  --use-async\n</code></pre>"},{"location":"ops/cli-reference/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 General error 2 Configuration error 3 Authentication error 4 Rate limit exceeded"},{"location":"ops/connector-inventory/","title":"Connector Inventory and Implementation Status","text":"<p>This document provides a comprehensive audit of all data connectors (GitHub, GitLab, Jira) including their current implementation status, missing features, sync capabilities, and recommendations.</p>"},{"location":"ops/connector-inventory/#executive-summary","title":"Executive Summary","text":"Provider Work Items Git Data CI/CD Deployments Incidents Sprints Overall GitHub Full Full Full Full Partial Full (Milestones) 90% GitLab Full Full Full Full Partial Full (Milestones) 90% Jira Full N/A N/A N/A N/A Full 95%"},{"location":"ops/connector-inventory/#1-github-connector","title":"1. GitHub Connector","text":""},{"location":"ops/connector-inventory/#11-architecture-overview","title":"1.1 Architecture Overview","text":"<p>Location: <code>src/dev_health_ops/</code> - Provider: <code>providers/github/provider.py</code> (work items) - Client: <code>providers/github/client.py</code> (PyGithub + GraphQL) - Processor: <code>processors/github.py</code> (git data sync) - Connector: <code>connectors/github.py</code> (low-level API) - Normalizer: <code>providers/github/normalize.py</code></p>"},{"location":"ops/connector-inventory/#12-implemented-data-types","title":"1.2 Implemented Data Types","text":"Data Type Status API Used Notes Repository metadata Implemented REST Full support Commits Implemented REST Up to 100 recent commits Commit stats (per-file) Implemented REST Last 50 commits (rate limits) Pull requests Implemented REST Full history with pagination PR reviews Implemented REST Includes review comments PR comments Implemented REST Issue + review comments Issues Implemented REST Full work item support Issue events Implemented REST For status transitions Milestones Implemented REST Mapped to sprints Workflow runs (CI/CD) Implemented REST GitHub Actions Deployments Implemented REST Environment deployments Incidents Partial REST Issues labeled \"incident\" only Projects v2 Implemented GraphQL Full field tracking + changes Blame Implemented GraphQL Per-file blame ranges File tree Implemented REST For backfill operations"},{"location":"ops/connector-inventory/#13-authentication","title":"1.3 Authentication","text":"Method Support Environment Variable Personal Access Token Full <code>GITHUB_TOKEN</code> GitHub Enterprise Full <code>GITHUB_BASE_URL</code> GitHub App Not implemented - OAuth App Not implemented - <p>Required Token Scopes: - <code>repo</code> - Required for private repositories - <code>read:org</code> - Recommended for organization repos</p>"},{"location":"ops/connector-inventory/#14-rate-limiting","title":"1.4 Rate Limiting","text":"Feature Implementation REST API (5,000/hr) Exponential backoff with shared gate GraphQL API Cost-based tracking Retry-After header Honored Concurrent request coordination Via <code>RateLimitGate</code>"},{"location":"ops/connector-inventory/#15-missing-implementations","title":"1.5 Missing Implementations","text":"Feature Priority Effort Notes GitHub App authentication Medium Medium Better for org-wide access Discussions Low Low Not commonly needed for metrics Wiki changes Low Low Rarely impacts dev health Security advisories Medium Medium Useful for security metrics Code scanning alerts Medium Medium Useful for quality metrics Dependabot alerts Medium Medium Useful for maintenance metrics Repository topics/tags Low Low Enrichment data Webhooks (real-time) High High Would enable real-time sync Branch protection status Low Low Governance metrics"},{"location":"ops/connector-inventory/#16-known-issues","title":"1.6 Known Issues","text":"<ol> <li>Commit depth limit: REST API limited to recent commits; full history requires clone</li> <li>Blame API costs: Per-file API calls; expensive for large repos</li> <li>Rate limit sharing: Multiple concurrent syncs can exhaust limits</li> <li>Incident detection: Only detects issues with \"incident\" label</li> </ol>"},{"location":"ops/connector-inventory/#17-recommendations","title":"1.7 Recommendations","text":"<ol> <li>Implement webhooks for near-real-time data ingestion</li> <li>Add security/Dependabot alert sync for risk metrics</li> <li>Consider GitHub App auth for better rate limits and org access</li> <li>Add commit signature verification for security audit</li> </ol>"},{"location":"ops/connector-inventory/#2-gitlab-connector","title":"2. GitLab Connector","text":""},{"location":"ops/connector-inventory/#21-architecture-overview","title":"2.1 Architecture Overview","text":"<p>Location: <code>src/dev_health_ops/</code> - Provider: <code>providers/gitlab/provider.py</code> (work items) - Client: <code>providers/gitlab/client.py</code> (python-gitlab) - Processor: <code>processors/gitlab.py</code> (git data sync) - Connector: <code>connectors/gitlab.py</code> (low-level API) - Normalizer: <code>providers/gitlab/normalize.py</code></p>"},{"location":"ops/connector-inventory/#22-implemented-data-types","title":"2.2 Implemented Data Types","text":"Data Type Status API Used Notes Project metadata Implemented REST Full support Commits Implemented REST Paginated with date filtering Commit stats Implemented REST Aggregate per commit Merge requests Implemented REST Full history with pagination MR notes/comments Implemented REST Full comment history Issues Implemented REST Full work item support Issue links Implemented REST For dependencies Resource label events Implemented REST Status transitions Resource state events Implemented REST Reopen detection Project milestones Implemented REST Mapped to sprints Group milestones Implemented REST Mapped to sprints Pipelines (CI/CD) Implemented REST Full pipeline history Deployments Implemented REST Environment deployments Incidents Partial REST Issues labeled \"incident\" only Blame Implemented REST Per-file blame Repository tree Implemented REST For file listing"},{"location":"ops/connector-inventory/#23-authentication","title":"2.3 Authentication","text":"Method Support Environment Variable Personal Access Token Full <code>GITLAB_TOKEN</code> Self-hosted instances Full <code>GITLAB_URL</code> Group Access Token Supported <code>GITLAB_TOKEN</code> OAuth tokens Not implemented - Deploy tokens Not implemented - <p>Required Token Scopes: - <code>read_api</code> - Required for API access - <code>read_repository</code> - Required for repository data</p>"},{"location":"ops/connector-inventory/#24-rate-limiting","title":"2.4 Rate Limiting","text":"Feature Implementation REST API (10 req/sec) Exponential backoff with shared gate Retry-After header Honored Concurrent request coordination Via <code>RateLimitGate</code>"},{"location":"ops/connector-inventory/#25-missing-implementations","title":"2.5 Missing Implementations","text":"Feature Priority Effort Notes GraphQL API usage Medium Medium More efficient for some queries Epics Medium Medium GitLab Premium feature MR approvals Medium Low Review metrics MR diff stats Low Low Already have aggregate stats Package registry Low Medium Not commonly needed Container registry Low Medium Not commonly needed Webhooks (real-time) High High Would enable real-time sync Value stream analytics Medium Medium Pre-computed metrics DORA metrics API High Medium Built-in DevOps metrics"},{"location":"ops/connector-inventory/#26-known-issues","title":"2.6 Known Issues","text":"<ol> <li>Commit stats aggregate only: Per-file stats not available via API</li> <li>Pipeline job details: Only pipeline-level status, not job-level</li> <li>Incident detection: Only detects issues with \"incident\" label</li> <li>Self-hosted rate limits: May vary from gitlab.com defaults</li> </ol>"},{"location":"ops/connector-inventory/#27-recommendations","title":"2.7 Recommendations","text":"<ol> <li>Integrate DORA metrics API for pre-computed deployment metrics</li> <li>Add webhook support for real-time data ingestion</li> <li>Consider GraphQL migration for more efficient queries</li> <li>Add Epic sync for portfolio-level tracking</li> <li>Add MR approval sync for review metrics</li> </ol>"},{"location":"ops/connector-inventory/#3-jira-connector","title":"3. Jira Connector","text":""},{"location":"ops/connector-inventory/#31-architecture-overview","title":"3.1 Architecture Overview","text":"<p>Location: <code>src/dev_health_ops/</code> - Provider: <code>providers/jira/provider.py</code> (work items) - Client: <code>providers/jira/client.py</code> (custom REST) - Normalizer: <code>providers/jira/normalize.py</code></p> <p>External: <code>atlassian/</code> subproject for GraphQL Gateway (AGG)</p>"},{"location":"ops/connector-inventory/#32-implemented-data-types","title":"3.2 Implemented Data Types","text":"Data Type Status API Used Notes Issues Implemented REST v3 Full JQL support Issue changelog Implemented REST v3 Status transitions Issue links Implemented REST v3 Dependencies Issue comments Implemented REST v3 Interaction events Projects Implemented REST v3 For team mapping Sprints Implemented Agile REST Sprint metadata Custom fields Implemented REST v3 Story points, epic links Status categories Implemented REST v3 Normalized to canonical"},{"location":"ops/connector-inventory/#33-authentication","title":"3.3 Authentication","text":"Method Support Environment Variables API Token (Cloud) Full <code>JIRA_EMAIL</code>, <code>JIRA_API_TOKEN</code>, <code>JIRA_BASE_URL</code> OAuth Not implemented - Personal Access Token Not implemented -"},{"location":"ops/connector-inventory/#34-rate-limiting","title":"3.4 Rate Limiting","text":"Feature Implementation HTTP 429 handling Exponential backoff Retry-After header Honored Concurrent request coordination Via <code>RateLimitGate</code>"},{"location":"ops/connector-inventory/#35-jira-configuration-options","title":"3.5 Jira Configuration Options","text":"Variable Purpose Default <code>JIRA_PROJECT_KEYS</code> Filter to specific projects All accessible <code>JIRA_JQL</code> Custom JQL override Built-in windowed query <code>JIRA_FETCH_ALL</code> Ignore date window <code>false</code> <code>JIRA_FETCH_COMMENTS</code> Include comments <code>true</code> <code>JIRA_COMMENTS_LIMIT</code> Max comments per issue <code>0</code> (unlimited) <code>JIRA_STORY_POINTS_FIELD</code> Custom field ID Auto-detect <code>JIRA_SPRINT_FIELD</code> Custom field ID <code>customfield_10020</code> <code>JIRA_EPIC_LINK_FIELD</code> Custom field ID Auto-detect"},{"location":"ops/connector-inventory/#36-missing-implementations","title":"3.6 Missing Implementations","text":"Feature Priority Effort Notes Jira Server/DC support Medium Medium Different auth model Boards API Low Low Board configuration Agile estimation Low Low Planning poker data Tempo worklogs Medium Medium Time tracking integration Advanced Roadmaps Medium High Portfolio data Webhooks (real-time) High High Would enable real-time sync Assets/Insight Low Medium CMDB data GraphQL Gateway (AGG) Medium High More efficient queries"},{"location":"ops/connector-inventory/#37-known-issues","title":"3.7 Known Issues","text":"<ol> <li>Changelog truncation: Large issues may have truncated changelogs</li> <li>Custom field discovery: Requires manual configuration per instance</li> <li>Cloud API changes: <code>/rest/api/3/search</code> deprecated for <code>/search/jql</code></li> <li>Rate limits vary: Different limits for different Jira plans</li> </ol>"},{"location":"ops/connector-inventory/#38-recommendations","title":"3.8 Recommendations","text":"<ol> <li>Add Jira Server/DC support for on-premise deployments</li> <li>Integrate AGG (GraphQL) from <code>atlassian/</code> subproject</li> <li>Add webhook support for real-time updates</li> <li>Implement Tempo integration for time tracking</li> <li>Add bulk change detection for large backlogs</li> </ol>"},{"location":"ops/connector-inventory/#4-cross-connector-issues","title":"4. Cross-Connector Issues","text":""},{"location":"ops/connector-inventory/#41-common-gaps","title":"4.1 Common Gaps","text":"Gap Impact Recommendation No real-time sync Data staleness Implement webhooks Manual identity mapping User fragmentation Auto-detect common patterns No incremental blame Slow backfills Cache blame data Limited incident detection Incomplete metrics Add dedicated incident providers"},{"location":"ops/connector-inventory/#42-data-model-alignment","title":"4.2 Data Model Alignment","text":"<p>All connectors normalize to unified models: - <code>WorkItem</code> - Issues, PRs, MRs - <code>WorkItemStatusTransition</code> - State changes - <code>WorkItemDependency</code> - Links/blockers - <code>WorkItemInteractionEvent</code> - Comments/reviews - <code>Sprint</code> - Iterations/milestones - <code>GitCommit</code>, <code>GitPullRequest</code>, etc. - Git data</p>"},{"location":"ops/connector-inventory/#43-rate-limit-coordination","title":"4.3 Rate Limit Coordination","text":"<p>Shared <code>RateLimitGate</code> infrastructure ensures: - Exponential backoff on 429s - Retry-After header compliance - Cross-worker coordination</p>"},{"location":"ops/connector-inventory/#5-implementation-roadmap","title":"5. Implementation Roadmap","text":""},{"location":"ops/connector-inventory/#phase-1-high-priority-recommended","title":"Phase 1: High Priority (Recommended)","text":"<ol> <li>Webhook ingestion framework</li> <li>Real-time data for all providers</li> <li> <p>Reduces sync latency from hours to seconds</p> </li> <li> <p>Security/compliance data</p> </li> <li>GitHub: Security advisories, Dependabot</li> <li>GitLab: Vulnerability reports</li> <li> <p>Impact: Risk/Security investment category</p> </li> <li> <p>DORA metrics integration</p> </li> <li>GitLab DORA API</li> <li>GitHub deployment frequency</li> <li>Impact: Pre-computed DevOps metrics</li> </ol>"},{"location":"ops/connector-inventory/#phase-2-medium-priority","title":"Phase 2: Medium Priority","text":"<ol> <li>Authentication improvements</li> <li>GitHub App auth</li> <li>Jira Server/DC support</li> <li> <p>OAuth flows for all providers</p> </li> <li> <p>AGG integration for Jira</p> </li> <li>Use GraphQL Gateway from <code>atlassian/</code> subproject</li> <li> <p>More efficient bulk queries</p> </li> <li> <p>Epic/Portfolio tracking</p> </li> <li>GitLab Epics</li> <li>Jira Advanced Roadmaps</li> <li>GitHub Projects hierarchy</li> </ol>"},{"location":"ops/connector-inventory/#phase-3-low-priority","title":"Phase 3: Low Priority","text":"<ol> <li>Additional data enrichment</li> <li>Repository topics/tags</li> <li>Code quality metrics</li> <li>Test coverage integration</li> </ol>"},{"location":"ops/connector-inventory/#6-testing-recommendations","title":"6. Testing Recommendations","text":""},{"location":"ops/connector-inventory/#unit-tests-required","title":"Unit Tests Required","text":"<ul> <li>[ ] Rate limit handling for each provider</li> <li>[ ] Pagination edge cases</li> <li>[ ] Error recovery scenarios</li> <li>[ ] Custom field mapping</li> </ul>"},{"location":"ops/connector-inventory/#integration-tests-required","title":"Integration Tests Required","text":"<ul> <li>[ ] Full sync cycle for each provider</li> <li>[ ] Incremental sync validation</li> <li>[ ] Multi-provider identity correlation</li> <li>[ ] Large repository handling</li> </ul>"},{"location":"ops/connector-inventory/#performance-tests-required","title":"Performance Tests Required","text":"<ul> <li>[ ] Concurrent sync capacity</li> <li>[ ] Memory usage under load</li> <li>[ ] Rate limit recovery time</li> </ul>"},{"location":"ops/deployment-guide/","title":"Deployment Guide","text":"<p>This guide covers deploying dev-health-ops across different container orchestration platforms.</p>"},{"location":"ops/deployment-guide/#quick-links","title":"Quick Links","text":"Platform Config Location Quick Start Kubernetes <code>deploy/kubernetes/</code> Jump to section Docker Compose <code>deploy/docker-compose/</code> Jump to section Docker Swarm <code>deploy/docker-swarm/</code> Jump to section Local Development <code>compose.yml</code> Jump to section"},{"location":"ops/deployment-guide/#prerequisites","title":"Prerequisites","text":""},{"location":"ops/deployment-guide/#required","title":"Required","text":"<ul> <li>Docker 20.10+ (for all deployment methods)</li> <li>Container registry access (for pulling images)</li> <li>Network access to GitHub, GitLab, and/or Jira APIs</li> </ul>"},{"location":"ops/deployment-guide/#platform-specific","title":"Platform-Specific","text":"Platform Requirements Kubernetes kubectl, Kubernetes 1.25+, Ingress Controller Docker Compose Docker Compose V2 Docker Swarm Docker Swarm initialized"},{"location":"ops/deployment-guide/#environment-variables","title":"Environment Variables","text":"<p>All deployment methods use the same environment variables:</p>"},{"location":"ops/deployment-guide/#database","title":"Database","text":"Variable Description Default <code>DATABASE_URI</code> Primary database connection Required <code>SECONDARY_DATABASE_URI</code> Secondary sink (optional) -"},{"location":"ops/deployment-guide/#provider-credentials","title":"Provider Credentials","text":"Variable Description <code>GITHUB_TOKEN</code> GitHub Personal Access Token <code>GITLAB_TOKEN</code> GitLab Private Token <code>GITLAB_URL</code> GitLab instance URL (default: gitlab.com) <code>JIRA_BASE_URL</code> Jira Cloud URL (e.g., your-org.atlassian.net) <code>JIRA_EMAIL</code> Jira account email <code>JIRA_API_TOKEN</code> Jira API token"},{"location":"ops/deployment-guide/#application","title":"Application","text":"Variable Description Default <code>GRAPHQL_QUERY_TIMEOUT</code> GraphQL timeout (seconds) 30 <code>LOG_LEVEL</code> Logging verbosity INFO <code>BATCH_SIZE</code> Records per batch 100 <code>MAX_WORKERS</code> Parallel workers 4"},{"location":"ops/deployment-guide/#kubernetes-deployment","title":"Kubernetes Deployment","text":""},{"location":"ops/deployment-guide/#file-structure","title":"File Structure","text":"<pre><code>deploy/kubernetes/\n\u251c\u2500\u2500 kustomization.yaml      # Kustomize entry point\n\u251c\u2500\u2500 namespace.yaml          # Namespace definition\n\u251c\u2500\u2500 configmap.yaml          # Application configuration\n\u251c\u2500\u2500 secrets.yaml            # Credentials (template)\n\u251c\u2500\u2500 clickhouse.yaml         # ClickHouse StatefulSet\n\u251c\u2500\u2500 redis.yaml              # Redis Deployment\n\u251c\u2500\u2500 api.yaml                # API Deployment + HPA\n\u251c\u2500\u2500 worker.yaml             # Celery Worker Deployment + HPA\n\u251c\u2500\u2500 cronjobs.yaml           # Scheduled sync jobs\n\u2514\u2500\u2500 ingress.yaml            # Ingress + NetworkPolicy\n</code></pre>"},{"location":"ops/deployment-guide/#quick-start","title":"Quick Start","text":"<pre><code>cd deploy/kubernetes\n\nkubectl create namespace dev-health\n\nkubectl create secret generic dev-health-secrets \\\n  --namespace dev-health \\\n  --from-literal=GITHUB_TOKEN=\"$GITHUB_TOKEN\" \\\n  --from-literal=DATABASE_URI=\"clickhouse://ch:ch@clickhouse:8123/default\"\n\nkubectl apply -k .\n</code></pre>"},{"location":"ops/deployment-guide/#using-kustomize-overlays","title":"Using Kustomize Overlays","text":"<p>Create environment-specific overlays:</p> <pre><code>deploy/kubernetes/\n\u251c\u2500\u2500 base/\n\u2502   \u2514\u2500\u2500 kustomization.yaml\n\u251c\u2500\u2500 overlays/\n\u2502   \u251c\u2500\u2500 production/\n\u2502   \u2502   \u2514\u2500\u2500 kustomization.yaml\n\u2502   \u2514\u2500\u2500 staging/\n\u2502       \u2514\u2500\u2500 kustomization.yaml\n</code></pre> <p>Example production overlay:</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - ../../base\npatchesStrategicMerge:\n  - api-patch.yaml\nimages:\n  - name: ghcr.io/your-org/dev-health-ops\n    newTag: v1.2.3\n</code></pre>"},{"location":"ops/deployment-guide/#external-secrets","title":"External Secrets","text":"<p>For production, use a secrets manager:</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: dev-health-secrets\n  namespace: dev-health\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: vault-backend\n    kind: ClusterSecretStore\n  target:\n    name: dev-health-secrets\n  data:\n    - secretKey: GITHUB_TOKEN\n      remoteRef:\n        key: dev-health/github\n        property: token\n</code></pre>"},{"location":"ops/deployment-guide/#monitoring","title":"Monitoring","text":"<p>The API exposes <code>/health</code> for liveness/readiness probes. For metrics:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: dev-health-api\n  namespace: dev-health\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: dev-health-api\n  endpoints:\n    - port: http\n      path: /metrics\n</code></pre>"},{"location":"ops/deployment-guide/#docker-compose-deployment","title":"Docker Compose Deployment","text":""},{"location":"ops/deployment-guide/#file-structure_1","title":"File Structure","text":"<pre><code>deploy/docker-compose/\n\u251c\u2500\u2500 compose.production.yml  # Production stack\n\u2514\u2500\u2500 .env.example            # Environment template\n</code></pre>"},{"location":"ops/deployment-guide/#quick-start_1","title":"Quick Start","text":"<pre><code>cd deploy/docker-compose\n\ncp .env.example .env\n\ndocker compose -f compose.production.yml up -d\n</code></pre>"},{"location":"ops/deployment-guide/#customization","title":"Customization","text":"<p>Override specific services:</p> <pre><code>docker compose -f compose.production.yml \\\n  -f compose.override.yml up -d\n</code></pre> <p>Example <code>compose.override.yml</code>:</p> <pre><code>services:\n  api:\n    deploy:\n      replicas: 4\n  worker:\n    environment:\n      - WORKER_CONCURRENCY=8\n</code></pre>"},{"location":"ops/deployment-guide/#running-sync-commands","title":"Running Sync Commands","text":"<pre><code>docker compose -f compose.production.yml run --rm api \\\n  dev-hops sync work-items --provider github --backfill 30\n\ndocker compose -f compose.production.yml run --rm api \\\n  dev-hops metrics daily\n</code></pre>"},{"location":"ops/deployment-guide/#docker-swarm-deployment","title":"Docker Swarm Deployment","text":""},{"location":"ops/deployment-guide/#file-structure_2","title":"File Structure","text":"<pre><code>deploy/docker-swarm/\n\u251c\u2500\u2500 stack.yml   # Swarm stack definition\n\u2514\u2500\u2500 README.md   # Setup instructions\n</code></pre>"},{"location":"ops/deployment-guide/#quick-start_2","title":"Quick Start","text":"<pre><code>docker swarm init\n\necho \"ch_password\" | docker secret create clickhouse_password -\necho \"$GITHUB_TOKEN\" | docker secret create github_token -\necho \"$GITLAB_TOKEN\" | docker secret create gitlab_token -\n\ndocker stack deploy -c deploy/docker-swarm/stack.yml dev-health\n</code></pre>"},{"location":"ops/deployment-guide/#scaling","title":"Scaling","text":"<pre><code>docker service scale dev-health_api=4\ndocker service scale dev-health_worker=4\n</code></pre>"},{"location":"ops/deployment-guide/#updates","title":"Updates","text":"<p>Rolling updates with zero downtime:</p> <pre><code>docker service update --image ghcr.io/your-org/dev-health-ops:v1.2.3 \\\n  dev-health_api\n</code></pre>"},{"location":"ops/deployment-guide/#local-development","title":"Local Development","text":"<p>Use the root <code>compose.yml</code> for local development:</p> <pre><code>docker compose up -d clickhouse redis\n\npip install -e .\n\ndev-hops api --reload\n\ncelery -A workers.celery_app worker --loglevel=debug\n</code></pre>"},{"location":"ops/deployment-guide/#storage-backends","title":"Storage Backends","text":""},{"location":"ops/deployment-guide/#clickhouse-recommended","title":"ClickHouse (Recommended)","text":"<p>Optimized for analytics workloads. Connection string:</p> <pre><code>clickhouse://user:password@host:8123/database\n</code></pre>"},{"location":"ops/deployment-guide/#postgresql","title":"PostgreSQL","text":"<p>For smaller deployments or existing Postgres infrastructure:</p> <pre><code>postgresql+asyncpg://user:password@host:5432/database\n</code></pre> <p>Requires Alembic migrations:</p> <pre><code>alembic upgrade head\n</code></pre>"},{"location":"ops/deployment-guide/#mongodb","title":"MongoDB","text":"<p>Document storage option:</p> <pre><code>mongodb://host:27017\n</code></pre>"},{"location":"ops/deployment-guide/#scheduled-sync-jobs","title":"Scheduled Sync Jobs","text":""},{"location":"ops/deployment-guide/#kubernetes-cronjobs","title":"Kubernetes CronJobs","text":"<p>CronJobs are defined in <code>deploy/kubernetes/cronjobs.yaml</code>:</p> Job Schedule Description daily-metrics 0 2 * * * Compute daily metrics sync-github 0 */6 * * * Sync GitHub work items sync-gitlab 30 */6 * * * Sync GitLab work items sync-jira 0 */4 * * * Sync Jira work items"},{"location":"ops/deployment-guide/#docker-compose-swarm","title":"Docker Compose / Swarm","text":"<p>Use host cron or a separate scheduler service:</p> <pre><code>0 2 * * * docker compose -f compose.production.yml run --rm api dev-hops metrics daily\n0 */6 * * * docker compose -f compose.production.yml run --rm api dev-hops sync work-items --provider github --backfill 1\n</code></pre>"},{"location":"ops/deployment-guide/#health-checks","title":"Health Checks","text":"Endpoint Purpose <code>/health</code> API liveness/readiness <code>/graphql</code> GraphQL playground"},{"location":"ops/deployment-guide/#clickhouse","title":"ClickHouse","text":"<pre><code>wget -q -O- http://clickhouse:8123/ping\n</code></pre>"},{"location":"ops/deployment-guide/#redis","title":"Redis","text":"<pre><code>redis-cli ping\n</code></pre>"},{"location":"ops/deployment-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ops/deployment-guide/#api-wont-start","title":"API Won't Start","text":"<ol> <li> <p>Check database connectivity:    <pre><code>kubectl logs -l app.kubernetes.io/name=dev-health-api\n</code></pre></p> </li> <li> <p>Verify secrets are mounted:    <pre><code>kubectl exec -it deploy/dev-health-api -- env | grep DATABASE\n</code></pre></p> </li> </ol>"},{"location":"ops/deployment-guide/#workers-not-processing","title":"Workers Not Processing","text":"<ol> <li> <p>Check Celery connection:    <pre><code>celery -A workers.celery_app inspect ping\n</code></pre></p> </li> <li> <p>Check Redis:    <pre><code>redis-cli -h redis INFO replication\n</code></pre></p> </li> </ol>"},{"location":"ops/deployment-guide/#sync-failures","title":"Sync Failures","text":"<ol> <li> <p>Check provider credentials:    <pre><code>curl -H \"Authorization: token $GITHUB_TOKEN\" https://api.github.com/user\n</code></pre></p> </li> <li> <p>Check rate limits:    <pre><code>curl -I -H \"Authorization: token $GITHUB_TOKEN\" https://api.github.com/rate_limit\n</code></pre></p> </li> </ol>"},{"location":"ops/deployment-guide/#security-recommendations","title":"Security Recommendations","text":"<ol> <li>Use secret managers (Vault, AWS Secrets Manager) instead of plain secrets</li> <li>Enable TLS on all endpoints</li> <li>Restrict network access using NetworkPolicies</li> <li>Rotate credentials regularly</li> <li>Use read-only tokens where possible (GitHub, GitLab)</li> <li>Audit API access via ingress logs</li> </ol>"},{"location":"plans/atlassian-client-integration/","title":"Integration Plan: Adopt atlassian Client in dev-health-ops","text":""},{"location":"plans/atlassian-client-integration/#executive-summary","title":"Executive Summary","text":"<p>Replace the custom Jira client in <code>dev-health-ops</code> with the shared <code>atlassian</code> client library. This consolidates Jira API integration, adds changelog/worklog/sprint support, and prepares for  GraphQL (AGG) adoption.</p>"},{"location":"plans/atlassian-client-integration/#current-state","title":"Current State","text":""},{"location":"plans/atlassian-client-integration/#dev-health-ops-jira-provider","title":"dev-health-ops Jira Provider","text":"Component Location Status Client <code>providers/jira/client.py</code> Custom requests-based client Normalizer <code>providers/jira/normalize.py</code> Maps raw JSON to internal models Provider <code>providers/jira/provider.py</code> Entry point <p>Capabilities: - Issue search via JQL (<code>/rest/api/3/search/jql</code>) - Issue comments (<code>/rest/api/3/issue/{key}/comment</code>) - Sprint fetch (<code>/rest/agile/1.0/sprint/{id}</code>) - Projects (<code>/rest/api/3/project/search</code>) - Rate limiting with backoff</p> <p>Gaps: - No changelog pagination (truncated on large issues) - No worklog iteration - No board-level sprint iteration - No GraphQL support - No canonical models (raw dict passing)</p>"},{"location":"plans/atlassian-client-integration/#atlassian-client-library","title":"atlassian Client Library","text":"Component Location Coverage REST Client <code>python/atlassian/rest/client.py</code> Full REST Issues <code>python/atlassian/rest/api/jira_issues.py</code> <code>iter_issues_via_rest()</code> \u2192 <code>JiraIssue</code> REST Changelog <code>python/atlassian/rest/api/jira_changelog.py</code> <code>iter_issue_changelog_via_rest()</code> \u2192 <code>JiraChangelogEvent</code> REST Worklogs <code>python/atlassian/rest/api/jira_worklogs.py</code> <code>iter_issue_worklogs_via_rest()</code> \u2192 <code>JiraWorklog</code> REST Sprints <code>python/atlassian/rest/api/jira_sprints.py</code> <code>iter_board_sprints_via_rest()</code> \u2192 <code>JiraSprint</code> REST Projects <code>python/atlassian/rest/api/jira_projects.py</code> Projects iteration GraphQL Issues <code>python/atlassian/graph/api/jira_issues.py</code> AGG GraphQL queries GraphQL Worklogs <code>python/atlassian/graph/api/jira_worklogs.py</code> AGG GraphQL queries GraphQL Sprints <code>python/atlassian/graph/api/jira_sprints.py</code> AGG GraphQL queries Canonical Models <code>python/atlassian/canonical_models.py</code> Typed dataclasses"},{"location":"plans/atlassian-client-integration/#integration-phases","title":"Integration Phases","text":""},{"location":"plans/atlassian-client-integration/#phase-0-foundation-prerequisite","title":"Phase 0: Foundation (Prerequisite)","text":"<p>Goal: Make atlassian installable as a dependency</p> <ul> <li>[ ] Add <code>atlassian</code> as git dependency or publish to internal PyPI</li> <li>[ ] Verify import works: <code>from atlassian.rest.api import iter_issues_via_rest</code></li> <li>[ ] Document required env vars in dev-health-ops</li> </ul> <p>Env vars needed: <pre><code># Basic auth (simpler for Cloud)\nATLASSIAN_EMAIL=\nATLASSIAN_API_TOKEN=\nATLASSIAN_JIRA_BASE_URL=https://your-org.atlassian.net\n\n# Or OAuth (for AGG)\nATLASSIAN_OAUTH_ACCESS_TOKEN=\nATLASSIAN_CLOUD_ID=\n</code></pre></p>"},{"location":"plans/atlassian-client-integration/#phase-1-issues-migration","title":"Phase 1: Issues Migration","text":"<p>Goal: Replace issue fetching with atlassian client</p> Step Action Risk 1.1 Create adapter: <code>atlassian.JiraIssue</code> \u2192 internal <code>WorkItem</code> Low 1.2 Wire <code>iter_issues_via_rest()</code> into provider Medium 1.3 Deprecate <code>JiraClient.iter_issues()</code> Low 1.4 Remove legacy issue code after validation Low <p>Mapping: <code>atlassian.JiraIssue</code> \u2192 <code>dev_health_ops.models.WorkItem</code></p>"},{"location":"plans/atlassian-client-integration/#phase-2-changelog-integration-new-capability","title":"Phase 2: Changelog Integration (NEW capability)","text":"<p>Goal: Add proper changelog pagination</p> Step Action Risk 2.1 Integrate <code>iter_issue_changelog_via_rest()</code> Low 2.2 Create mapper: <code>JiraChangelogEvent</code> \u2192 internal changelog model Low 2.3 Update processors to consume changelog events Medium <p>Note: Current dev-health-ops uses <code>expand=changelog</code> which truncates. This phase enables full history.</p>"},{"location":"plans/atlassian-client-integration/#phase-3-worklog-integration-new-capability","title":"Phase 3: Worklog Integration (NEW capability)","text":"<p>Goal: Add worklog iteration for time tracking metrics</p> Step Action Risk 3.1 Integrate <code>iter_issue_worklogs_via_rest()</code> Low 3.2 Create mapper: <code>JiraWorklog</code> \u2192 internal worklog model Low 3.3 Add worklog sink for persistence Medium"},{"location":"plans/atlassian-client-integration/#phase-4-sprint-iteration-enhanced-capability","title":"Phase 4: Sprint Iteration (Enhanced capability)","text":"<p>Goal: Replace single-sprint fetch with board-level iteration</p> Step Action Risk 4.1 Integrate <code>iter_board_sprints_via_rest()</code> Low 4.2 Create mapper: <code>JiraSprint</code> \u2192 internal sprint model Low 4.3 Update sprint sync to iterate boards Medium <p>Current: <code>JiraClient.get_sprint(sprint_id)</code> fetches one sprint at a time. New: Iterate all sprints for a board with state filtering.</p>"},{"location":"plans/atlassian-client-integration/#phase-5-agg-graphql-future","title":"Phase 5: AGG GraphQL (Future)","text":"<p>Goal: Enable Atlassian GraphQL Gateway for enhanced queries</p> Step Action Risk 5.1 Add GraphQL client setup (OAuth tokens) Medium 5.2 Create hybrid strategy: REST fallback, GraphQL primary High 5.3 Enable Opsgenie team correlation via AGG Medium <p>Depends on: Issue #226 (AGG integration framework)</p>"},{"location":"plans/atlassian-client-integration/#migration-strategy","title":"Migration Strategy","text":""},{"location":"plans/atlassian-client-integration/#approach-parallel-run","title":"Approach: Parallel Run","text":"<ol> <li>Add atlassian client alongside existing client</li> <li>Feature flag to toggle between implementations</li> <li>Run both in shadow mode, compare outputs</li> <li>Validate via metrics: same issue counts, same changelog lengths</li> <li>Remove legacy client after 1 sprint of stable operation</li> </ol>"},{"location":"plans/atlassian-client-integration/#rollback-plan","title":"Rollback Plan","text":"<ul> <li>Feature flag reverts to legacy client instantly</li> <li>No database schema changes in Phases 1-4</li> <li>Canonical models are additive, not replacing</li> </ul>"},{"location":"plans/atlassian-client-integration/#effort-estimates","title":"Effort Estimates","text":"Phase Effort Dependencies Phase 0: Foundation 2h None Phase 1: Issues 4h Phase 0 Phase 2: Changelog 3h Phase 1 Phase 3: Worklogs 3h Phase 1 Phase 4: Sprints 2h Phase 1 Phase 5: AGG GraphQL 8h Phases 1-4, #226 <p>Total: ~22h (excluding Phase 5)</p>"},{"location":"plans/atlassian-client-integration/#success-criteria","title":"Success Criteria","text":"<ul> <li>[ ] All existing Jira sync tests pass with new client</li> <li>[ ] Changelog fully paginated (no truncation)</li> <li>[ ] Worklog data available in metrics</li> <li>[ ] Sprint board iteration working</li> <li>[ ] No performance regression (same or better API call count)</li> <li>[ ] Feature flag for instant rollback</li> </ul>"},{"location":"plans/atlassian-client-integration/#related-issues","title":"Related Issues","text":""},{"location":"plans/atlassian-client-integration/#225-jira-serverdc-support-blocked-until-agg-or-server-api-added-to-atlassian","title":"225 - Jira Server/DC support (blocked until AGG or Server API added to atlassian)","text":""},{"location":"plans/atlassian-client-integration/#226-agg-integration-framework-enables-phase-5","title":"226 - AGG integration framework (enables Phase 5)","text":""},{"location":"plans/atlassian-client-integration/#227-gitlab-epic-sync-separate-track","title":"227 - GitLab Epic sync (separate track)","text":""},{"location":"product/concepts/","title":"Concepts","text":""},{"location":"product/concepts/#workunits","title":"WorkUnits","text":"<p>A WorkUnit is an evidence container (PR, issue, incident, etc.). It is never a category.</p>"},{"location":"product/concepts/#evidence-quality","title":"Evidence quality","text":"<p>Every categorization emits an evidence quality value and band. Low evidence quality is a data-quality signal.</p>"},{"location":"product/concepts/#probability-distributions","title":"Probability distributions","text":"<p>Categorization is a distribution across canonical subcategories, rolled up deterministically to themes.</p>"},{"location":"product/concepts/#inspectability","title":"Inspectability","text":"<p>Every computed output should map back to evidence: - raw objects (PRs, issues, commits) - derived relationships (work graph edges) - materialized aggregates</p>"},{"location":"product/investment-taxonomy/","title":"Investment Taxonomy","text":"<p>Source of truth: <code>dev-health-ops/investment_taxonomy.py</code></p>"},{"location":"product/investment-taxonomy/#themes","title":"Themes","text":"<ul> <li><code>feature_delivery\"</code></li> <li><code>maintenance\"</code></li> <li><code>operational\"</code></li> <li><code>quality\"</code></li> <li><code>risk\"</code></li> </ul>"},{"location":"product/investment-taxonomy/#subcategories-by-theme","title":"Subcategories (by theme)","text":""},{"location":"product/investment-taxonomy/#feature_delivery","title":"<code>feature_delivery\"</code>","text":""},{"location":"product/investment-taxonomy/#maintenance","title":"<code>maintenance\"</code>","text":""},{"location":"product/investment-taxonomy/#operational","title":"<code>operational\"</code>","text":""},{"location":"product/investment-taxonomy/#quality","title":"<code>quality\"</code>","text":""},{"location":"product/investment-taxonomy/#risk","title":"<code>risk\"</code>","text":""},{"location":"product/investment-taxonomy/#guarantees","title":"Guarantees","text":"<ul> <li>Subcategory keys are canonical and fixed.</li> <li>Theme roll-up is deterministic via <code>theme_of(subcategory)</code>.</li> </ul>"},{"location":"product/prd/","title":"Dev Health: Product Requirements Document","text":"<p>Last updated: 2026-01-25</p>"},{"location":"product/prd/#purpose","title":"Purpose","text":"<p>Dev Health is built to make operating modes visible using inspectable evidence. It answers: - Where effort is invested - Why flow degrades - Where durability risk concentrates</p> <p>This system is explicitly designed to avoid turning metrics into scorecards.</p>"},{"location":"product/prd/#scope","title":"Scope","text":"<ul> <li>Ingest: Git providers + work tracking + optional incident/deploy signals</li> <li>Normalize: unified models and timelines</li> <li>Persist: sinks (ClickHouse/Postgres/etc.)</li> <li>Compute: materialized metrics + investment distributions</li> <li>Serve: GraphQL analytics API</li> <li>Visualize: dev-health-web (primary) + optional Grafana dashboards (panel plugin lives in <code>dev-health-panels</code>)</li> </ul>"},{"location":"product/prd/#product-pillars","title":"Product pillars","text":"<ol> <li>Investment View (canonical)    Theme and subcategory distributions for WorkUnits, persisted at compute-time.</li> <li>Flow &amp; constraints    Cycle decomposition, throughput, WIP, review load/latency.</li> <li>Durability risk    Churn, hotspots, ownership concentration.</li> <li>Well-being signals (team-level)    After-hours and weekend ratios; pattern drift.</li> </ol>"},{"location":"product/prd/#guardrails-non-negotiable","title":"Guardrails (non-negotiable)","text":"<ul> <li>No person-to-person comparisons.</li> <li>LLM is allowed at compute-time for categorization only; UX-time explanation must not recategorize.</li> <li>WorkUnits are evidence containers, not categories.</li> <li>Categories are fixed, canonical keys.</li> </ul>"},{"location":"product/prd/#success-criteria","title":"Success criteria","text":"<ul> <li>A new engineer can explain each view and its backing computation without reading code.</li> <li>Every chart is traceable to (query \u2192 table/view \u2192 evidence).</li> <li>Adding a provider or sink follows a documented contract.</li> </ul>"},{"location":"product/prd/#references","title":"References","text":"<ul> <li><code>dev-health-ops/AGENTS.md</code> (repo rules)</li> <li><code>docs/90-appendix/agent-instructions/*</code> (deep dives)</li> </ul>"},{"location":"product/product-brief/","title":"Developer Health Ops: Product Brief","text":"<p>Dev Health Ops is the backend for Dev Health: ingestion, normalization, persistence, and computations.</p>"},{"location":"product/product-brief/#one-sentence","title":"One sentence","text":"<p>An OSS analytics platform for team operating modes and developer health, backed by provider data and computed metrics stored in DB sinks.</p>"},{"location":"product/product-brief/#what-it-is","title":"What it is","text":"<ul> <li>Provider connectors + processors</li> <li>Storage sinks</li> <li>Materialized metrics and distributions</li> <li>GraphQL analytics API</li> </ul>"},{"location":"product/product-brief/#what-it-is-not","title":"What it is not","text":"<ul> <li>HR tooling</li> <li>Individual ranking</li> <li>A black-box score</li> </ul>"},{"location":"product/product-brief/#canonical-lens-investment-view","title":"Canonical lens: Investment View","text":"<p>Investment is the stable abstraction for \u201cwhere effort goes.\u201d - Compute-time distributions - Theme-level leadership view - Subcategory drill-down - No WorkUnit-as-category</p>"},{"location":"user-guide/investment-view/","title":"Investment View","text":"<p>This page is the leadership-facing definition of the canonical investment lens.</p>"},{"location":"user-guide/investment-view/#what-it-answers","title":"What it answers","text":"<ul> <li>Where is engineering effort going?</li> <li>Is the system in planned delivery mode, reactive mode, or maintenance mode?</li> <li>What is dominating attention (and for how long)?</li> </ul>"},{"location":"user-guide/investment-view/#core-mechanics","title":"Core mechanics","text":"<ul> <li>A WorkUnit receives a compute-time distribution over subcategories.</li> <li>Subcategories roll up deterministically to themes.</li> <li>Aggregations are probability-weighted.</li> </ul>"},{"location":"user-guide/investment-view/#taxonomy-canonical-keys","title":"Taxonomy (canonical keys)","text":"<p>See: <code>docs/00-overview/04-investment-taxonomy.md</code>.</p>"},{"location":"user-guide/investment-view/#non-negotiables","title":"Non-negotiables","text":"<ul> <li>No WorkUnit-as-category.</li> <li>No user-defined categories.</li> <li>No \u201cunknown\u201d output.</li> </ul>"},{"location":"user-guide/investment-view/#related-docs","title":"Related docs","text":"<ul> <li>LLM categorization contract</li> <li>Work graph (relationships and materialization)</li> </ul>"},{"location":"user-guide/views-index/","title":"Views and Charts","text":"<p>Each view answers a specific operational question. Views are explanations with drill-down, not a wall of widgets.</p>"},{"location":"user-guide/views-index/#core-views","title":"Core views","text":"<ul> <li>Investment Mix (treemap/sunburst)</li> <li>Investment Flows (sankey)</li> <li>Investment Expense (stacked area)</li> <li>Code Hotspots (heatmap/treemap/sunburst)</li> <li>PR Flow (stage breakdown)</li> <li>Quadrants (raw-value state classification)</li> <li>Flame diagrams (single-point decomposition)</li> </ul>"},{"location":"user-guide/views-index/#interpretation-rules","title":"Interpretation rules","text":"<ul> <li>Trends over snapshots.</li> <li>Compare within the same window + filter context.</li> <li>Treat \u201cunknown/unclassified\u201d as a pipeline bug, not an output.</li> </ul>"},{"location":"user-guide/work-graph/","title":"Work Graph","text":"<p>The work graph is the structure that links evidence and supports drill-down.</p>"},{"location":"user-guide/work-graph/#what-it-is","title":"What it is","text":"<p>A graph-like model of WorkUnits and relationships: - work item \u2194 PR \u2194 commits \u2194 files \u2194 incidents (when present)</p>"},{"location":"user-guide/work-graph/#why-it-exists","title":"Why it exists","text":"<ul> <li>Enables explainability without recomputation.</li> <li>Provides drill-down paths from aggregates to evidence.</li> <li>Supports flow and investment distribution materialization.</li> </ul>"},{"location":"user-guide/work-graph/#what-it-is-not","title":"What it is not","text":"<ul> <li>A replacement for provider-native objects.</li> <li>A scoring layer.</li> </ul>"},{"location":"user-guide/views/code-hotspots/","title":"Code Hotspots","text":""},{"location":"user-guide/views/code-hotspots/#what-it-answers","title":"What it answers","text":"<ul> <li>Where is churn concentrated?</li> <li>Which areas are persistent risk multipliers?</li> </ul>"},{"location":"user-guide/views/code-hotspots/#typical-visualizations","title":"Typical visualizations","text":"<ul> <li>Heatmap (time \u00d7 component)</li> <li>Treemap/sunburst (repo \u2192 path \u2192 file)</li> </ul>"},{"location":"user-guide/views/code-hotspots/#backing-computations","title":"Backing computations","text":"<ul> <li>churn aggregation</li> <li>hotspot ranking (persistence + magnitude)</li> </ul>"},{"location":"user-guide/views/flame-diagrams/","title":"Flame diagrams","text":"<p>Flame diagrams explain a single selected point by decomposing contributions (repo/path/file/etc.).</p> <p>UX rule: - \u201cExplore\u201d must render the flame chart (not only navigate).</p>"},{"location":"user-guide/views/investment-expense/","title":"Investment Expense (Stacked Area)","text":""},{"location":"user-guide/views/investment-expense/#what-it-answers","title":"What it answers","text":"<ul> <li>How does theme composition drift over time?</li> <li>Are changes spiky (incidents) or gradual (structural)?</li> </ul>"},{"location":"user-guide/views/investment-expense/#backing-surfaces","title":"Backing surfaces","text":"<ul> <li>GraphQL <code>analytics.timeseries</code></li> <li>SQL compiler <code>compile_timeseries</code></li> </ul>"},{"location":"user-guide/views/investment-flows/","title":"Investment Flows (Sankey)","text":""},{"location":"user-guide/views/investment-flows/#what-it-answers","title":"What it answers","text":"<ul> <li>How does effort concentrate or fragment across themes and scopes?</li> <li>Where is the shift between delivery and reactive work?</li> </ul>"},{"location":"user-guide/views/investment-flows/#requirements","title":"Requirements","text":"<ul> <li>Sankey must support time slicing or baseline deltas, otherwise it becomes decorative.</li> <li>Scope node (repo/team) is optional and depends on mapping availability.</li> </ul>"},{"location":"user-guide/views/investment-flows/#backing-surfaces","title":"Backing surfaces","text":"<ul> <li>GraphQL <code>analytics.sankey</code></li> <li>SQL compiler <code>compile_sankey</code></li> </ul>"},{"location":"user-guide/views/investment-mix/","title":"Investment View \u2014 Product Specification","text":""},{"location":"user-guide/views/investment-mix/#status","title":"Status","text":"<ul> <li>Signals: POC only, retired</li> <li>Investment View: Canonical model going forward</li> </ul>"},{"location":"user-guide/views/investment-mix/#primary-question","title":"Primary Question","text":"<p>\"Where is human effort actually being invested across the organization, and what is the cost to people when certain kinds of work dominate?\"</p> <p>This is: - NOT a work taxonomy - NOT a reporting layer - An organizational mirror for leadership</p>"},{"location":"user-guide/views/investment-mix/#core-design-principles-non-negotiable","title":"Core Design Principles (Non-Negotiable)","text":""},{"location":"user-guide/views/investment-mix/#1-messy-human-inputs-are-normalized","title":"1. Messy Human Inputs Are Normalized","text":"<ul> <li>Jira labels, issue types, GitHub labels are inputs only</li> <li>They are never surfaced as truth</li> <li>Provider-native categories are normalized away</li> </ul>"},{"location":"user-guide/views/investment-mix/#2-investment-categories-are-canonical","title":"2. Investment Categories Are Canonical","text":"<ul> <li>Fixed vocabulary (see below)</li> <li>Not user-configurable</li> <li>Comparable across teams, tools, and time</li> </ul>"},{"location":"user-guide/views/investment-mix/#3-investment-categorization-is-decisive","title":"3. Investment Categorization Is Decisive","text":"<ul> <li>Always produces a distribution</li> <li>Never returns \"unknown\"</li> <li>Weak evidence lowers evidence quality, not output existence</li> </ul>"},{"location":"user-guide/views/investment-mix/#4-evidence-quality-correctness","title":"4. Evidence Quality \u2260 Correctness","text":"<ul> <li>Indicates corroboration strength</li> <li>Does not block visibility</li> <li>Low quality means \"inferred from weak signals,\" not \"probably wrong\"</li> </ul>"},{"location":"user-guide/views/investment-mix/#5-the-output-is-about-people","title":"5. The Output Is About People","text":"<ul> <li>Effort concentration</li> <li>Support load</li> <li>Long-term cost of neglect</li> </ul>"},{"location":"user-guide/views/investment-mix/#canonical-investment-categories","title":"Canonical Investment Categories","text":""},{"location":"user-guide/views/investment-mix/#themes-fixed-leadership-facing","title":"Themes (Fixed, Leadership-Facing)","text":"Theme Description Feature Delivery New value, customer-requested, roadmap items Operational / Support External support, internal support, incident response Maintenance / Tech Debt Refactoring, upgrades, cleanup Quality / Reliability Testing, observability, stability work Risk / Security Security fixes, compliance, vulnerability remediation <p>Rules: - No synonyms - No overrides - No per-team customization</p>"},{"location":"user-guide/views/investment-mix/#subcategories-fixed-per-theme","title":"Subcategories (Fixed Per Theme)","text":"<p>Each theme has a curated subcategory set providing resolution without fragmenting language.</p> <p>Example structure: <pre><code>Operational\n \u251c\u2500\u2500 External-facing\n \u251c\u2500\u2500 Internal support\n \u251c\u2500\u2500 Incident response\n \u2514\u2500\u2500 On-call / reactive\n\nFeature Delivery\n \u251c\u2500\u2500 Customer-requested\n \u251c\u2500\u2500 Strategic / roadmap\n \u2514\u2500\u2500 Enablement / platform\n</code></pre></p> <p>Rules: - Subcategories are not user-defined - Subcategories are comparable across orgs - Subcategories roll up cleanly into themes</p>"},{"location":"user-guide/views/investment-mix/#categorization-hierarchy","title":"Categorization Hierarchy","text":"<pre><code>Evidence (WorkUnits)\n   \u2193\nSubcategory (what flavor of work within a theme)\n   \u2193\nTheme (investment category)\n</code></pre> <p>Where: - Theme answers: \"What kind of organizational investment is this?\" - Subcategory answers: \"What flavor of that investment is consuming people?\" - Evidence (WorkUnits) answers: \"What concrete activity supports this inference?\"</p> <p>Critical distinction: - WorkUnits are evidence containers, NOT categories - WorkUnits never appear as categorization layers - WorkUnits never appear as peers to themes/subcategories in UI</p>"},{"location":"user-guide/views/investment-mix/#data-model","title":"Data Model","text":""},{"location":"user-guide/views/investment-mix/#investment-categorization-output","title":"Investment Categorization Output","text":"<pre><code>{\n  \"entity_id\": \"work_unit_id\",\n  \"investment\": {\n    \"themes\": {\n      \"operational\": 0.47,\n      \"feature_delivery\": 0.33,\n      \"maintenance\": 0.15,\n      \"quality\": 0.05,\n      \"risk\": 0.0\n    },\n    \"subcategories\": {\n      \"operational.external\": 0.29,\n      \"operational.internal\": 0.18,\n      \"feature_delivery.customer\": 0.21,\n      \"feature_delivery.platform\": 0.12,\n      \"maintenance.refactor\": 0.15,\n      \"quality.testing\": 0.05\n    }\n  },\n  \"evidence_quality\": {\n    \"value\": 0.64,\n    \"band\": \"moderate\"\n  },\n  \"evidence\": {\n    \"textual\": [\"Matched phrases: 'hotfix', 'incident response'\"],\n    \"structural\": [\"Linked to on-call incident issue\"],\n    \"contextual\": [\"PR merged directly to main during outage window\"]\n  }\n}\n</code></pre>"},{"location":"user-guide/views/investment-mix/#guarantees","title":"Guarantees","text":"<ul> <li>Theme probabilities sum to ~1.0</li> <li>Subcategory probabilities sum to theme probabilities</li> <li>Evidence arrays always present (may be empty)</li> <li>Evidence quality always emitted</li> <li>Categorization never returns \"unknown\"</li> <li>WorkUnit never appears as a category</li> </ul>"},{"location":"user-guide/views/investment-mix/#categorization-logic","title":"Categorization Logic","text":""},{"location":"user-guide/views/investment-mix/#signal-priority-order","title":"Signal Priority Order","text":"<ol> <li>Textual intent (primary)</li> <li>Issue title &amp; description</li> <li>PR title &amp; description</li> <li> <p>Commit messages</p> </li> <li> <p>Provider metadata (supporting)</p> </li> <li>Jira issue type</li> <li>Labels</li> <li> <p>Milestones</p> </li> <li> <p>Structural context (corroboration)</p> </li> <li>Relationships</li> <li>Timing</li> <li>Repo scope</li> </ol> <p>Text drives the category. Structure adjusts and corroborates.</p>"},{"location":"user-guide/views/investment-mix/#evidence-quality","title":"Evidence Quality","text":""},{"location":"user-guide/views/investment-mix/#definition","title":"Definition","text":"<p>Evidence quality indicates how strongly the investment categorization is corroborated by independent signals.</p>"},{"location":"user-guide/views/investment-mix/#quality-bands","title":"Quality Bands","text":"Band Range Meaning High 0.8+ Strong corroboration from multiple sources Moderate 0.5-0.8 Reasonable corroboration Low &lt;0.5 Inferred from sparse signals"},{"location":"user-guide/views/investment-mix/#contributors","title":"Contributors","text":"<ul> <li>Presence of descriptive text</li> <li>Agreement between issue, PR, and commits</li> <li>Presence of provider metadata</li> <li>Structural alignment</li> </ul> <p>Low evidence quality means: - \"This is inferred from weak or sparse signals\"</p> <p>It does NOT mean: - \"This is probably wrong\"</p>"},{"location":"user-guide/views/investment-mix/#investment-views-ux","title":"Investment Views (UX)","text":""},{"location":"user-guide/views/investment-mix/#1-treemap-investment-composition","title":"1. Treemap \u2014 Investment Composition","text":"<ul> <li>Nodes: Investment category (theme-level by default)</li> <li>Optional split: category \u00d7 repo_scope or team</li> <li>Size: Probability-weighted effort</li> <li>Opacity: Evidence quality</li> </ul> <p>Purpose: Show where effort is going, regardless of how work was labeled.</p>"},{"location":"user-guide/views/investment-mix/#2-sunburst-investment-distribution","title":"2. Sunburst \u2014 Investment Distribution","text":"<p>Hierarchy: <pre><code>Investment Category (Theme)\n \u2514\u2500\u2500 Repo scope / Team\n     \u2514\u2500\u2500 Work clusters (optional drill-down)\n</code></pre></p> <p>Purpose: Show concentration of investment and who absorbs it.</p>"},{"location":"user-guide/views/investment-mix/#3-sankey-flow-of-investment-pressure","title":"3. Sankey \u2014 Flow of Investment Pressure","text":"<ul> <li>Source: Investment category</li> <li>Target: Repo scope / Team</li> <li>Weight: Probability-weighted effort</li> </ul> <p>Purpose: Make support load, maintenance drag, and feature pressure visible.</p>"},{"location":"user-guide/views/investment-mix/#drill-down-contract","title":"Drill-Down Contract","text":"<ol> <li>Default: Theme-only (leadership readable)</li> <li>Drill: Theme \u2192 Subcategory \u2192 Evidence (WorkUnits)</li> </ol> <p>Never show WorkUnits as top-level segments or peers to categories.</p>"},{"location":"user-guide/views/investment-mix/#what-is-explicitly-not-shown","title":"What Is Explicitly Not Shown","text":"<ul> <li>Jira issue types</li> <li>GitHub labels</li> <li>Raw ticket metadata</li> <li>\"Bug vs Feature\" debates</li> </ul> <p>These are inputs, not outcomes.</p>"},{"location":"user-guide/views/pr-flow/","title":"PR Flow","text":""},{"location":"user-guide/views/pr-flow/#what-it-answers","title":"What it answers","text":"<ul> <li>Where work stalls (pickup, review, merge)?</li> <li>Is the bottleneck capacity or latency?</li> </ul>"},{"location":"user-guide/views/pr-flow/#backing-computations","title":"Backing computations","text":"<ul> <li>stage timestamp extraction per provider</li> <li>windowed aggregation by scope</li> </ul>"},{"location":"user-guide/views/quadrants/","title":"Quadrants","text":"<p>Quadrants classify system state using raw values only.</p> <p>Required quadrant pairs: - Churn \u00d7 Throughput - Cycle Time \u00d7 Throughput - WIP \u00d7 Throughput - Review Load \u00d7 Review Latency</p> <p>Guardrails: - No scoring - No rankings</p>"},{"location":"visualizations/patterns/","title":"Visualization Patterns","text":"<p>Guidelines for selecting and implementing visualizations in Dev Health.</p>"},{"location":"visualizations/patterns/#chart-selection-matrix","title":"Chart Selection Matrix","text":"Need Chart Type When to Use Pattern detection Heatmap Cyclical congestion, fragmentation, risk build-up Single trend/KPI Line chart Directional movement, simple comparisons System state Quadrant Mode classification, trajectory analysis Single item diagnosis Flame diagram Understanding delays in one work item Investment composition Treemap Where effort is allocated by theme Investment distribution Sunburst Concentration and who absorbs it Investment flow Sankey Pressure flows between categories and teams"},{"location":"visualizations/patterns/#heatmaps","title":"Heatmaps","text":""},{"location":"visualizations/patterns/#when-to-use","title":"When to Use","text":"<ul> <li>Cyclical congestion: Hour-of-day \u00d7 weekday patterns</li> <li>Fragmentation: Work spread across repos/modules/systems</li> <li>Risk build-up: Hotspots intensifying over time</li> </ul>"},{"location":"visualizations/patterns/#when-not-to-use","title":"When NOT to Use","text":"<ul> <li>Single trend line needed</li> <li>Executive summaries without dimensional breakdown</li> <li>Cannot trace cells to real artifacts</li> </ul>"},{"location":"visualizations/patterns/#implementation-rules","title":"Implementation Rules","text":"<ul> <li>Every cell must link to evidence for that bucket</li> <li>If you cannot trace a cell to artifacts, use line chart instead</li> <li>Use consistent color scales across related views</li> <li>Include legends and axis labels</li> </ul>"},{"location":"visualizations/patterns/#line-charts","title":"Line Charts","text":""},{"location":"visualizations/patterns/#when-to-use_1","title":"When to Use","text":"<ul> <li>Clear directional movement over time (e.g., median cycle time)</li> <li>Simple before/after comparisons</li> <li>Executive-level summaries</li> <li>KPI tracking</li> </ul>"},{"location":"visualizations/patterns/#when-not-to-use_1","title":"When NOT to Use","text":"<ul> <li>Multi-dimensional pattern detection needed</li> <li>Averages would hide important patterns</li> <li>Need to identify specific problematic periods</li> </ul>"},{"location":"visualizations/patterns/#quadrants","title":"Quadrants","text":""},{"location":"visualizations/patterns/#purpose","title":"Purpose","text":"<p>Classify system modes without ranking or judging performance.</p>"},{"location":"visualizations/patterns/#questions-they-answer","title":"Questions They Answer","text":"<ul> <li>What mode is this team/repo operating in right now?</li> <li>Which direction are we moving across recent windows?</li> <li>Where do effort and outcomes drift out of alignment?</li> </ul>"},{"location":"visualizations/patterns/#usage-rules","title":"Usage Rules","text":"<ul> <li>Use when reasoning about system behavior, not single trends</li> <li>Quadrants are hypothesis starters, not conclusions</li> <li>Always link to evidence</li> <li>Follow up with heatmaps or flame diagrams for diagnosis</li> </ul>"},{"location":"visualizations/patterns/#required-quadrants","title":"Required Quadrants","text":""},{"location":"visualizations/patterns/#1-churn-throughput","title":"1. Churn \u00d7 Throughput","text":"Quadrant Interpretation High churn + High throughput Deliberate refactors, major changes High churn + Low throughput Thrashing, rework loops Low churn + High throughput Efficient delivery Low churn + Low throughput Constraints or hidden blocks"},{"location":"visualizations/patterns/#2-cycle-time-throughput","title":"2. Cycle Time \u00d7 Throughput","text":"Quadrant Interpretation Rising cycle + Stable throughput Coordination debt accumulating Fast cycle + High throughput Healthy flow Slow cycle + Low throughput System under stress Fast cycle + Low throughput Small batch, capacity available"},{"location":"visualizations/patterns/#3-wip-throughput","title":"3. WIP \u00d7 Throughput","text":"Quadrant Interpretation High WIP + Flat throughput Early saturation warning High WIP + High throughput At capacity, watch for overload Low WIP + High throughput Efficient, focused Low WIP + Low throughput Underutilized or blocked"},{"location":"visualizations/patterns/#4-review-load-review-latency","title":"4. Review Load \u00d7 Review Latency","text":"Quadrant Interpretation High load + High latency Structural bottleneck High load + Low latency Reviewer doing well, watch for burnout Low load + High latency Disengagement or competing priorities Low load + Low latency Healthy review process"},{"location":"visualizations/patterns/#zone-maps-experimental","title":"Zone Maps (Experimental)","text":"<p>Optional overlays highlighting fuzzy, overlapping regions with common constraint patterns.</p> <p>Rules: - Zone maps are heuristics - If they feel prescriptive, turn them off - Never use for performance judgment</p>"},{"location":"visualizations/patterns/#flame-diagrams","title":"Flame Diagrams","text":""},{"location":"visualizations/patterns/#when-to-use_2","title":"When to Use","text":"<ul> <li>Understanding where time was spent in single work item</li> <li>Visualizing waiting, rework, or handoffs</li> <li>Diagnosing whether delays were sequential or overlapped</li> <li>Identifying rework loops hidden in aggregate metrics</li> </ul>"},{"location":"visualizations/patterns/#when-not-to-use_2","title":"When NOT to Use","text":"<ul> <li>Fleet-wide signals needed</li> <li>Aggregate trend analysis</li> <li>Comparing across many items</li> </ul>"},{"location":"visualizations/patterns/#relationship-to-cycle-time","title":"Relationship to Cycle Time","text":"Tool Scope Purpose Flame diagram Single item Diagnostic, explain outlier Cycle time Portfolio Strategic, prove trend <p>Use flames to explain why a single item took long. Use cycle time to prove the trend across the system.</p>"},{"location":"visualizations/patterns/#investment-views","title":"Investment Views","text":""},{"location":"visualizations/patterns/#treemap","title":"Treemap","text":"<ul> <li>Nodes: Theme-level (default) or theme \u00d7 scope/team</li> <li>Size: Probability-weighted effort</li> <li>Opacity: Evidence quality</li> <li>Purpose: Show where effort goes, regardless of labels</li> </ul>"},{"location":"visualizations/patterns/#sunburst","title":"Sunburst","text":"<p>Hierarchical drill-down: <pre><code>Theme\n \u2514\u2500\u2500 Repo scope / Team\n     \u2514\u2500\u2500 Work clusters (optional)\n</code></pre></p> <p>Purpose: Show concentration and who absorbs investment.</p>"},{"location":"visualizations/patterns/#sankey","title":"Sankey","text":"<ul> <li>Source: Investment category (theme)</li> <li>Target: Repo scope / Team</li> <li>Weight: Probability-weighted effort</li> <li>Purpose: Make pressure flows visible</li> </ul>"},{"location":"visualizations/patterns/#scope-guardrails","title":"Scope Guardrails","text":""},{"location":"visualizations/patterns/#organizationteamrepo-views","title":"Organization/Team/Repo Views","text":"<ul> <li>Show teams, repos, or services only</li> <li>Never show individual rankings</li> <li>Aggregation is the default</li> </ul>"},{"location":"visualizations/patterns/#individual-views","title":"Individual Views","text":"<ul> <li>Show one person across time windows</li> <li>Optionally show as a path (trajectory)</li> <li>Purpose: Reflection and coaching</li> <li>Never comparative</li> </ul>"},{"location":"visualizations/patterns/#forbidden-patterns","title":"Forbidden Patterns","text":"<ul> <li>Person-to-person rankings or comparisons</li> <li>\"Top performers\" / \"Bottom performers\" lists</li> <li>Stack-ranking visualizations</li> <li>Public leaderboards</li> </ul>"},{"location":"visualizations/patterns/#guardrails-summary","title":"Guardrails Summary","text":"Rule Rationale No person-to-person rankings Prevents misuse for judgment Individual views are single-person only For reflection, not comparison Heatmap cells trace to evidence Maintains inspectability Quadrants show raw values only No percentiles or scoring Flames for diagnosis, cycle time for trends Right tool for right question"},{"location":"visualizations/patterns/#drill-down-patterns","title":"Drill-Down Patterns","text":""},{"location":"visualizations/patterns/#recommended-flow","title":"Recommended Flow","text":"<pre><code>Quadrant \u2192 Heatmap slice \u2192 Flame diagram \u2192 Evidence\n</code></pre> <ol> <li>Quadrant: \"What state are we in?\"</li> <li>Heatmap: \"Where and when is pressure accumulating?\"</li> <li>Flame: \"Why did this specific item take so long?\"</li> <li>Evidence: The actual artifacts (PRs, issues, commits)</li> </ol>"},{"location":"visualizations/patterns/#implementation","title":"Implementation","text":"<ul> <li>Every visualization should support drilling to the next level</li> <li>Evidence should always be the terminal destination</li> <li>Maintain context as user drills down</li> </ul>"}]}